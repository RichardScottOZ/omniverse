{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Theoretical Analysis on Estimation of FLOPs, Parameters, Peak Memory Footprint, and Checkpoint Size\n",
    "\n",
    "[![Twitter Handle](https://img.shields.io/badge/Twitter-@gaohongnan-blue?style=social&logo=twitter)](https://twitter.com/gaohongnan)\n",
    "[![LinkedIn Profile](https://img.shields.io/badge/@gaohongnan-blue?style=social&logo=linkedin)](https://linkedin.com/in/gao-hongnan)\n",
    "[![GitHub Profile](https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&logo=github)](https://github.com/gao-hongnan)\n",
    "\n",
    "```{contents}\n",
    ":local:\n",
    "```\n",
    "\n",
    "This notebook references from\n",
    "[Andrej Karpathy's NanoGPT](https://github.com/karpathy/nanoGPT/blob/master/transformer_sizing.ipynb),\n",
    "which originally stores a bunch of analysis about a Transformer, e.g. estimates\n",
    "the number of FLOPs, parameters, peak memory footprint, checkpoint size, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum, IntEnum\n",
    "from typing import Literal\n",
    "\n",
    "import pandas as pd\n",
    "from rich.pretty import pprint\n",
    "from tabulate import tabulate\n",
    "from torch import nn\n",
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations, Constants and Enums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    num_decoder_blocks: int = 12\n",
    "    context_length: int = 1024\n",
    "    n_embd: int = 768\n",
    "    ffw_size: int = 3072  # note, this is 4 * n_embd\n",
    "    n_head: int = 12\n",
    "    vocab_size: int = 50257\n",
    "    bias: Literal[False] = False\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        assert self.ffw_size == 4 * self.n_embd, \"ffw_size must be 4 * n_embd\"\n",
    "        assert self.bias is False, \"bias must be False in this experiment.\"\n",
    "\n",
    "class GPT2ModelType(Enum):\n",
    "    GPT2 = \"gpt2\"\n",
    "    GPT2_MEDIUM = \"gpt2-medium\"\n",
    "    GPT2_LARGE = \"gpt2-large\"\n",
    "    GPT2_XL = \"gpt2-xl\"\n",
    "\n",
    "class ByteUnits(IntEnum):\n",
    "    B = 1           # Byte = 1 byte\n",
    "    KB = 1000       # Kilobyte = 10^3 bytes\n",
    "    MB = 1000**2    # Megabyte = 10^6 bytes\n",
    "    GB = 1000**3    # Gigabyte = 10^9 bytes\n",
    "\n",
    "\n",
    "class FloatingPointPrecision(IntEnum):\n",
    "    FP32 = 4        # 32-bit floating-point, 4 bytes\n",
    "    FP16 = 2        # 16-bit floating-point, 2 bytes\n",
    "    BFLOAT16 = 2    # bfloat16, 16-bit, 2 bytes\n",
    "\n",
    "class GPUMemory(Enum):\n",
    "    A100_40GB = 40e9  # 40 GB for NVIDIA A100\n",
    "    V100_16GB = 16e9  # 16 GB for NVIDIA V100\n",
    "    V100_32GB = 32e9  # 32 GB for NVIDIA V100\n",
    "    T4_16GB = 16e9    # 16 GB for NVIDIA T4\n",
    "    P100_16GB = 16e9  # 16 GB for NVIDIA P100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Trainable Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_trainable_parameters(model: nn.Module, include_bias: bool = True) -> int:\n",
    "    \"\"\"Returns the number of trainable parameters in the model.\"\"\"\n",
    "    if not include_bias:\n",
    "        return sum(p.numel() for name, p in model.named_parameters() if p.requires_grad and 'bias' not in name)\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = GPT2LMHeadModel.from_pretrained(GPT2ModelType.GPT2.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters in GPT2 model: 124337664 (excluding bias) and 124439808 (including bias).\n"
     ]
    }
   ],
   "source": [
    "gpt2_params_no_bias = total_trainable_parameters(gpt2, include_bias=False)\n",
    "gpt2_params_with_bias = total_trainable_parameters(gpt2, include_bias=True)\n",
    "\n",
    "print(f\"Number of trainable parameters in GPT2 model: {gpt2_params_no_bias} (excluding bias) and {gpt2_params_with_bias} (including bias).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Karpathy's blog post assumed that there is no bias for simplicity, we will\n",
    "also assume that there is no bias in the linear layers. We confirmed that the\n",
    "number of params (`124337664`) for the smallest GPT-2 model indeed matches the\n",
    "number of params given by Karpathy.\n",
    "\n",
    "In what follows, we would assume the smallest GPT-2 model and work out the\n",
    "theoretical model for the Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_args = {\n",
    "#     'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "#     'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "#     'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "#     'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "# }[model_type]\n",
    "\n",
    "def params(\n",
    "    num_decoder_blocks: int = 12,\n",
    "    context_length: int = 1024,\n",
    "    n_embd: int = 768,\n",
    "    ffw_size: int = 3072,\n",
    "    vocab_size: int = 50257,\n",
    ") -> OrderedDict[str, int]:\n",
    "    \"\"\"estimates the number of parameters in the model\"\"\"\n",
    "    out = OrderedDict()\n",
    "\n",
    "    # token and position embeddings\n",
    "    out[\"embedding/position\"] = n_embd * context_length\n",
    "    out[\"embedding/token\"] = n_embd * vocab_size\n",
    "    out[\"embedding\"] = out[\"embedding/position\"] + out[\"embedding/token\"]\n",
    "\n",
    "    # attention blocks\n",
    "    out[\"attention/ln\"] = n_embd  # note, bias=False in our LN\n",
    "    out[\"attention/kqv\"] = n_embd * 3 * n_embd\n",
    "    out[\"attention/proj\"] = n_embd**2\n",
    "    out[\"attention\"] = out[\"attention/ln\"] + out[\"attention/kqv\"] + out[\"attention/proj\"]\n",
    "\n",
    "    # MLP blocks\n",
    "    assert ffw_size == 4 * n_embd, \"ffw_size must be 4 * n_embd\"\n",
    "    out[\"mlp/ln\"] = n_embd\n",
    "    out[\"mlp/ffw\"] = n_embd * ffw_size\n",
    "    out[\"mlp/proj\"] = ffw_size * n_embd\n",
    "    out[\"mlp\"] = out[\"mlp/ln\"] + out[\"mlp/ffw\"] + out[\"mlp/proj\"]\n",
    "\n",
    "    # the transformer and the rest of it\n",
    "    out[\"block\"] = out[\"attention\"] + out[\"mlp\"]\n",
    "    out[\"transformer\"] = num_decoder_blocks * out[\"block\"]\n",
    "    out[\"ln_f\"] = n_embd  # final layernorm\n",
    "    out[\"dense\"] = 0  # 0 because of parameter sharing. This layer uses the weights from the embedding layer\n",
    "\n",
    "    # total\n",
    "    out[\"total\"] = out[\"embedding\"] + out[\"transformer\"] + out[\"ln_f\"] + out[\"dense\"]\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We see: 124337664, Expected: 124337664, Match: True\n",
      "\n",
      "+--------------------+------------+-----------------------+\n",
      "|        Name        | Parameters |       Ratio (%)       |\n",
      "+--------------------+------------+-----------------------+\n",
      "| embedding/position |   786432   |  0.6324970042866496   |\n",
      "|  embedding/token   |  38597376  |  31.042384711361475   |\n",
      "|     embedding      |  39383808  |  31.674881715648123   |\n",
      "|    attention/ln    |    768     | 0.0006176728557486812 |\n",
      "|   attention/kqv    |  1769472   |  1.4231182596449616   |\n",
      "|   attention/proj   |   589824   |  0.47437275321498723  |\n",
      "|     attention      |  2360064   |  1.8981086857156975   |\n",
      "|       mlp/ln       |    768     | 0.0006176728557486812 |\n",
      "|      mlp/ffw       |  2359296   |   1.897491012859949   |\n",
      "|      mlp/proj      |  2359296   |   1.897491012859949   |\n",
      "|        mlp         |  4719360   |   3.795599698575646   |\n",
      "|       block        |  7079424   |   5.693708384291344   |\n",
      "|    transformer     |  84953088  |   68.32450061149613   |\n",
      "|        ln_f        |    768     | 0.0006176728557486812 |\n",
      "|       dense        |     0      |          0.0          |\n",
      "|       total        | 124337664  |         100.0         |\n",
      "+--------------------+------------+-----------------------+\n"
     ]
    }
   ],
   "source": [
    "params_dict = params()\n",
    "gpt2_params_no_bias_manual = params_dict[\"total\"]\n",
    "\n",
    "# Compare to expected PyTorch model parameter count\n",
    "expected_params = gpt2_params_no_bias\n",
    "comparison_result = gpt2_params_no_bias_manual == expected_params\n",
    "comparison_msg = f\"We see: {gpt2_params_no_bias_manual}, Expected: {expected_params}, Match: {comparison_result}\"\n",
    "\n",
    "data = {\n",
    "    \"Name\": params_dict.keys(),\n",
    "    \"Parameters\": params_dict.values(),\n",
    "    \"Ratio (%)\": [value / gpt2_params_no_bias_manual * 100 for value in params_dict.values()],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Printing comparison result and parameter distribution table\n",
    "print(comparison_msg + \"\\n\")\n",
    "print(tabulate(df, headers=\"keys\", tablefmt=\"pretty\", showindex=False, numalign=\"right\", floatfmt=\".4f\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Checkpoint Size and Fluff Ratio \n",
    "\n",
    "The functions below perform a series of calculations related to the size\n",
    "of a GPT-2 model checkpoint, both measured and estimated, and computes the\n",
    "\"fluff ratio\" to compare these sizes. The purpose of these calculations is to\n",
    "evaluate how closely the estimated size of a GPT-2 model checkpoint matches the\n",
    "actual measured size, and to quantify any overhead or additional data in the\n",
    "checkpoint file as a percentage of the estimated size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_checkpoint_size(params_count: int, precision: FloatingPointPrecision, units: ByteUnits) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the estimated checkpoint size in specified units.\n",
    "\n",
    "    This function estimates the checkpoint size for a model given the number\n",
    "    of parameters, the precision of these parameters, and\n",
    "    the desired units for the result. It accounts for the AdamW optimizer's\n",
    "    storage requirements by adding two times the parameter bytes to account\n",
    "    for the optimizer's moment and velocity vectors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params_count : int\n",
    "        The number of parameters excluding biases.\n",
    "    precision : FloatingPointPrecision\n",
    "        The floating point precision of the parameters.\n",
    "    units : ByteUnits\n",
    "        The units for the resulting checkpoint size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The estimated checkpoint size in the specified units.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The AdamW optimizer requires additional storage for each parameter\n",
    "    for maintaining momentum and variance vectors, hence the calculation\n",
    "    includes 2 * params_bytes to accommodate these.\n",
    "    \"\"\"\n",
    "    params_bytes = params_count * precision.value\n",
    "    params_and_buffers_bytes = params_bytes + 2 * params_bytes  # AdamW optimizer buffers\n",
    "    return params_and_buffers_bytes / units.value\n",
    "\n",
    "\n",
    "def calculate_fluff_ratio(measured_bytes: int, estimated_bytes: float, units: ByteUnits) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the fluff ratio between measured and estimated checkpoint sizes.\n",
    "\n",
    "    The fluff ratio is a measure of the overhead or additional data in the\n",
    "    checkpoint file, expressed as a percentage of the estimated size. This\n",
    "    function converts the estimated size from gigabytes (or specified units)\n",
    "    to bytes before calculating the ratio to ensure consistency in units.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    measured_bytes : int\n",
    "        The actual size of the checkpoint file, in bytes.\n",
    "    estimated_bytes : float\n",
    "        The estimated size of the checkpoint file, in the specified units.\n",
    "    units : ByteUnits\n",
    "        The units in which the estimated bytes are provided.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The fluff ratio, expressed as a percentage.\n",
    "    \"\"\"\n",
    "    estimated_bytes_in_bytes = estimated_bytes * units.value\n",
    "    return (measured_bytes / estimated_bytes_in_bytes) * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Measured Checkpoint Size in Bytes**:\n",
    "\n",
    "    - `gpt2_checkpoint_size_measured_in_bytes` is assigned a numerical value\n",
    "      that represents the actual size of a GPT-2 model checkpoint file in bytes.\n",
    "      This value is obtained from the output of the Unix command\n",
    "      `wc -c ckpt.pt`, which counts the number of bytes in the file `ckpt.pt`.\n",
    "\n",
    "2. **Estimated Checkpoint Size in Bytes**:\n",
    "\n",
    "    - The `calculate_checkpoint_size` function is called with the number of\n",
    "      parameters excluding biases (`gpt2_params_no_bias`), the precision of the\n",
    "      model's parameters (`FloatingPointPrecision.FP32`), and the unit of\n",
    "      measurement (`ByteUnits.B` for bytes). This function calculates the\n",
    "      estimated total size of the checkpoint in bytes, taking into account the\n",
    "      parameters and the additional storage required for the AdamW optimizer's\n",
    "      buffers.\n",
    "    - It is worth noting we are assuming floating-point precision of 32 bits (4\n",
    "      bytes) for the model's parameters, and hence we are multiplying the number\n",
    "      of parameters by 4 to obtain the size in bytes.\n",
    "\n",
    "    - The AdamW optimizer, which is commonly used in training deep learning\n",
    "      models for tasks like those involving GPT-2, maintains two additional\n",
    "      values (buffers) for each parameter: the first for the moment vector (`m`)\n",
    "      and the second for the squared moment vector (`v`). These buffers are used\n",
    "      to adapt the learning rates for each parameter during training. This is\n",
    "      why the storage requirement triples (`params_bytes + 2*params_bytes`),\n",
    "      accounting for the original parameters plus the two buffers.\n",
    "\n",
    "3. **Fluff Ratio Calculation**:\n",
    "\n",
    "    - The `calculate_fluff_ratio` function is called with the measured size in\n",
    "      bytes, the estimated size in bytes, and the unit of measurement for the\n",
    "      estimated size (bytes). This function calculates the fluff ratio, which\n",
    "      indicates the percentage of overhead or additional data in the measured\n",
    "      checkpoint file compared to the estimated size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-------------------+\n",
      "|              Metric               |       Value       |\n",
      "+-----------------------------------+-------------------+\n",
      "| Measured Checkpoint Size (bytes)  |    1542470366     |\n",
      "|   Measured Checkpoint Size (GB)   |    1.542470366    |\n",
      "| Estimated Checkpoint Size (bytes) |   1492051968.0    |\n",
      "|  Estimated Checkpoint Size (GB)   |    1.492051968    |\n",
      "|            Fluff Ratio            | 103.3791314968461 |\n",
      "+-----------------------------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "gpt2_checkpoint_size_measured_in_bytes = 1542470366  # from 'wc -c ckpt.pt'\n",
    "gpt2_checkpoint_size_measured_in_gb = gpt2_checkpoint_size_measured_in_bytes / ByteUnits.GB\n",
    "\n",
    "gpt2_checkpoint_size_estimated_in_bytes = calculate_checkpoint_size(\n",
    "    params_count=gpt2_params_no_bias,\n",
    "    precision=FloatingPointPrecision.FP32,\n",
    "    units=ByteUnits.B,\n",
    ")\n",
    "gpt2_checkpoint_size_estimated_in_gb = gpt2_checkpoint_size_estimated_in_bytes / ByteUnits.GB\n",
    "\n",
    "\n",
    "fluff_ratio = calculate_fluff_ratio(\n",
    "    measured_bytes=gpt2_checkpoint_size_measured_in_bytes,\n",
    "    estimated_bytes=gpt2_checkpoint_size_estimated_in_bytes,\n",
    "    units=ByteUnits.B,\n",
    ")\n",
    "\n",
    "data = [\n",
    "    [\"Measured Checkpoint Size (bytes)\", gpt2_checkpoint_size_measured_in_bytes],\n",
    "    [\"Measured Checkpoint Size (GB)\", gpt2_checkpoint_size_measured_in_gb],\n",
    "    [\"Estimated Checkpoint Size (bytes)\", gpt2_checkpoint_size_estimated_in_bytes],\n",
    "    [\"Estimated Checkpoint Size (GB)\", gpt2_checkpoint_size_estimated_in_gb],\n",
    "    [\"Fluff Ratio\", fluff_ratio]\n",
    "]\n",
    "\n",
    "print(tabulate(data, headers=[\"Metric\", \"Value\"], tablefmt=\"pretty\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Memory Footprint of Loading Model and Optimizer\n",
    "\n",
    "We can roughly understand that a checkpoint represents the amount of memory\n",
    "needed to store not just the model itself (its weights) but also additional\n",
    "information related to the optimizer state when you're using GPUs for deep\n",
    "learning tasks.\n",
    "\n",
    "When loading a model from a checkpoint for further training or inference, the\n",
    "GPU memory must accommodate the model weights and the optimizer state (if\n",
    "continuing training).\n",
    "\n",
    "Below, we estimate the ratio of our GPU memory that will be taken up by\n",
    "the model and optimizer state when loading a GPT-2 model from a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory ratio taken up just for parameters: 3.73%\n"
     ]
    }
   ],
   "source": [
    "def calculate_memory_ratio(checkpoint_size: float, gpu_memory: GPUMemory) -> str:\n",
    "    memory_ratio = checkpoint_size / gpu_memory.value * 100\n",
    "    return f\"Memory ratio taken up just for parameters: {memory_ratio:.2f}%\"\n",
    "\n",
    "print(calculate_memory_ratio(checkpoint_size=gpt2_checkpoint_size_estimated_in_bytes, gpu_memory=GPUMemory.A100_40GB))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming an A100 GPU with roughly 40GB memory, then the code calculates the\n",
    "percentage of the GPU memory that the estimated checkpoint size (in bytes)\n",
    "occupies. This calculation gives an insight into how much of the GPU's memory is\n",
    "dedicated to storing the model's weights and the optimizer's buffers, without\n",
    "considering other memory usages such as activations during forward and backward\n",
    "passes.\n",
    "\n",
    "This percentage is relatively small, implying that most of the GPU memory is\n",
    "actually used for activations. Activations are the intermediate outputs of\n",
    "layers during the forward pass and their gradients during the backward pass,\n",
    "which can consume significant amounts of memory, especially in deep models and\n",
    "with large batch sizes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating FLOPs for a Single Forward Pass\n",
    "\n",
    "In order to estimate FLOPs for a single forward pass, we would first need to\n",
    "define what is a FLOPS.\n",
    "\n",
    "### Basics of Floating Point Numbers\n",
    "\n",
    "-   **Floating Point Representation**: In computers, numbers can be represented\n",
    "    in various formats, and one common format is floating point. This format is\n",
    "    used to represent real numbers (numbers with fractions) using a fixed amount\n",
    "    of memory, allowing for a wide range of values. A floating point number is\n",
    "    composed of a sign, an exponent, and a mantissa (or significand). This\n",
    "    representation can handle very large numbers, very small numbers, and\n",
    "    fractions.\n",
    "-   **Operations on Floating Point Numbers**: Operations on floating point\n",
    "    numbers include addition, subtraction, multiplication, and division. Each of\n",
    "    these operations takes one or more floating point numbers as input and\n",
    "    produces a floating point number as output.\n",
    "\n",
    "### Floating Point Operations (FLOPs)\n",
    "\n",
    "Floating Point Operations, or FLOPs, refer to individual mathematical operations\n",
    "(additions, subtractions, multiplications, divisions) performed on\n",
    "[floating point numbers](https://en.wikipedia.org/wiki/Floating-point_arithmetic).\n",
    "Each operation counts as one FLOP.\n",
    "\n",
    "### Counting FLOPs of Matrix Multiplications\n",
    "\n",
    "In the context of deep learning, many operations are done via matrix\n",
    "multiplications, we will take a look at how to count FLOPs for matrix\n",
    "multiplications next.\n",
    "\n",
    "Deep learning, particularly in neural networks, relies heavily on matrix\n",
    "multiplications. A single matrix multiplication operation involves multiple\n",
    "floating point multiplications and additions.\n",
    "\n",
    "Consider two matrices $\\mathbf{A}$ and $\\mathbf{B}$ of size $m \\times n$ and\n",
    "$n \\times p$:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix}\n",
    "a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n",
    "a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1} & a_{m2} & \\cdots & a_{mn}\n",
    "\\end{bmatrix}_{m \\times n} \\quad \\mathbf{B} = \\begin{bmatrix}\n",
    "b_{11} & b_{12} & \\cdots & b_{1p} \\\\\n",
    "b_{21} & b_{22} & \\cdots & b_{2p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "b_{n1} & b_{n2} & \\cdots & b_{np}\n",
    "\\end{bmatrix}_{n \\times p}\n",
    "$$\n",
    "\n",
    "It is easy to see that if we want to compute the product\n",
    "$\\mathbf{C} = \\mathbf{A} \\mathbf{B}$, the element $c_{ij}$ of $\\mathbf{C}$ is\n",
    "given by:\n",
    "\n",
    "$$\n",
    "c_{ij} = \\sum_{k=1}^{n} a_{ik} b_{kj}\n",
    "$$\n",
    "\n",
    "and therefore there are a total of $m \\times n \\times p$ multiplications and\n",
    "$m \\times (n-1) \\times p$ additions. This amounts to roughly:\n",
    "\n",
    "$$\n",
    "m \\times n \\times p + m \\times (n-1) \\times p \\approx 2 \\times m \\times n \\times p\n",
    "$$\n",
    "\n",
    "FLOPs. Note this is basically because matrix multiplication is a series of dot\n",
    "products, and each dot product involves $n$ multiplications and $n-1$ additions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+---------------------+\n",
      "|       name       |    flops     |      ratio (%)      |\n",
      "+------------------+--------------+---------------------+\n",
      "|  attention/kqv   |  3623878656  | 1.2425508965889174  |\n",
      "| attention/scores |  1610612736  | 0.5522448429284077  |\n",
      "| attention/reduce |  1610612736  | 0.5522448429284077  |\n",
      "|  attention/proj  |  1207959552  | 0.41418363219630583 |\n",
      "|    attention     |  8053063680  | 2.7612242146420387  |\n",
      "|     mlp/ffw1     |  4831838208  | 1.6567345287852233  |\n",
      "|     mlp/ffw2     |  4831838208  | 1.6567345287852233  |\n",
      "|       mlp        |  9663676416  | 3.3134690575704466  |\n",
      "|      block       | 17716740096  |  6.074693272212485  |\n",
      "|   transformer    | 212600881152 |  72.89631926654981  |\n",
      "|      dense       | 79047426048  |  27.10368073345018  |\n",
      "|  forward_total   | 291648307200 |        100.0        |\n",
      "|  backward_total  | 583296614400 |        200.0        |\n",
      "|      total       | 874944921600 |        300.0        |\n",
      "+------------------+--------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "def flops(\n",
    "    num_decoder_blocks: int = 12,\n",
    "    context_length: int = 1024,\n",
    "    n_embd: int = 768,\n",
    "    n_head: int = 12,\n",
    "    ffw_size: int = 3072,\n",
    "    vocab_size: int = 50257,\n",
    ") -> OrderedDict[str, int]:\n",
    "    # we only count Weight FLOPs, all other layers (LayerNorm, Softmax, etc) are effectively irrelevant\n",
    "    # we count actual FLOPs, not MACs. Hence 2* all over the place\n",
    "    # basically for any matrix multiply A (BxC) @ B (CxD) -> (BxD) flops are 2*B*C*D\n",
    "\n",
    "    out = OrderedDict()\n",
    "    head_size = n_embd // n_head\n",
    "\n",
    "    # attention blocks\n",
    "    # 1) the projection to key, query, values\n",
    "    out[\"attention/kqv\"] = 2 * context_length * (n_embd * 3 * n_embd)\n",
    "    # 2) calculating the attention scores\n",
    "    out[\"attention/scores\"] = 2 * context_length * context_length * n_embd\n",
    "    # 3) the reduction of the values (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "    out[\"attention/reduce\"] = 2 * n_head * (context_length * context_length * head_size)\n",
    "    # 4) the final linear projection\n",
    "    out[\"attention/proj\"] = 2 * context_length * (n_embd * n_embd)\n",
    "    out[\"attention\"] = sum(out[\"attention/\" + k] for k in [\"kqv\", \"scores\", \"reduce\", \"proj\"])\n",
    "\n",
    "    # MLP blocks\n",
    "    ffw_size = 4 * n_embd  # feed forward size\n",
    "    out[\"mlp/ffw1\"] = 2 * context_length * (n_embd * ffw_size)\n",
    "    out[\"mlp/ffw2\"] = 2 * context_length * (ffw_size * n_embd)\n",
    "    out[\"mlp\"] = out[\"mlp/ffw1\"] + out[\"mlp/ffw2\"]\n",
    "\n",
    "    # the transformer and the rest of it\n",
    "    out[\"block\"] = out[\"attention\"] + out[\"mlp\"]\n",
    "    out[\"transformer\"] = num_decoder_blocks * out[\"block\"]\n",
    "    out[\"dense\"] = 2 * context_length * (n_embd * vocab_size)\n",
    "\n",
    "    # forward,backward,total\n",
    "    out[\"forward_total\"] = out[\"transformer\"] + out[\"dense\"]\n",
    "    out[\"backward_total\"] = 2 * out[\"forward_total\"]  # use common estimate of bwd = 2*fwd\n",
    "    out[\"total\"] = out[\"forward_total\"] + out[\"backward_total\"]\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "f = flops()\n",
    "flops_total = f[\"forward_total\"]\n",
    "\n",
    "table = [(\"name\", \"flops\", \"ratio (%)\")]\n",
    "for k, v in f.items():\n",
    "    table.append((k, v, v / flops_total * 100))\n",
    "\n",
    "print(tabulate(table, headers=\"firstrow\", tablefmt=\"pretty\", numalign=\"right\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "palm_flops: 875062886400, flops: 874944921600, ratio: 1.0001\n"
     ]
    }
   ],
   "source": [
    "# now here is an estimate copy pasted from the PaLM paper\n",
    "# this formula is often used to calculate MFU (model flops utilization)\n",
    "def palm_flops():\n",
    "    \"\"\"estimate of the model flops following PaLM paper formula\"\"\"\n",
    "    # non-embedding model parameters. note that we do not subtract the\n",
    "    # embedding/token params because those are tied and get used in the last layer.\n",
    "    N = params()['total'] - params()['emebedding/position']\n",
    "    L, H, Q, T = n_layer, n_head, n_embd//n_head, block_size\n",
    "    mf_per_token = 6*N + 12*L*H*Q*T\n",
    "    mf = mf_per_token * block_size\n",
    "    return mf\n",
    "\n",
    "print(f\"palm_flops: {palm_flops():d}, flops: {flops()['total']:d}, ratio: {palm_flops()/flops()['total']:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok they are quite similar, giving some confidence that my math in flops() function was ~ok. Now, A100 is cited at 312TFLOPS bfloat16 on tensor cores. So what is our model flops utilization (MFU)? I trained the model above with a batch_size of 20 and grad_accum of 5, which runs in about 755ms on a single A100 GPU. We get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction of A100 used: 37.14%\n"
     ]
    }
   ],
   "source": [
    "# here is what we currently roughly measure\n",
    "batch_size = 20 * 5 # 5 is grad_accum, so total batch size is 100\n",
    "measured_time = 0.755 # in seconds per iteration\n",
    "measured_throughput = batch_size / measured_time\n",
    "flops_achieved = f['total'] * measured_throughput\n",
    "\n",
    "# A100 is cited to be 312 TFLOPS of bloat16 running on tensor cores\n",
    "a100_flops_promised = 312e12\n",
    "\n",
    "# the fraction of the A100 that we are using:\n",
    "print(f\"fraction of A100 used: {flops_achieved / a100_flops_promised * 100:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, we'd prefer to be somewhere around 50%+, and not just for a single GPU but for an entire DDP run. So we still have some work to do, but at least we're within a factor of ~2X of what is achievable with this GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time needed to train the model: 3.46 days\n"
     ]
    }
   ],
   "source": [
    "# Finally let's check out the 6ND approximation as total cost of training in FLOPs\n",
    "model_size = params()['total'] # this is number of parameters, N\n",
    "tokens_num = 300e9 # 300B tokens, this is dataset size in tokens, D\n",
    "a100_flops = 312e12 # 312 TFLOPS\n",
    "assumed_mfu = 0.3 # assume this model flops utilization (take the current 37% from above and add some DDP overhead)\n",
    "flops_throughput = a100_flops * 8 * assumed_mfu # assume an 8XA100 node at 30% utilization\n",
    "flops_needed = 6 * model_size * tokens_num # 6ND\n",
    "time_needed_s = flops_needed / flops_throughput # in seconds\n",
    "print(f\"time needed to train the model: {time_needed_s/3600/24:.2f} days\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not a bad estimate at all. I trained this model and it converged in roughly 4 days. Btw as a good reference for where 6ND comes from and some intuition around it I recommend [Dzmitry's post](https://medium.com/@dzmitrybahdanau/the-flops-calculus-of-language-model-training-3b19c1f025e4)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, FLOPs are just one constraint, the other that we have to keep a close track of is the memory bandwidth. TODO estimate LOAD/STORE costs of our model later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f5833218766b48e6e35e4452ee875aac0e2188d05bbe5298f2c62b79f08b222"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
