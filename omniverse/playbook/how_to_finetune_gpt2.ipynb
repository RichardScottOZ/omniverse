{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import copy\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import DictConfig, ListConfig\n",
    "from omegaconf import OmegaConf as om\n",
    "from tqdm import tqdm\n",
    "\n",
    "from omnivault._types._alias import Missing\n",
    "from omnivault._types._sentinel import MISSING\n",
    "from omnivault.core.logger import RichLogger\n",
    "from omnivault.transformer.config.composer import Composer, DataConfig\n",
    "from omnivault.transformer.config.constants import MaybeConstant\n",
    "from omnivault.transformer.config.criterion import CRITERION_REGISTRY\n",
    "from omnivault.transformer.config.decoder import DecoderConfig\n",
    "from omnivault.transformer.config.generator import GeneratorConfig\n",
    "from omnivault.transformer.config.global_ import MaybeGlobal\n",
    "from omnivault.transformer.config.logger import LoggerConfig\n",
    "from omnivault.transformer.config.optim import OPTIMIZER_REGISTRY\n",
    "from omnivault.transformer.config.scheduler import SCHEDULER_REGISTRY, LambdaLRConfig\n",
    "from omnivault.transformer.config.trainer import TrainerConfig\n",
    "from omnivault.transformer.core.dataset import AdderDataset, create_loader, split_dataset\n",
    "from omnivault.transformer.core.optim import apply_weight_decay_to_different_param_groups\n",
    "from omnivault.transformer.core.scheduler import noam_lr_decay\n",
    "from omnivault.transformer.core.state import State\n",
    "from omnivault.transformer.core.tokenizer import AdderTokenizer\n",
    "from omnivault.transformer.core.trainer import Trainer, TrainerEvent\n",
    "from omnivault.transformer.core.vocabulary import AdderVocabulary\n",
    "from omnivault.transformer.decoder.core import GPTDecoder\n",
    "from omnivault.transformer.utils.config_utils import load_yaml_config, merge_configs\n",
    "from omnivault.transformer.utils.general_utils import create_directory, download_file, validate_and_cleanup\n",
    "from omnivault.transformer.utils.reproducibility import seed_all\n",
    "from omnivault.transformer.utils.visualization import save_plot_history\n",
    "from omnivault.transformer.config.composer import Composer\n",
    "from omnivault.transformer.config.criterion import CRITERION_REGISTRY\n",
    "from omnivault.transformer.config.decoder import (\n",
    "    AddNormConfig,\n",
    "    DecoderBlockConfig,\n",
    "    DecoderConfig,\n",
    "    MultiHeadedAttentionConfig,\n",
    "    PositionwiseFeedForwardConfig,\n",
    ")\n",
    "from omnivault.transformer.config.generator import GeneratorConfig\n",
    "from omnivault.transformer.config.optim import OPTIMIZER_REGISTRY\n",
    "from omnivault.transformer.config.scheduler import SCHEDULER_REGISTRY\n",
    "from omnivault.transformer.config.trainer import TrainerConfig\n",
    "from omnivault.transformer.core.dataset import (\n",
    "    construct_dummy_batch_future_masks,\n",
    "    construct_dummy_batch_target_padding_masks,\n",
    ")\n",
    "from omnivault.transformer.core.state import State\n",
    "from omnivault.transformer.core.trainer import Trainer, TrainerEvent\n",
    "from omnivault.transformer.decoder.core import GPTDecoder, GPTDecoderBlock\n",
    "from omnivault.transformer.modules.attention.core import ScaledDotProductAttention\n",
    "from omnivault.transformer.utils.general_utils import cleanup\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v:  auto\n"
     ]
    }
   ],
   "source": [
    "trainer_config = TrainerConfig(\n",
    "    device=\"auto\",\n",
    "    max_epochs=10,\n",
    "    eval_every_n_steps=10000,\n",
    "    log_every_n_steps=10000,\n",
    "    # use_amp=True,\n",
    "    autocast_config={\"enabled\": False}, # , \"dtype\": torch.float16, \"cache_enabled\": True},\n",
    "    # scaler_config={\n",
    "    #     \"enabled\": True,\n",
    "    #     \"init_scale\": 2.0**16,\n",
    "    #     \"growth_factor\": 2.0,\n",
    "    #     \"backoff_factor\": 0.5,\n",
    "    #     \"growth_interval\": 2000,\n",
    "    # },\n",
    "    # gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    clip_grad_norm={\"max_norm\": 1.0, \"norm_type\": 2.0, \"error_if_nonfinite\": False, \"foreach\": None},\n",
    "    apply_weight_decay_to_different_param_groups=False,\n",
    "    step_scheduler_on_batch_or_epoch=\"epoch\",\n",
    "    save_dir=\"./data/reversal/checkpoints\",\n",
    "    save_every_epoch=False,\n",
    "    save_best_only=True,\n",
    "    monitor=\"valid_this_epoch_average_loss\",\n",
    "    mode=\"min\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_config.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(2024, True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to shift input and output in decoder only whereas in encoder only there is no need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseDataset(Dataset):\n",
    "\n",
    "    def __init__(self, num_categories, seq_len, size):\n",
    "        super().__init__()\n",
    "        self.num_categories = num_categories\n",
    "        self.seq_len = seq_len\n",
    "        self.size = size\n",
    "\n",
    "        self.data = torch.randint(self.num_categories, size=(self.size, self.seq_len))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp_data = self.data[idx]\n",
    "        labels = torch.flip(inp_data, dims=(0,))\n",
    "        return inp_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dummy_batch_future_masks(batch_size: int, seq_len: int) -> torch.BoolTensor:\n",
    "    \"\"\"Broadcast mask from shape (L, L) to (B, L, L) then (B, 1, L, L).\"\"\"\n",
    "    # Create a mask for a single sequence\n",
    "    mask = torch.ones((seq_len, seq_len), dtype=torch.bool)\n",
    "    mask = mask.contiguous()\n",
    "    # broadcast mask from shape (L, L) to (B, L, L)\n",
    "    masks = mask.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "    # broadcast mask from shape (B, L, L) to (B, 1, L, L)\n",
    "    masks = masks.unsqueeze(1)\n",
    "    return torch.BoolTensor(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "    batch: List[Tuple[torch.Tensor, torch.Tensor]]\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    sources, targets = zip(*batch)\n",
    "\n",
    "    sources = torch.stack(sources)\n",
    "    targets = torch.stack(targets)\n",
    "\n",
    "    batch_size, seq_len = targets.size(0), targets.size(1)\n",
    "\n",
    "    future_masks = construct_dummy_batch_future_masks(batch_size, seq_len)\n",
    "    target_padding_masks = construct_dummy_batch_target_padding_masks(batch_size, seq_len)\n",
    "\n",
    "    return sources, targets, future_masks, target_padding_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 16\n",
    "NUM_CATEGORIES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = partial(ReverseDataset, 10, 16)\n",
    "train_loader = DataLoader(dataset(50000), batch_size=128, shuffle=True, drop_last=True, pin_memory=True, collate_fn=custom_collate_fn)\n",
    "val_loader   = DataLoader(dataset(1000), batch_size=128, collate_fn=custom_collate_fn)\n",
    "test_loader  = DataLoader(dataset(10000), batch_size=128, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_data, labels = train_loader.dataset[0]\n",
    "print(\"Input data:\", inp_data)\n",
    "print(\"Labels:    \", labels)\n",
    "\n",
    "for i, (inp_data, labels, future_masks, target_padding_masks) in enumerate(train_loader):\n",
    "    print(\"Batch\", i)\n",
    "    print(\"Input data:\", inp_data)\n",
    "    print(\"Labels:    \", labels)\n",
    "    print(\"Future masks:\", future_masks)\n",
    "    print(\"Target padding masks:\", target_padding_masks)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDecoderReverse(GPTDecoder):\n",
    "    def __init__(self, config: DecoderConfig) -> None:\n",
    "        super().__init__(config)\n",
    "        # fmt: off\n",
    "        self.d_model       : int           = config.d_model\n",
    "        self.tok_embed     : nn.Embedding  = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.pos_embed     : nn.Parameter  = nn.Parameter(torch.zeros(1, config.context_length, config.d_model))\n",
    "        self.decoder_blocks: nn.ModuleList = nn.ModuleList([GPTDecoderBlock(config) for _ in range(config.num_decoder_blocks)]) # PyTorch did not make ModuleList a proper container, maybe open a PR to make it inherit Generic[T]???\n",
    "\n",
    "        self.dropout       : nn.Dropout    = nn.Dropout(config.dropout)\n",
    "        self.layer_norm    : nn.LayerNorm  = nn.LayerNorm(config.d_model)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(config.d_model, config.d_model),\n",
    "            nn.LayerNorm(config.d_model),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.d_model, NUM_CATEGORIES)\n",
    "        )\n",
    "        # fmt: on\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for parameter_name, parameter in self.named_parameters():\n",
    "            if parameter_name.endswith(\"context_projection.weight\"):\n",
    "                mean = 0.0\n",
    "                std_dev = 0.02 / torch.sqrt(torch.tensor(2 * config.num_decoder_blocks, dtype=torch.float))\n",
    "                torch.nn.init.normal_(parameter, mean=mean, std=std_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_on_reverse_dataset(trainer: Trainer, num_batches_to_eval: int | None = None) -> None:\n",
    "    generator_config = trainer.composer.generator\n",
    "\n",
    "    model = trainer.model\n",
    "    model.eval()\n",
    "\n",
    "    dataloader = trainer.valid_loader  # Assuming you've set your test_loader to use the ReverseDataset\n",
    "    assert dataloader is not None\n",
    "\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    progress_bar = tqdm(enumerate(dataloader, start=1), desc=\"Evaluating on Reverse Dataset\", leave=False)\n",
    "    for batch_index, (inputs, labels, target_padding_masks, future_masks) in progress_bar:\n",
    "        inputs, labels = inputs.to(trainer.device), labels.to(trainer.device)\n",
    "        target_padding_masks = target_padding_masks.to(trainer.device)\n",
    "        future_masks = future_masks.to(trainer.device)\n",
    "\n",
    "        logits = model(inputs, target_padding_masks=target_padding_masks, future_masks=future_masks)\n",
    "        predictions = logits.argmax(dim=-1)\n",
    "\n",
    "        total_correct += (predictions == labels).float().sum().item()\n",
    "        total_samples += labels.numel()\n",
    "\n",
    "        if num_batches_to_eval and batch_index >= num_batches_to_eval:\n",
    "            print(\"Early stopping evaluation.\")\n",
    "            break\n",
    "\n",
    "    overall_accuracy = total_correct / total_samples\n",
    "    trainer.logger.info(\"Overall Accuracy on Reverse Dataset: {:.4f}\".format(overall_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_self_attention_mha_config = MultiHeadedAttentionConfig(\n",
    "    attention=ScaledDotProductAttention(), d_model=32, H=1, dropout=0.0\n",
    ")\n",
    "\n",
    "feed_forward_config = PositionwiseFeedForwardConfig(\n",
    "    d_model=32, d_ff=32 * 2, activation=nn.GELU(approximate=\"tanh\"), dropout=0.0, bias=True\n",
    ")\n",
    "\n",
    "add_norm_config_1 = AddNormConfig(feature_dim=32, dropout=0.0)\n",
    "add_norm_config_2 = AddNormConfig(feature_dim=32, dropout=0.0)\n",
    "\n",
    "# Create DecoderBlockConfig\n",
    "decoder_block_config = DecoderBlockConfig(\n",
    "    masked_self_attention_mha=masked_self_attention_mha_config,\n",
    "    feed_forward=feed_forward_config,\n",
    "    add_norm_1=add_norm_config_1,\n",
    "    add_norm_2=add_norm_config_2,\n",
    ")\n",
    "\n",
    "# Create the overall DecoderConfig\n",
    "model_config = DecoderConfig(\n",
    "    d_model=32,\n",
    "    vocab_size=NUM_CATEGORIES,\n",
    "    context_length=SEQ_LEN,\n",
    "    num_decoder_blocks=1,\n",
    "    dropout=0.0,\n",
    "    decoder_block=decoder_block_config,\n",
    ")\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "\n",
    "optimizer_config_cls = OPTIMIZER_REGISTRY[\"torch.optim.Adam\"]\n",
    "optimizer_pydantic_config = optimizer_config_cls(name=\"torch.optim.Adam\", lr=4e-3)\n",
    "\n",
    "criterion_config_cls = CRITERION_REGISTRY[\"torch.nn.CrossEntropyLoss\"]\n",
    "criterion_pydantic_config = criterion_config_cls(name=\"torch.nn.CrossEntropyLoss\")\n",
    "\n",
    "scheduler_config_cls = SCHEDULER_REGISTRY[\"torch.optim.lr_scheduler.CosineAnnealingLR\"]\n",
    "scheduler_pydantic_config = scheduler_config_cls(name=\"torch.optim.lr_scheduler.CosineAnnealingLR\", T_max=10)\n",
    "\n",
    "trainer_config = TrainerConfig(\n",
    "    device=\"cpu\",\n",
    "    max_epochs=10,\n",
    "    eval_every_n_steps=10000,\n",
    "    log_every_n_steps=10000,\n",
    "    # use_amp=True,\n",
    "    autocast_config={\"enabled\": False}, # , \"dtype\": torch.float16, \"cache_enabled\": True},\n",
    "    # scaler_config={\n",
    "    #     \"enabled\": True,\n",
    "    #     \"init_scale\": 2.0**16,\n",
    "    #     \"growth_factor\": 2.0,\n",
    "    #     \"backoff_factor\": 0.5,\n",
    "    #     \"growth_interval\": 2000,\n",
    "    # },\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    clip_grad_norm={\"max_norm\": 1.0, \"norm_type\": 2.0, \"error_if_nonfinite\": False, \"foreach\": None},\n",
    "    apply_weight_decay_to_different_param_groups=False,\n",
    "    step_scheduler_on_batch_or_epoch=\"epoch\",\n",
    "    save_dir=\"./data/reversal/checkpoints\",\n",
    "    save_every_epoch=False,\n",
    "    save_best_only=True,\n",
    "    monitor=\"valid_this_epoch_average_loss\",\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "generator_config = GeneratorConfig(temperature=1.0, max_tokens=SEQ_LEN, greedy=False, top_k=10, top_p=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composer = Composer(\n",
    "    model=model_config,\n",
    "    optimizer=optimizer_pydantic_config,\n",
    "    criterion=criterion_pydantic_config,\n",
    "    scheduler=scheduler_pydantic_config,\n",
    "    trainer=trainer_config,\n",
    "    generator=generator_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTDecoderReverse(model_config).to(composer.trainer.device)\n",
    "optimizer = optimizer_pydantic_config.build(params=model.parameters())\n",
    "criterion = criterion_pydantic_config.create_instance()\n",
    "\n",
    "composer.scheduler = scheduler_pydantic_config\n",
    "scheduler = scheduler_pydantic_config.build(optimizer=optimizer)\n",
    "\n",
    "#composer.pretty_print()\n",
    "\n",
    "\n",
    "state = State(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    # vocabulary=vocabulary,\n",
    "    # tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "device = composer.trainer.device\n",
    "trainer = Trainer(\n",
    "    state=state,\n",
    "    composer=composer,\n",
    "    logger=None,\n",
    "    device=device,  # type: ignore[arg-type]\n",
    ")\n",
    "\n",
    "trainer.add_callback(\n",
    "    TrainerEvent.ON_VALID_EPOCH_END.value,\n",
    "    lambda trainer: evaluate_on_reverse_dataset(trainer, num_batches_to_eval=None),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_trained_state = trainer.fit(train_loader=train_loader, valid_loader=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on importance of LR. lol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "last_decoder_block = _trained_state.model.decoder_blocks[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_decoder_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_self_attention_mha = last_decoder_block.masked_self_attention_mha\n",
    "context_vector, attention_weights = masked_self_attention_mha.context_vector, masked_self_attention_mha.attention_weights\n",
    "\n",
    "# Number of heads\n",
    "num_heads = attention_weights.size(1)\n",
    "\n",
    "print(\"Context vector shape:\", context_vector.size())\n",
    "print(\"Attention weights shape:\", attention_weights.size())\n",
    "print(\"Number of heads:\", num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data: tensor([2, 0, 4, 0, 3, 0, 0, 1, 3, 9, 7, 4, 1, 8, 0, 8])\n",
    "# Labels:     tensor([8, 0, 8, 1, 4, 7, 9, 3, 1, 0, 0, 3, 0, 4, 0, 2])\n",
    "\n",
    "# Labels for each character in the sequence, including BOS\n",
    "input = \"2040300139741808\"\n",
    "label = input[::-1]\n",
    "\n",
    "# Loop over each head and plot its heatmap\n",
    "for head in range(num_heads):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    # Extract attention weights for the last sample in the last batch for this head\n",
    "    attention_matrix = attention_weights[-1, head, :, :].detach().numpy()\n",
    "\n",
    "    sns.heatmap(attention_matrix, annot=True, cmap='viridis', xticklabels=input, yticklabels=input)\n",
    "    plt.title(f\"Attention Weights Heatmap for '{input}' - Head {head+1}\")\n",
    "    plt.xlabel(\"Keys\")\n",
    "    plt.ylabel(\"Queries\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap diagonal is correct because in the above image, we can\n",
    "treat it as a 2 by 2 matrix, where the first row is the first token from query,\n",
    "interacting with every other token as keys. So if first token is 2 and last token is 8,\n",
    "then it should be matching each other because of the \"reversal\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omniverse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
