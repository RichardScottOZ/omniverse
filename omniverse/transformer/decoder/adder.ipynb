{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Table of contents**<a id='toc0_'></a>    \n",
        "- [Decoder (GPT)](#toc1_)    \n",
        "  - [Constants](#toc1_1_)    \n",
        "  - [Reproducibility](#toc1_2_)    \n",
        "  - [Utilities](#toc1_3_)    \n",
        "  - [Config](#toc1_4_)    \n",
        "  - [Dataset](#toc1_5_)    \n",
        "    - [Refactor](#toc1_5_1_)    \n",
        "  - [Construct Batches, Collate Function and DataLoader](#toc1_6_)    \n",
        "  - [DataLoader](#toc1_7_)    \n",
        "    - [Example](#toc1_7_1_)    \n",
        "  - [Model](#toc1_8_)    \n",
        "    - [Masks](#toc1_8_1_)    \n",
        "      - [Padding Mask](#toc1_8_1_1_)    \n",
        "      - [Look-Ahead Mask (Future Mask)](#toc1_8_1_2_)    \n",
        "      - [Using Both Masks in the Decoder](#toc1_8_1_3_)    \n",
        "  - [2-Digits Addition](#toc1_9_)    \n",
        "  - [Adder Decoder Walkthrough](#toc1_10_)    \n",
        "    - [Target Padding Mask (`target_padding_mask`)](#toc1_10_1_)    \n",
        "    - [Future Mask (`future_mask`)](#toc1_10_2_)    \n",
        "    - [Example of Source Padding and Future Masks](#toc1_10_3_)    \n",
        "      - [First Sample First Token](#toc1_10_3_1_)    \n",
        "      - [First Sample Fourth Token](#toc1_10_3_2_)    \n",
        "    - [Further Add a Singleton Dimension in Masks](#toc1_10_4_)    \n",
        "    - [MultiHeadAttention](#toc1_10_5_)    \n",
        "      - [A Primer](#toc1_10_5_1_)    \n",
        "      - [An Example](#toc1_10_5_2_)    \n",
        "    - [AddNorm (Residual Connection + Layer Normalization)](#toc1_10_6_)    \n",
        "      - [Residual Block](#toc1_10_6_1_)    \n",
        "      - [Layer Normalization](#toc1_10_6_2_)    \n",
        "      - [Combining Both](#toc1_10_6_3_)    \n",
        "    - [How Loss is Computed?](#toc1_10_7_)    \n",
        "  - [Potential to use Module Dict?](#toc1_11_)    \n",
        "  - [Training with GPT-like Model](#toc1_12_)    \n",
        "    - [Loss Computation](#toc1_12_1_)    \n",
        "    - [Example](#toc1_12_2_)    \n",
        "    - [Confusion: Training versus Inference](#toc1_12_3_)    \n",
        "    - [Training vs Inference](#toc1_12_4_)    \n",
        "  - [Questions](#toc1_13_)    \n",
        "    - [Why Masked == 0 in some?](#toc1_13_1_)    \n",
        "    - [what is the reason of setting the attention scores's mask indexes to negative infinity](#toc1_13_2_)    \n",
        "    - [Why do we need both ignore index in Loss and also negative infinity mask](#toc1_13_3_)    \n",
        "    - [Target and Preds/Logits Shape](#toc1_13_4_)    \n",
        "    - [Why do we flatten prediction and target (logits)?](#toc1_13_5_)    \n",
        "      - [Background](#toc1_13_5_1_)    \n",
        "      - [Traditional Loss Computation](#toc1_13_5_2_)    \n",
        "      - [Why Flatten?](#toc1_13_5_3_)    \n",
        "      - [Step-by-step Flattening](#toc1_13_5_4_)    \n",
        "    - [Why sometimes unsqueeze masks?](#toc1_13_6_)    \n",
        "    - [Why does sequence length differ for source and target, usually I thought it is just all L, same.](#toc1_13_7_)    \n",
        "    - [Am i right to assume that the core idea of autoregressive model like decoder only (GPT like) is that for a given sample, there will eventually be L rows where L is the seq length, and therefore I can intuitively view it as 1 sample having L samples, since for each row, we will compute the loss. Am I right in my understanding? Do not hesistate to correct me.](#toc1_13_8_)    \n",
        "    - [QKV Again](#toc1_13_9_)    \n",
        "      - [Background and Assumptions](#toc1_13_9_1_)    \n",
        "      - [Context Vector](#toc1_13_9_2_)    \n",
        "      - [Query (Q), Key (K), and Value (V)](#toc1_13_9_3_)    \n",
        "      - [Mathematical Description](#toc1_13_9_4_)    \n",
        "  - [TODO](#toc1_14_)    \n",
        "  - [References and Further Readings](#toc1_15_)    \n",
        "\n",
        "<!-- vscode-jupyter-toc-config\n",
        "\tnumbering=false\n",
        "\tanchor=true\n",
        "\tflat=false\n",
        "\tminLevel=1\n",
        "\tmaxLevel=6\n",
        "\t/vscode-jupyter-toc-config -->\n",
        "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a id='toc1_'></a>[Decoder (GPT)](#toc0_)\n",
        "\n",
        "In one of the examples provided with minGPT, Karpathy demonstrates training a\n",
        "GPT model to learn the addition of two numbers presented as strings. This is a\n",
        "simple task designed to illustrate how a transformer model can be trained on a\n",
        "sequence-to-sequence task where the input is a sequence of characters\n",
        "representing an addition operation (like \"12 + 35\") and the output is the\n",
        "sequence of characters representing the result of the addition (like \"47\").\n",
        "\n",
        "This example serves as a proof-of-concept to show that transformer models, which\n",
        "are often used for language-related tasks, can learn other patterns or\n",
        "\"languages,\" such as the \"language\" of arithmetic. It's a creative way to\n",
        "showcase the flexibility of the GPT architecture beyond natural language\n",
        "processing tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Cvk1SmuCrdzK"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, Subset, random_split\n",
        "import rich\n",
        "import torch\n",
        "# from d2l import torch as d2l\n",
        "from rich.pretty import pprint\n",
        "from torch import nn\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.lr_scheduler import LRScheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "def find_root_dir(current_path: Path = Path.cwd(), marker: str = '.git') -> Path | None:\n",
        "    \"\"\"\n",
        "    Find the root directory by searching for a directory or file that serves as a\n",
        "    marker.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    current_path : Path\n",
        "        The starting path to search from.\n",
        "    marker : str\n",
        "        The name of the file or directory that signifies the root.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Path or None\n",
        "        The path to the root directory. Returns None if the marker is not found.\n",
        "    \"\"\"\n",
        "    current_path = current_path.resolve()\n",
        "    for parent in current_path.parents:\n",
        "        if (parent / marker).exists():\n",
        "            return parent\n",
        "    return None\n",
        "\n",
        "current_file_path = Path(os.getcwd())\n",
        "root_dir          = find_root_dir(current_file_path, marker='omnivault')\n",
        "\n",
        "if root_dir is not None:\n",
        "    sys.path.append(str(root_dir))\n",
        "    from omnivault._types._alias import Accuracy, Loss\n",
        "    from omnivault.transformer.config.composer import Composer, DataConfig\n",
        "    from omnivault.transformer.config.constants import MaybeConstant\n",
        "    from omnivault.transformer.config.decoder import (\n",
        "        AddNormConfig,\n",
        "        DecoderBlockConfig,\n",
        "        DecoderConfig,\n",
        "        MultiHeadedAttentionConfig,\n",
        "        PositionwiseFeedForwardConfig,\n",
        "    )\n",
        "    from omnivault.transformer.config.global_ import MaybeGlobal\n",
        "    from omnivault.transformer.config.optim import AdamConfig, OptimizerConfig\n",
        "    from omnivault.transformer.core.dataset import AdderDataset, create_loader, split_dataset\n",
        "    from omnivault.transformer.core.trainer import Trainer\n",
        "    from omnivault.transformer.core.vocabulary import AdderVocabulary\n",
        "    from omnivault.transformer.decoder.core import GPTDecoder\n",
        "    from omnivault.transformer.modules.attention.core import ScaledDotProductAttention\n",
        "    from omnivault.transformer.utils.reproducibility import seed_all\n",
        "    from omnivault.transformer.core.tokenizer import AdderTokenizer\n",
        "else:\n",
        "    raise ImportError(\"Root directory not found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[PosixPath('/Users/gaohn/omniverse/omniverse/omniverse/transformer'),\n",
              " PosixPath('/Users/gaohn/omniverse/omniverse/omniverse'),\n",
              " PosixPath('/Users/gaohn/omniverse/omniverse'),\n",
              " PosixPath('/Users/gaohn/omniverse'),\n",
              " PosixPath('/Users/gaohn'),\n",
              " PosixPath('/Users'),\n",
              " PosixPath('/')]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(current_file_path.parents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Composer</span><span style=\"font-weight: bold\">(</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">constants</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MaybeConstant</span><span style=\"font-weight: bold\">(</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">NUM_DIGITS</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">TOKENS</span>=<span style=\"font-weight: bold\">[</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'0'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'1'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'3'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'4'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'5'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'6'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'7'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'8'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'9'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'+'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'*'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'-'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'='</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;BOS&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;EOS&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;PAD&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;UNK&gt;'</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">]</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">)</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">global_</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MaybeGlobal</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">seed</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">42</span>, <span style=\"color: #808000; text-decoration-color: #808000\">debug</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">data</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DataConfig</span><span style=\"font-weight: bold\">(</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">context_length</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">dataset_name</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">dataset_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">dataset_path</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">dataset_dir</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">dataset_url</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">split</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2</span><span style=\"font-weight: bold\">]</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">collate_fn</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'batch_first'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'pad_token_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"font-weight: bold\">}</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">train_loader</span>=<span style=\"font-weight: bold\">{</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'shuffle'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'num_workers'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'pin_memory'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'drop_last'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">}</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">valid_loader</span>=<span style=\"font-weight: bold\">{</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'shuffle'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'num_workers'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'pin_memory'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'drop_last'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">}</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">test_loader</span>=<span style=\"font-weight: bold\">{</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'shuffle'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'num_workers'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'pin_memory'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'drop_last'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">}</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">)</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #800080; text-decoration-color: #800080\">MISSING</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">optimizer</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AdamConfig</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'torch.optim.Adam'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">lr</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2</span>, <span style=\"color: #808000; text-decoration-color: #808000\">betas</span>=<span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.98</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">eps</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-09</span><span style=\"font-weight: bold\">)</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">criterion</span>=<span style=\"color: #800080; text-decoration-color: #800080\">MISSING</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">trainer</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TrainerConfig</span><span style=\"font-weight: bold\">(</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">device</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'auto'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">max_epochs</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">eval_interval</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">save_dir</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'checkpoints'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">save_every_epoch</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">)</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">generator</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GeneratorConfig</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">max_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000</span>, <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">greedy</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">top_k</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>\n",
              "<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mComposer\u001b[0m\u001b[1m(\u001b[0m\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33mconstants\u001b[0m=\u001b[1;35mMaybeConstant\u001b[0m\u001b[1m(\u001b[0m\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[33mNUM_DIGITS\u001b[0m=\u001b[1;36m2\u001b[0m,\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[33mTOKENS\u001b[0m=\u001b[1m[\u001b[0m\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'0'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'1'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'2'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'3'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'4'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'5'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'6'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'7'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'8'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'9'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'+'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'*'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'-'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'='\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32mBOS\u001b[0m\u001b[32m>'\u001b[0m\u001b[39m,\u001b[0m\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'<EOS>'\u001b[0m\u001b[39m,\u001b[0m\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'<PAD>'\u001b[0m\u001b[39m,\u001b[0m\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'<UNK\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[1m]\u001b[0m\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1m)\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33mglobal_\u001b[0m=\u001b[1;35mMaybeGlobal\u001b[0m\u001b[1m(\u001b[0m\u001b[33mseed\u001b[0m=\u001b[1;36m42\u001b[0m, \u001b[33mdebug\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33mdata\u001b[0m=\u001b[1;35mDataConfig\u001b[0m\u001b[1m(\u001b[0m\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[33mcontext_length\u001b[0m=\u001b[1;36m128\u001b[0m,\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[33mdataset_name\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[33mdataset_size\u001b[0m=\u001b[1;36m2\u001b[0m,\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[33mdataset_path\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[33mdataset_dir\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[33mdataset_url\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[33msplit\u001b[0m=\u001b[1m[\u001b[0m\u001b[1;36m0.7\u001b[0m, \u001b[1;36m0.1\u001b[0m, \u001b[1;36m0.2\u001b[0m\u001b[1m]\u001b[0m,\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[33mcollate_fn\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'batch_first'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \u001b[32m'pad_token_id'\u001b[0m: \u001b[1;36m16\u001b[0m\u001b[1m}\u001b[0m,\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[33mtrain_loader\u001b[0m=\u001b[1m{\u001b[0m\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'batch_size'\u001b[0m: \u001b[1;36m32\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'shuffle'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'num_workers'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'pin_memory'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'drop_last'\u001b[0m: \u001b[3;91mFalse\u001b[0m\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[1m}\u001b[0m,\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[33mvalid_loader\u001b[0m=\u001b[1m{\u001b[0m\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'batch_size'\u001b[0m: \u001b[1;36m32\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'shuffle'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'num_workers'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'pin_memory'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'drop_last'\u001b[0m: \u001b[3;91mFalse\u001b[0m\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[1m}\u001b[0m,\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[33mtest_loader\u001b[0m=\u001b[1m{\u001b[0m\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'batch_size'\u001b[0m: \u001b[1;36m32\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'shuffle'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'num_workers'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'pin_memory'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'drop_last'\u001b[0m: \u001b[3;91mFalse\u001b[0m\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[1m}\u001b[0m\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1m)\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33mmodel\u001b[0m=\u001b[35mMISSING\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33moptimizer\u001b[0m=\u001b[1;35mAdamConfig\u001b[0m\u001b[1m(\u001b[0m\u001b[33mname\u001b[0m=\u001b[32m'torch.optim.Adam'\u001b[0m, \u001b[33mlr\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33mbetas\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m0.9\u001b[0m, \u001b[1;36m0.98\u001b[0m\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-09\u001b[0m\u001b[1m)\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33mcriterion\u001b[0m=\u001b[35mMISSING\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33mtrainer\u001b[0m=\u001b[1;35mTrainerConfig\u001b[0m\u001b[1m(\u001b[0m\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[33mdevice\u001b[0m=\u001b[32m'auto'\u001b[0m,\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[33mnum_epochs\u001b[0m=\u001b[1;36m10\u001b[0m,\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[33meval_interval\u001b[0m=\u001b[1;36m1\u001b[0m,\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[33msave_dir\u001b[0m=\u001b[32m'checkpoints'\u001b[0m,\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[33msave_every_epoch\u001b[0m=\u001b[3;91mFalse\u001b[0m\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1m)\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33mgenerator\u001b[0m=\u001b[1;35mGeneratorConfig\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmax_tokens\u001b[0m=\u001b[1;36m1000\u001b[0m, \u001b[33mtemperature\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33mgreedy\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mtop_k\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m\n",
              "\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "constants: MaybeConstant = MaybeConstant(NUM_DIGITS=2, TOKENS=[\n",
        "            \"0\",\n",
        "            \"1\",\n",
        "            \"2\",\n",
        "            \"3\",\n",
        "            \"4\",\n",
        "            \"5\",\n",
        "            \"6\",\n",
        "            \"7\",\n",
        "            \"8\",\n",
        "            \"9\",\n",
        "            \"+\",\n",
        "            \"*\",\n",
        "            \"-\",\n",
        "            \"=\",\n",
        "            \"<BOS>\",\n",
        "            \"<EOS>\",\n",
        "            \"<PAD>\",\n",
        "            \"<UNK>\",\n",
        "        ]\n",
        ")\n",
        "global_: MaybeGlobal = MaybeGlobal(seed=42, dataset_size=2)\n",
        "data_config: DataConfig = DataConfig()\n",
        "optimizer_config = AdamConfig(name=\"torch.optim.Adam\", lr=0.2, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "config = Composer(constants=constants, global_=global_, data=data_config, optimizer=optimizer_config)\n",
        "pprint(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(omnivault.transformer.config.optim.AdamConfig, True, True)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(optimizer_config), isinstance(optimizer_config, OptimizerConfig), issubclass(type(optimizer_config), OptimizerConfig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_1_'></a>[Constants](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DYDfkBu2rdzM"
      },
      "outputs": [],
      "source": [
        "DEBUG  = True\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_2_'></a>[Reproducibility](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mWV1lAHkrdzN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/gaohn/omniverse/omniverse/omnivault/transformer/utils/reproducibility.py:69: UserWarning: Deterministic mode is activated. This will negatively impact performance.\n",
            "  configure_deterministic_mode()\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seed_all(config.global_.seed, seed_torch=True, set_torch_deterministic=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_3_'></a>[Utilities](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DQ7N8MfGrdzP"
      },
      "outputs": [],
      "source": [
        "def forward_hook(\n",
        "    module: nn.Module, input: Tuple[torch.Tensor], output: torch.Tensor\n",
        ") -> None:\n",
        "    \"\"\"Custom hook function to print layer information.\"\"\"\n",
        "    if not hasattr(module, \"has_printed\"):\n",
        "        module.has_printed = False\n",
        "\n",
        "    if not module.has_printed:\n",
        "        print(f\"Layer: {module.__class__.__name__}\")\n",
        "        print(f\"Input shape: {str(input[0].shape)}\")\n",
        "        print(f\"Output shape: {str(output.shape)}\")\n",
        "        module.has_printed = True\n",
        "\n",
        "\n",
        "def are_both_models_same(state_dict_1, state_dict_2):\n",
        "    # Check if both models have the same keys\n",
        "    if set(state_dict_1.keys()) != set(state_dict_2.keys()):\n",
        "        return False\n",
        "\n",
        "    # Check if all tensors have the same shape and values\n",
        "    for key in state_dict_1.keys():\n",
        "        if state_dict_1[key].shape != state_dict_2[key].shape:\n",
        "            return False\n",
        "        if not torch.allclose(state_dict_1[key], state_dict_2[key]):\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "    # return model_1.state_dict().__str__() == model_2.state_dict().__str__()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_5_'></a>[Dataset](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'0'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'1'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'3'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'4'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'5'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'6'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'7'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'8'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'9'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'+'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'*'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'-'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'='</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;BOS&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;EOS&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;PAD&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;UNK&gt;'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m{\u001b[0m\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'0'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'1'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'2'\u001b[0m: \u001b[1;36m2\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'3'\u001b[0m: \u001b[1;36m3\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'4'\u001b[0m: \u001b[1;36m4\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'5'\u001b[0m: \u001b[1;36m5\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'6'\u001b[0m: \u001b[1;36m6\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'7'\u001b[0m: \u001b[1;36m7\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'8'\u001b[0m: \u001b[1;36m8\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'9'\u001b[0m: \u001b[1;36m9\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'+'\u001b[0m: \u001b[1;36m10\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'*'\u001b[0m: \u001b[1;36m11\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'-'\u001b[0m: \u001b[1;36m12\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'='\u001b[0m: \u001b[1;36m13\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32mBOS\u001b[0m\u001b[32m>'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m14\u001b[0m\u001b[39m,\u001b[0m\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'<EOS>'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m15\u001b[0m\u001b[39m,\u001b[0m\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'<PAD>'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m16\u001b[0m\u001b[39m,\u001b[0m\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'<UNK\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m: \u001b[1;36m17\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'0'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'3'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'4'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'5'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'6'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'7'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'8'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'9'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'+'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'*'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'-'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'='</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;BOS&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;EOS&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;PAD&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;UNK&gt;'</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m{\u001b[0m\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m0\u001b[0m: \u001b[32m'0'\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m1\u001b[0m: \u001b[32m'1'\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m2\u001b[0m: \u001b[32m'2'\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m3\u001b[0m: \u001b[32m'3'\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m4\u001b[0m: \u001b[32m'4'\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m5\u001b[0m: \u001b[32m'5'\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m6\u001b[0m: \u001b[32m'6'\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m7\u001b[0m: \u001b[32m'7'\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m8\u001b[0m: \u001b[32m'8'\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m9\u001b[0m: \u001b[32m'9'\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m10\u001b[0m: \u001b[32m'+'\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m11\u001b[0m: \u001b[32m'*'\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m12\u001b[0m: \u001b[32m'-'\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m13\u001b[0m: \u001b[32m'='\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m14\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32mBOS\u001b[0m\u001b[32m>'\u001b[0m\u001b[39m,\u001b[0m\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m15\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'<EOS>'\u001b[0m\u001b[39m,\u001b[0m\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m16\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'<PAD>'\u001b[0m\u001b[39m,\u001b[0m\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m17\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'<UNK\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;36m18\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'0'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'1'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'3'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'4'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'5'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'6'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'7'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'8'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'9'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'+'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'*'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'-'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'='</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;BOS&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;EOS&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;PAD&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;UNK&gt;'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m{\u001b[0m\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'0'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'1'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'2'\u001b[0m: \u001b[1;36m2\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'3'\u001b[0m: \u001b[1;36m3\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'4'\u001b[0m: \u001b[1;36m4\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'5'\u001b[0m: \u001b[1;36m5\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'6'\u001b[0m: \u001b[1;36m6\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'7'\u001b[0m: \u001b[1;36m7\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'8'\u001b[0m: \u001b[1;36m8\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'9'\u001b[0m: \u001b[1;36m9\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'+'\u001b[0m: \u001b[1;36m10\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'*'\u001b[0m: \u001b[1;36m11\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'-'\u001b[0m: \u001b[1;36m12\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'='\u001b[0m: \u001b[1;36m13\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32mBOS\u001b[0m\u001b[32m>'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m14\u001b[0m\u001b[39m,\u001b[0m\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'<EOS>'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m15\u001b[0m\u001b[39m,\u001b[0m\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'<PAD>'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m16\u001b[0m\u001b[39m,\u001b[0m\n",
              "\u001b[2;32m│   \u001b[0m\u001b[32m'<UNK\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m: \u001b[1;36m17\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'0'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'3'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'4'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'5'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'6'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'7'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'8'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'9'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'+'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'*'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'-'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'='</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;BOS&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;EOS&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;PAD&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;UNK&gt;'</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m{\u001b[0m\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m0\u001b[0m: \u001b[32m'0'\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m1\u001b[0m: \u001b[32m'1'\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m2\u001b[0m: \u001b[32m'2'\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m3\u001b[0m: \u001b[32m'3'\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m4\u001b[0m: \u001b[32m'4'\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m5\u001b[0m: \u001b[32m'5'\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m6\u001b[0m: \u001b[32m'6'\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m7\u001b[0m: \u001b[32m'7'\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m8\u001b[0m: \u001b[32m'8'\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m9\u001b[0m: \u001b[32m'9'\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m10\u001b[0m: \u001b[32m'+'\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m11\u001b[0m: \u001b[32m'*'\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m12\u001b[0m: \u001b[32m'-'\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m13\u001b[0m: \u001b[32m'='\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m14\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32mBOS\u001b[0m\u001b[32m>'\u001b[0m\u001b[39m,\u001b[0m\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m15\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'<EOS>'\u001b[0m\u001b[39m,\u001b[0m\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m16\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'<PAD>'\u001b[0m\u001b[39m,\u001b[0m\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;36m17\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'<UNK\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"font-weight: bold\">]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m[\u001b[0m\u001b[1;36m14\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m15\u001b[0m\u001b[1m]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"font-weight: bold\">]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m[\u001b[0m\u001b[1;36m14\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m15\u001b[0m\u001b[1m]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded sentence: [14, 1, 5, 10, 5, 7, 13, 0, 7, 2, 15]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'15+57=072'</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[32m'15+\u001b[0m\u001b[32m57\u001b[0m\u001b[32m=\u001b[0m\u001b[32m072\u001b[0m\u001b[32m'\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"font-weight: bold\">]]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m14\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m13\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m15\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[1;36m14\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m13\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m15\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'15+57=072'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'01+02=003'</span><span style=\"font-weight: bold\">]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m[\u001b[0m\u001b[32m'15+\u001b[0m\u001b[32m57\u001b[0m\u001b[32m=\u001b[0m\u001b[32m072\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'01+\u001b[0m\u001b[32m02\u001b[0m\u001b[32m=\u001b[0m\u001b[32m003\u001b[0m\u001b[32m'\u001b[0m\u001b[1m]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "vocab = AdderVocabulary.from_tokens(tokens=config.constants.TOKENS, num_digits=config.constants.NUM_DIGITS)\n",
        "token_to_index = vocab.token_to_index\n",
        "index_to_token = vocab.index_to_token\n",
        "vocab_size = vocab.vocab_size\n",
        "pprint(token_to_index)\n",
        "pprint(index_to_token)\n",
        "pprint(vocab_size)\n",
        "\n",
        "\n",
        "tokenizer = AdderTokenizer(vocabulary=vocab)\n",
        "pprint(tokenizer.vocabulary.token_to_index)\n",
        "pprint(tokenizer.vocabulary.index_to_token)\n",
        "pprint(tokenizer.encode(\"1\"))\n",
        "pprint(tokenizer.encode(\"+\"))\n",
        "\n",
        "sequence = \"15+57=072\"\n",
        "sequences = [\"15+57=072\", \"01+02=003\"]  # , \"95+53=148\", \"15+10=025\"]\n",
        "\n",
        "encoded_sentence = tokenizer.encode(sequence)\n",
        "print(f\"Encoded sentence: {encoded_sentence}\")\n",
        "decoded_sentence = tokenizer.decode(encoded_sentence)\n",
        "pprint(decoded_sentence)\n",
        "\n",
        "encoded_sentences = tokenizer.encode_batch(sequences)  # type: ignore[attr-defined]\n",
        "pprint(encoded_sentences)\n",
        "decoded_sentences = tokenizer.decode_batch(encoded_sentences)  # type: ignore[attr-defined]\n",
        "pprint(decoded_sentences)\n",
        "\n",
        "PAD = vocab.token_to_index[vocab.PAD]\n",
        "UNK = vocab.token_to_index[vocab.UNK]\n",
        "ADD = vocab.token_to_index[vocab.ADD]\n",
        "EQUAL = vocab.token_to_index[vocab.EQUAL]\n",
        "BOS = vocab.token_to_index[vocab.BOS]\n",
        "EOS = vocab.token_to_index[vocab.EOS]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZX9ORv5IrdzR"
      },
      "outputs": [],
      "source": [
        "def pad_number(num: int, length: int) -> str:\n",
        "    \"\"\"\n",
        "    Pad numbers with zeros in front so that they have uniform length.\n",
        "\n",
        "    Note, if a + b = c and num digits allowed to add is 2, then for\n",
        "    a and b we always pad to length 2, but for c we always pad to length 3.\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    6 + 90 = 96 -> 06 + 90 = 096\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    num : int\n",
        "        Number to be padded.\n",
        "    num_digits : int\n",
        "        Length of the resulting padded number string.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        Padded number string.\n",
        "    \"\"\"\n",
        "    return str(num).zfill(length)\n",
        "\n",
        "\n",
        "def equation_to_string(a: int, b: int, c: int, num_digits: int) -> str:\n",
        "    \"\"\"\n",
        "    Formats the addition equation as a string.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    a : int\n",
        "        First addend.\n",
        "    b : int\n",
        "        Second addend.\n",
        "    c : int\n",
        "        Sum of a and b.\n",
        "    num_digits : int\n",
        "        Number of digits each number in the equation should have.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        Formatted equation string.\n",
        "    \"\"\"\n",
        "    padded_a = pad_number(a, num_digits)\n",
        "    padded_b = pad_number(b, num_digits)\n",
        "    padded_c = pad_number(c, num_digits + 1) # note the padding here!\n",
        "    return f\"{padded_a}+{padded_b}={padded_c}\"\n",
        "\n",
        "def decode_equation(vocab: AdderVocabulary, equation: torch.Tensor | List[int]) -> str:\n",
        "    \"\"\"\n",
        "    Convert an equation in list format to string format.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    equation : List[int]\n",
        "        The equation in list format.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        The equation in string format.\n",
        "    \"\"\"\n",
        "    if isinstance(equation, torch.Tensor):\n",
        "        equation = equation.tolist()\n",
        "\n",
        "    UNK = vocab.token_to_index[vocab.UNK]\n",
        "    decoded_equation = \"\".join([str(index_to_token.get(x, UNK)) for x in equation])\n",
        "    return decoded_equation.replace(\"<BOS>\", \"\").replace(\"<EOS>\", \"\")\n",
        "\n",
        "def batch_decode_equation(vocab: AdderVocabulary, equations: torch.Tensor | List[List[int]]) -> List[str]:\n",
        "    decoded_equations = []\n",
        "    for equation in equations:\n",
        "        decoded_equation = decode_equation(vocab, equation)\n",
        "        decoded_equations.append(decoded_equation)\n",
        "    return decoded_equations\n",
        "\n",
        "def encode_equation(vocab: AdderVocabulary, equation: str, num_digits: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Convert an equation (up to the equal sign in it) in string format to a list.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    equation : str\n",
        "        The equation in string format.\n",
        "    num_digits : int\n",
        "        Number of digits each number in the equation should have.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "        The equation in list format as a tensor.\n",
        "    \"\"\"\n",
        "    plus_idx = equation.index(vocab.ADD)\n",
        "    equal_idx = equation.index(vocab.EQUAL)\n",
        "\n",
        "    BOS = vocab.token_to_index[vocab.BOS]\n",
        "    UNK = vocab.token_to_index[vocab.UNK]\n",
        "\n",
        "    a = pad_number(int(equation[:plus_idx]), num_digits)\n",
        "    b = pad_number(int(equation[plus_idx + 1:equal_idx]), num_digits)\n",
        "\n",
        "    new_equation = f\"{a}+{b}=\"\n",
        "\n",
        "    return torch.tensor(\n",
        "        [BOS] + [token_to_index.get(n, UNK) for n in new_equation],\n",
        "        dtype=torch.int\n",
        "    ).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SovDUghLrdzR"
      },
      "outputs": [],
      "source": [
        "def create_add_dataset(\n",
        "    vocab: AdderVocabulary, num_digits: int, dataset_size: int, rng_seed: int = 1337\n",
        ") -> Tuple[List[torch.Tensor], List[str]]:\n",
        "    BOS = vocab.token_to_index[vocab.BOS]\n",
        "    EOS = vocab.token_to_index[vocab.EOS]\n",
        "    UNK = vocab.token_to_index[vocab.UNK]\n",
        "\n",
        "    rng = torch.Generator()\n",
        "    rng.manual_seed(rng_seed)\n",
        "\n",
        "    max_num = 10**num_digits - 1\n",
        "\n",
        "    dataset_str = []\n",
        "    for _ in range(dataset_size):\n",
        "        a = torch.randint(low=0, high=max_num + 1, size=(1,), generator=rng).item()\n",
        "        b = torch.randint(low=0, high=max_num + 1, size=(1,), generator=rng).item()\n",
        "        c = a + b\n",
        "\n",
        "        equation = equation_to_string(a, b, c, num_digits)\n",
        "\n",
        "        dataset_str.append(equation)\n",
        "\n",
        "    dataset_tensor = [\n",
        "        torch.tensor([BOS] + [token_to_index.get(n, UNK) for n in x] + [EOS])\n",
        "        for x in dataset_str\n",
        "    ]\n",
        "    return dataset_tensor, dataset_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xEfK777ErdzR",
        "outputId": "20b75567-38b1-4e96-9892-08f60d18195d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"font-weight: bold\">])</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"font-weight: bold\">])</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"font-weight: bold\">])</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"font-weight: bold\">])</span>\n",
              "<span style=\"font-weight: bold\">]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m[\u001b[0m\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m14\u001b[0m,  \u001b[1;36m1\u001b[0m,  \u001b[1;36m5\u001b[0m, \u001b[1;36m10\u001b[0m,  \u001b[1;36m5\u001b[0m,  \u001b[1;36m7\u001b[0m, \u001b[1;36m13\u001b[0m,  \u001b[1;36m0\u001b[0m,  \u001b[1;36m7\u001b[0m,  \u001b[1;36m2\u001b[0m, \u001b[1;36m15\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m14\u001b[0m,  \u001b[1;36m9\u001b[0m,  \u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m,  \u001b[1;36m0\u001b[0m,  \u001b[1;36m0\u001b[0m, \u001b[1;36m13\u001b[0m,  \u001b[1;36m0\u001b[0m,  \u001b[1;36m9\u001b[0m,  \u001b[1;36m2\u001b[0m, \u001b[1;36m15\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m14\u001b[0m,  \u001b[1;36m9\u001b[0m,  \u001b[1;36m5\u001b[0m, \u001b[1;36m10\u001b[0m,  \u001b[1;36m5\u001b[0m,  \u001b[1;36m3\u001b[0m, \u001b[1;36m13\u001b[0m,  \u001b[1;36m1\u001b[0m,  \u001b[1;36m4\u001b[0m,  \u001b[1;36m8\u001b[0m, \u001b[1;36m15\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m14\u001b[0m,  \u001b[1;36m1\u001b[0m,  \u001b[1;36m5\u001b[0m, \u001b[1;36m10\u001b[0m,  \u001b[1;36m1\u001b[0m,  \u001b[1;36m0\u001b[0m, \u001b[1;36m13\u001b[0m,  \u001b[1;36m0\u001b[0m,  \u001b[1;36m2\u001b[0m,  \u001b[1;36m5\u001b[0m, \u001b[1;36m15\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
              "\u001b[1m]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'15+57=072'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'92+00=092'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'95+53=148'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'15+10=025'</span><span style=\"font-weight: bold\">]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m[\u001b[0m\u001b[32m'15+\u001b[0m\u001b[32m57\u001b[0m\u001b[32m=\u001b[0m\u001b[32m072\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'92+\u001b[0m\u001b[32m00\u001b[0m\u001b[32m=\u001b[0m\u001b[32m092\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'95+\u001b[0m\u001b[32m53\u001b[0m\u001b[32m=\u001b[0m\u001b[32m148\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'15+\u001b[0m\u001b[32m10\u001b[0m\u001b[32m=\u001b[0m\u001b[32m025\u001b[0m\u001b[32m'\u001b[0m\u001b[1m]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "('15+57=072', '15+57=072')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_tensor, dataset_str = create_add_dataset(vocab=vocab, num_digits=2, dataset_size=4)\n",
        "pprint(dataset_tensor)\n",
        "pprint(dataset_str)\n",
        "\n",
        "decode_equation(vocab, dataset_tensor[0]), decode_equation(vocab, [15, 1, 5, 10, 5, 7, 13, 0, 7, 2, 14])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([14,  0,  1, 10,  0,  2, 13], dtype=torch.int32),\n",
              " [14, 0, 1, 10, 0, 2, 13])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encode_equation(vocab, \"01+02=\", num_digits=2), encode_equation(vocab, \"01+02=\", num_digits=2).tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHs3grM1rdzS"
      },
      "source": [
        "Some notes:\n",
        "\n",
        "1. We included other operations besides addition for future use. So it may seem redundant for now.\n",
        "2. Kapathy's version is more efficient since for an expression such as `15+87=102` would be encoded as `1587102` since for one, we restrict the `num_digits` to be fixed, this means that if `num_digits=2`, then it follows that only numbers that are less or equals to 2 digits can be added together. As a result, `1587102` will always be interpreted as first 2 digit = first num, next 2 digits = second num, last 3 digits as third or the answer (sum of first two). Let's look at two more examples to appreciate this:\n",
        "   1. `0639045 <> 6 + 39 = 45`\n",
        "   2. `5101052 <> 51 + 1 = 52`\n",
        "3. He also encoded the answer backwards because its easier for GPT model, but for a first review, I will not do so to avoid confusion.\n",
        "\n",
        "KAPATHY:\n",
        "\n",
        "As one more example, the problem 6 + 39 = 45 would be encoded as:\n",
        "\n",
        "\"0639054\"\n",
        "\n",
        "where you will notice that we are padding with zeros to make sure that we always\n",
        "produce strings of the exact same size: n + n + (n + 1). When n=2, this is 7.\n",
        "At test time, we will feed in an addition problem by giving the first 2n digits,\n",
        "and hoping that the GPT model completes the sequence with the next (n+1) digits\n",
        "correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5tTF5J-rdzS"
      },
      "source": [
        "TODO: to make it inherit Dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_6_'></a>[Construct Batches, Collate Function and DataLoader](#toc0_)\n",
        "\n",
        "DISTINGUISH BETWEEN GPT DECODER ONLY VS ENCODER DECODER SEQ TO SEQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "G4jzuj9yrdzU",
        "outputId": "2e368854-1357-4091-dd2d-f201b302d32a"
      },
      "outputs": [],
      "source": [
        "# construct_future_mask(seq_len=3)[:, None, None, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "    # counter = 1\n",
        "    # for x, y, pad_mask, future_mask in dataset:  # type: ignore[attr-defined]\n",
        "    #     print(\"x\")\n",
        "    #     pprint(x)\n",
        "    #     pprint(isinstance(x, torch.LongTensor))\n",
        "\n",
        "    #     print(\"y\")\n",
        "    #     pprint(y)\n",
        "    #     print(\"pad\")\n",
        "    #     pprint(pad_mask)\n",
        "    #     print(\"future\")\n",
        "    #     pprint(future_mask)\n",
        "    #     if counter == 2:\n",
        "    #         break\n",
        "    # # at this junction it is possible for the seq len\n",
        "    # # to vary. Dataset only cares about generating 1 single\n",
        "    # # sample data point and do not worry about different\n",
        "    # # sequence length across other samples.\n",
        "    # # but in torch we train via batches, and with different\n",
        "    # # batch sizes we may encounter issues like you know\n",
        "    # # matrix multiplication may not work.\n",
        "\n",
        "    # # As we see later, the collate fn will be passed into\n",
        "    # # dataloader. where dataloader gather individual samples\n",
        "    # # from dataset into BATCHES \\mathcal{B}. But they\n",
        "    # # dont care if your individual samples from dataset\n",
        "    # # is of diff length, or if you want to broadcast some\n",
        "    # # padding or future mask TO BE THE SAME AS BATCH SIZE\n",
        "    # # IN SOME DIMENSION.\n",
        "\n",
        "    # # The `collate_fn` defines how to combine these variable-length samples into a\n",
        "    # # batch. This usually involves padding the sequences in the batch to a common\n",
        "    # # length, which is typically the length of the longest sequence in the batch.\n",
        "\n",
        "    # # Assuming your dataset is initialized as `my_dataset`\n",
        "    # # and your `collate_fn` is defined as shown above\n",
        "    # dataloader = DataLoader(\n",
        "    #     dataset,\n",
        "    #     batch_size=4,\n",
        "    #     collate_fn=lambda batch: collate_fn(batch, batch_first=True, pad_token_id=16),\n",
        "    # )\n",
        "\n",
        "    # for i, batch in enumerate(dataloader):\n",
        "    #     (\n",
        "    #         inputs_padded,\n",
        "    #         targets_padded,\n",
        "    #         padding_masks_padded_and_expanded,\n",
        "    #         future_masks_expanded,\n",
        "    #     ) = batch\n",
        "\n",
        "    #     # Print shapes\n",
        "    #     print(f\"Batch {i+1}\")\n",
        "    #     print(\"Inputs Shape:\", inputs_padded.shape)\n",
        "    #     print(\"Targets Shape:\", targets_padded.shape)\n",
        "    #     print(\"Padding Masks Shape:\", padding_masks_padded_and_expanded.shape)\n",
        "    #     print(\"Future Masks Shape:\", future_masks_expanded.shape)\n",
        "\n",
        "    #     # Print values (consider printing only a part of each tensor for large datasets)\n",
        "    #     print(\"Inputs Values:\", inputs_padded)\n",
        "    #     print(\"Targets Values:\", targets_padded)\n",
        "    #     print(\"Padding Masks Values:\", padding_masks_padded_and_expanded)\n",
        "    #     print(\"Future Masks Values:\", future_masks_expanded)\n",
        "\n",
        "    #     # Add a separator for readability between batches\n",
        "    #     print(\"-\" * 50)\n",
        "\n",
        "    #     # Optionally, break after a few batches to avoid too much output\n",
        "    #     if i >= 2:  # Change this number based on how many batches you want to inspect\n",
        "    #         break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_7_'></a>[DataLoader](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DataConfig</span><span style=\"font-weight: bold\">(</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">context_length</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">dataset_name</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">dataset_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">dataset_path</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">dataset_dir</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">dataset_url</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">split</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">collate_fn</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'batch_first'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'pad_token_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"font-weight: bold\">}</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">train_loader</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'shuffle'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num_workers'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'pin_memory'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'drop_last'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">}</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">valid_loader</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'shuffle'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num_workers'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'pin_memory'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'drop_last'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">}</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">test_loader</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'shuffle'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num_workers'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'pin_memory'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'drop_last'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">}</span>\n",
              "<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mDataConfig\u001b[0m\u001b[1m(\u001b[0m\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33mcontext_length\u001b[0m=\u001b[1;36m128\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33mdataset_name\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33mdataset_size\u001b[0m=\u001b[1;36m8\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33mdataset_path\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33mdataset_dir\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33mdataset_url\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33msplit\u001b[0m=\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33mcollate_fn\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'batch_first'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \u001b[32m'pad_token_id'\u001b[0m: \u001b[1;36m16\u001b[0m\u001b[1m}\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33mtrain_loader\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'batch_size'\u001b[0m: \u001b[1;36m2\u001b[0m, \u001b[32m'shuffle'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \u001b[32m'num_workers'\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[32m'pin_memory'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'drop_last'\u001b[0m: \u001b[3;91mFalse\u001b[0m\u001b[1m}\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33mvalid_loader\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'batch_size'\u001b[0m: \u001b[1;36m2\u001b[0m, \u001b[32m'shuffle'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'num_workers'\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[32m'pin_memory'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'drop_last'\u001b[0m: \u001b[3;91mFalse\u001b[0m\u001b[1m}\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33mtest_loader\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'batch_size'\u001b[0m: \u001b[1;36m2\u001b[0m, \u001b[32m'shuffle'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'num_workers'\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[32m'pin_memory'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'drop_last'\u001b[0m: \u001b[3;91mFalse\u001b[0m\u001b[1m}\u001b[0m\n",
              "\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/gaohn/miniconda3/envs/omniverse/lib/python3.9/site-packages/torch/utils/data/dataset.py:414: UserWarning: Length of split at index 1 is 0. This might result in an empty dataset.\n",
            "  warnings.warn(f\"Length of split at index {i} is 0. \"\n",
            "/Users/gaohn/miniconda3/envs/omniverse/lib/python3.9/site-packages/torch/utils/data/dataset.py:414: UserWarning: Length of split at index 2 is 0. This might result in an empty dataset.\n",
            "  warnings.warn(f\"Length of split at index {i} is 0. \"\n"
          ]
        }
      ],
      "source": [
        "config.data.train_loader[\"batch_size\"] = 2\n",
        "config.data.valid_loader[\"batch_size\"] = 2\n",
        "config.data.test_loader[\"batch_size\"] = 2\n",
        "config.data.dataset_size = 8\n",
        "config.data.split = [1, 0, 0]\n",
        "config.global_.seed = 1337\n",
        "\n",
        "vocab = AdderVocabulary.from_tokens(tokens=config.constants.TOKENS, num_digits=config.constants.NUM_DIGITS)\n",
        "dataset_tensor, dataset_str = create_add_dataset(vocab=vocab, num_digits=config.constants.NUM_DIGITS, dataset_size=config.data.dataset_size)\n",
        "\n",
        "tokenizer = AdderTokenizer(vocabulary=vocab)\n",
        "adder_dataset = AdderDataset(data=dataset_str, tokenizer=tokenizer)\n",
        "\n",
        "pprint(config.data)\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = split_dataset(\n",
        "    dataset=adder_dataset,\n",
        "    split=config.data.split,\n",
        "    seed=config.global_.seed\n",
        ")\n",
        "\n",
        "\n",
        "train_loader = create_loader(\n",
        "    dataset=train_dataset,\n",
        "    loader_config=config.data.train_loader,\n",
        "    collate_fn_config=config.data.collate_fn,\n",
        ")\n",
        "\n",
        "valid_loader = create_loader(\n",
        "    dataset=val_dataset,\n",
        "    loader_config=config.data.valid_loader,\n",
        "    collate_fn_config=config.data.collate_fn,\n",
        ")\n",
        "\n",
        "test_loader = create_loader(\n",
        "    dataset=test_dataset,\n",
        "    loader_config=config.data.test_loader,\n",
        "    collate_fn_config=config.data.collate_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch Size: 2\n",
            "Inputs Shape: torch.Size([2, 10])\n",
            "Targets Shape: torch.Size([2, 10])\n",
            "Decoded Equation: 95+53=148\n",
            "--------------------------------------------------------------------------------\n",
            "Batch Size: 2\n",
            "Inputs Shape: torch.Size([2, 10])\n",
            "Targets Shape: torch.Size([2, 10])\n",
            "Decoded Equation: 34+90=124\n",
            "--------------------------------------------------------------------------------\n",
            "Batch Size: 2\n",
            "Inputs Shape: torch.Size([2, 10])\n",
            "Targets Shape: torch.Size([2, 10])\n",
            "Decoded Equation: 12+20=032\n",
            "--------------------------------------------------------------------------------\n",
            "Batch Size: 2\n",
            "Inputs Shape: torch.Size([2, 10])\n",
            "Targets Shape: torch.Size([2, 10])\n",
            "Decoded Equation: 90+38=128\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "seed_all(133338, seed_torch=True)\n",
        "\n",
        "for batch in train_loader:\n",
        "    # Each batch is a tuple containing all elements for the batch\n",
        "    inputs_padded, targets_padded, padding_masks_padded_and_expanded, future_masks_expanded = batch\n",
        "\n",
        "    # Print the length of each component in the batch\n",
        "    print(\"Batch Size:\", len(inputs_padded))  # This prints the number of samples in the batch\n",
        "\n",
        "    # Now you can print shapes or other properties of each batch element\n",
        "    print(\"Inputs Shape:\", inputs_padded.shape)\n",
        "    print(\"Targets Shape:\", targets_padded.shape)\n",
        "\n",
        "    # Decoding and other processing can be done here\n",
        "    # For example, decoding the first sequence in the batch\n",
        "    print(\"Decoded Equation:\", decode_equation(vocab, inputs_padded[0].tolist()))\n",
        "\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note here the padding in collate is \"redundant\" since in our earlier code\n",
        "we ensured that all sample has same number of characters by way of padding\n",
        "zeros in front. For example, `23 + 3 =26` will become `23 + 03 = 026`. Consequently,\n",
        "all samples $\\in$ batch will have same length by definition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_7_1_'></a>[Example](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import torch\n",
        "# from typing import List\n",
        "# from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# sequences = [\n",
        "#     torch.tensor([1, 2]),\n",
        "#     torch.tensor([3, 4, 5]),\n",
        "#     torch.tensor([6, 7, 8, 9]),\n",
        "#     torch.tensor([2, 3]),\n",
        "# ]\n",
        "# # Let's say PAD is represented by the integer 16\n",
        "# PAD = 16\n",
        "# sample_dataloader = DataLoader(\n",
        "#     sequences,\n",
        "#     batch_size=4,\n",
        "#     collate_fn=lambda b: collate_fn(b, batch_first=True, padding_value=PAD),\n",
        "# )\n",
        "# batch = next(iter(sample_dataloader))\n",
        "# pprint(batch)\n",
        "\n",
        "# # Create the pad mask\n",
        "# pad_mask = batch == PAD\n",
        "# print(pad_mask)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IEFJbHyrdzU"
      },
      "source": [
        "In PyTorch, `input == PAD` will perform element-wise comparison between each element of the `input` tensor and the constant `PAD`. The `PAD` constant is usually an integer that represents the padding token in a sequence. For example, in NLP tasks, padding tokens are often used to make all sequences in a batch the same length.\n",
        "\n",
        "If `input` is a tensor of shape `(Batch Size, Sequence Length)`, then `input == PAD` will return a boolean tensor of the same shape, where each element at position `(i, j)` will be `True` if `input[i, j]` is equal to `PAD`, and `False` otherwise. This boolean tensor will serve as a mask that identifies where the padding tokens are located in the original `input` tensor.\n",
        "\n",
        "---\n",
        "\n",
        "The `src_mask` term in the context of transformers typically refers to the \"source mask,\" which is designed to prevent the self-attention mechanism from considering certain tokens in the source sequence. This mask is applied to the attention scores before the softmax operation during the calculation of self-attention. The primary purposes of using such a mask are:\n",
        "\n",
        "1. Padding Masking: When sequences are batched together, shorter sequences are often padded with special tokens (usually denoted by zeros or a specific padding token) to match the length of the longest sequence in the batch. The `src_mask` helps the model ignore these padding tokens by setting the corresponding attention scores to a large negative value (usually `-inf`), so that they become zero after the softmax operation.\n",
        "\n",
        "2. Future Information Masking: In some tasks like sequence-to-sequence prediction, it's important that a token does not attend to future tokens in the sequence. This is another use-case for the mask, although this is more commonly seen in the target mask (`tgt_mask`) rather than the source mask (`src_mask`).\n",
        "\n",
        "To determine whether `src_mask` in a specific implementation is the padding mask from the dataloader, you'll need to check the code where this variable is defined or used. Typically, if the mask is intended to filter out padding tokens, then yes, it could very well be the same as the padding mask generated during data loading.\n",
        "\n",
        "The actual implementation may vary, but the concept generally remains the same. You'll often see the mask being used in the self-attention calculation, specifically right before the softmax operation to zero out particular positions.\n",
        "\n",
        "---\n",
        "\n",
        "What are `pad_mask`?\n",
        "\n",
        "The primary reason for using a padding mask (`pad_mask`) is to ensure that the model does not consider padding tokens during training or inference. Padding tokens are usually added to sequences to make them have a uniform length, but they don't carry any meaningful information. Ignoring them is crucial for several reasons:\n",
        "\n",
        "1. **Attention Mechanisms**: If your model uses attention, the mask ensures that attention scores for padding tokens are set to a very low value (often negative infinity), so that these scores don't affect the weighted sum of the input sequence.\n",
        "\n",
        "2. **Loss Computation**: When computing loss, padding tokens should not contribute. Including them could mislead the model during training, as they don't represent genuine mistakes in prediction.\n",
        "\n",
        "3. **Output Interpretation**: When the model makes predictions, padding tokens should not be taken into account for tasks like sequence-to-sequence translation, summarization, etc.\n",
        "\n",
        "4. **Computational Efficiency**: In some models or algorithms, knowing which tokens are padding can speed up computation by allowing the model to skip unnecessary operations.\n",
        "\n",
        "In summary, the padding mask is a utility to ensure that the model focuses only on the meaningful parts of the input sequence, thereby improving both accuracy and computational efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fu0C3l_ArdzU",
        "outputId": "d2928468-1002-4de0-fa44-f3fec74b15c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[14,  9,  0, 10,  3,  8, 13,  1,  2,  8],\n",
              "         [14,  1,  5, 10,  5,  7, 13,  0,  7,  2]]),\n",
              " tensor([[16, 16, 16, 16, 16, 16,  1,  2,  8, 15],\n",
              "         [16, 16, 16, 16, 16, 16,  0,  7,  2, 15]]),\n",
              " tensor([[[[True, True, True, True, True, True, True, True, True, True],\n",
              "           [True, True, True, True, True, True, True, True, True, True],\n",
              "           [True, True, True, True, True, True, True, True, True, True],\n",
              "           [True, True, True, True, True, True, True, True, True, True],\n",
              "           [True, True, True, True, True, True, True, True, True, True],\n",
              "           [True, True, True, True, True, True, True, True, True, True],\n",
              "           [True, True, True, True, True, True, True, True, True, True],\n",
              "           [True, True, True, True, True, True, True, True, True, True],\n",
              "           [True, True, True, True, True, True, True, True, True, True],\n",
              "           [True, True, True, True, True, True, True, True, True, True]]],\n",
              " \n",
              " \n",
              "         [[[True, True, True, True, True, True, True, True, True, True],\n",
              "           [True, True, True, True, True, True, True, True, True, True],\n",
              "           [True, True, True, True, True, True, True, True, True, True],\n",
              "           [True, True, True, True, True, True, True, True, True, True],\n",
              "           [True, True, True, True, True, True, True, True, True, True],\n",
              "           [True, True, True, True, True, True, True, True, True, True],\n",
              "           [True, True, True, True, True, True, True, True, True, True],\n",
              "           [True, True, True, True, True, True, True, True, True, True],\n",
              "           [True, True, True, True, True, True, True, True, True, True],\n",
              "           [True, True, True, True, True, True, True, True, True, True]]]]),\n",
              " tensor([[[[ True, False, False, False, False, False, False, False, False, False],\n",
              "           [ True,  True, False, False, False, False, False, False, False, False],\n",
              "           [ True,  True,  True, False, False, False, False, False, False, False],\n",
              "           [ True,  True,  True,  True, False, False, False, False, False, False],\n",
              "           [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
              "           [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
              "           [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
              "           [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
              "           [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
              "           [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]]],\n",
              " \n",
              " \n",
              "         [[[ True, False, False, False, False, False, False, False, False, False],\n",
              "           [ True,  True, False, False, False, False, False, False, False, False],\n",
              "           [ True,  True,  True, False, False, False, False, False, False, False],\n",
              "           [ True,  True,  True,  True, False, False, False, False, False, False],\n",
              "           [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
              "           [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
              "           [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
              "           [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
              "           [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
              "           [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]]]]))"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch = next(iter(train_loader))\n",
        "batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqb7TK4JrdzV"
      },
      "source": [
        "WHY concat target instead of stack them normally?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "YV5dtxROrdzV",
        "outputId": "ac59be5a-86a0-4428-aec6-5dc0e2043e99"
      },
      "outputs": [],
      "source": [
        "# construct_batches(batch)[0].shape, construct_batches(batch)[1].shape, construct_batches(batch)[2].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_8_'></a>[Model](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_8_1_'></a>[Masks](#toc0_)\n",
        "\n",
        "In Transformer models, especially in the decoder, two types of masks are commonly used: padding masks and look-ahead masks (or future masks). Here's why each is important and why you might need both:\n",
        "\n",
        "#### <a id='toc1_8_1_1_'></a>[Padding Mask](#toc0_)\n",
        "\n",
        "1. **Why it's needed**: When you're dealing with sequences of different lengths, you pad the shorter sequences with zeros to make them the same length as the longest one in the batch. These zero-paddings should not contribute to the output of the attention mechanism.\n",
        "  \n",
        "2. **Where it's used**: Both the encoder and the decoder use padding masks.\n",
        "\n",
        "3. **How it works**: The padding mask marks the padded positions so that they can be excluded from contributing to the attention mechanism. In practice, you'll typically set the corresponding attention scores to negative infinity before applying the softmax operation.\n",
        "\n",
        "#### <a id='toc1_8_1_2_'></a>[Look-Ahead Mask (Future Mask)](#toc0_)\n",
        "\n",
        "1. **Why it's needed**: In the decoder, each position can only attend to positions that come before it in the sequence to maintain the auto-regressive property. This is different from the encoder, where all positions can attend to all other positions.\n",
        "\n",
        "2. **Where it's used**: This mask is specifically for the decoder.\n",
        "\n",
        "3. **How it works**: The look-ahead mask is used to mask out future positions (i.e., positions that come after the current position) so that they don't contribute to the current attention scores. Before the softmax operation, you'll mark these positions so that their contributions are effectively zero.\n",
        "\n",
        "#### <a id='toc1_8_1_3_'></a>[Using Both Masks in the Decoder](#toc0_)\n",
        "\n",
        "It's possible to use both types of masks in the decoder to address different requirements:\n",
        "\n",
        "- Padding mask is for ignoring padded positions.\n",
        "- Look-ahead mask is for ensuring that each position only attends to positions before it in the sequence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_9_'></a>[2-Digits Addition](#toc0_)\n",
        "\n",
        "- `max_seq_len` example: `<BOS>90+38=128<EOS>`\n",
        "- But for our case, `max_seq_len` is the same across all samples since we padded zeros in front to make them all the same length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dataset, dataset_str = create_add_dataset(vocab, self.num_digits, self.dataset_size)\n",
        "\n",
        "# # write dataset_str to a file\n",
        "# # with open(\"dataset_str.txt\", \"w\") as f:\n",
        "# #     for item in dataset_str:\n",
        "# #         f.write(\"%s\\n\" % item)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DataConfig</span><span style=\"font-weight: bold\">(</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">context_length</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">dataset_name</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">dataset_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10000</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">dataset_path</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'../../../data/adder/dataset_str.txt'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">dataset_dir</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">dataset_url</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">split</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span><span style=\"font-weight: bold\">]</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">collate_fn</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'batch_first'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'pad_token_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"font-weight: bold\">}</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">train_loader</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'shuffle'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num_workers'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'pin_memory'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'drop_last'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">}</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">valid_loader</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'shuffle'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num_workers'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'pin_memory'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'drop_last'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">}</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">test_loader</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'shuffle'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num_workers'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'pin_memory'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'drop_last'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">}</span>\n",
              "<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mDataConfig\u001b[0m\u001b[1m(\u001b[0m\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33mcontext_length\u001b[0m=\u001b[1;36m128\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33mdataset_name\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33mdataset_size\u001b[0m=\u001b[1;36m10000\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33mdataset_path\u001b[0m=\u001b[32m'../../../data/adder/dataset_str.txt'\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33mdataset_dir\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33mdataset_url\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33msplit\u001b[0m=\u001b[1m[\u001b[0m\u001b[1;36m0.7\u001b[0m, \u001b[1;36m0.2\u001b[0m, \u001b[1;36m0.1\u001b[0m\u001b[1m]\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33mcollate_fn\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'batch_first'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \u001b[32m'pad_token_id'\u001b[0m: \u001b[1;36m16\u001b[0m\u001b[1m}\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33mtrain_loader\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'batch_size'\u001b[0m: \u001b[1;36m256\u001b[0m, \u001b[32m'shuffle'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \u001b[32m'num_workers'\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[32m'pin_memory'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'drop_last'\u001b[0m: \u001b[3;91mFalse\u001b[0m\u001b[1m}\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33mvalid_loader\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'batch_size'\u001b[0m: \u001b[1;36m256\u001b[0m, \u001b[32m'shuffle'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'num_workers'\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[32m'pin_memory'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'drop_last'\u001b[0m: \u001b[3;91mFalse\u001b[0m\u001b[1m}\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33mtest_loader\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'batch_size'\u001b[0m: \u001b[1;36m256\u001b[0m, \u001b[32m'shuffle'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'num_workers'\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[32m'pin_memory'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'drop_last'\u001b[0m: \u001b[3;91mFalse\u001b[0m\u001b[1m}\u001b[0m\n",
              "\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset_size = 10000\n",
        "batch_size   = 256\n",
        "\n",
        "config.data.train_loader[\"batch_size\"] = batch_size\n",
        "config.data.valid_loader[\"batch_size\"] = batch_size\n",
        "config.data.test_loader[\"batch_size\"] = batch_size\n",
        "config.data.split = [0.7, 0.2, 0.1]\n",
        "config.data.dataset_size = dataset_size\n",
        "config.data.dataset_path = \"../../../data/adder/dataset_str.txt\"\n",
        "\n",
        "\n",
        "num_digits = config.constants.NUM_DIGITS\n",
        "pprint(config.data)\n",
        "\n",
        "with open(config.data.dataset_path, \"r\") as file:\n",
        "    data = [line.strip() for line in file]\n",
        "\n",
        "vocab = AdderVocabulary.from_tokens(tokens=config.constants.TOKENS, num_digits=config.constants.NUM_DIGITS)\n",
        "tokenizer = AdderTokenizer(vocabulary=vocab)\n",
        "adder_dataset = AdderDataset(data=data, tokenizer=tokenizer)\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = split_dataset(\n",
        "    dataset=adder_dataset,\n",
        "    split=config.data.split,\n",
        "    seed=config.global_.seed\n",
        ")\n",
        "\n",
        "\n",
        "train_loader = create_loader(\n",
        "    dataset=train_dataset,\n",
        "    loader_config=config.data.train_loader,\n",
        "    collate_fn_config=config.data.collate_fn,\n",
        ")\n",
        "\n",
        "valid_loader = create_loader(\n",
        "    dataset=val_dataset,\n",
        "    loader_config=config.data.valid_loader,\n",
        "    collate_fn_config=config.data.collate_fn,\n",
        ")\n",
        "\n",
        "test_loader = create_loader(\n",
        "    dataset=test_dataset,\n",
        "    loader_config=config.data.test_loader,\n",
        "    collate_fn_config=config.data.collate_fn,\n",
        ")\n",
        "\n",
        "train_size, val_size, test_size = len(train_dataset), len(val_dataset), len(test_dataset)\n",
        "\n",
        "# max_seq_len is determined by 1+ num_digits + 1 + num_digits + 1 + num_digits + 1 + 1\n",
        "# where the 1s represent BOS, Plus sign, Equal sign, the extra digit in the sum, EOS, respectively.\n",
        "max_seq_len = 1 + 1 + 1 + 1 + 2 * num_digits + (num_digits + 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_seq_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "NTROZtD4rdzc",
        "outputId": "253c0d26-f2d1-49ad-bd5c-b623d6c79339"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_size: 270226, train_set_size: 7000\n"
          ]
        }
      ],
      "source": [
        "# Create individual component configurations\n",
        "masked_self_attention_mha_config = MultiHeadedAttentionConfig(\n",
        "     attention=ScaledDotProductAttention(),\n",
        "    d_model=128, H=4, dropout=0.1\n",
        ")\n",
        "\n",
        "feed_forward_config = PositionwiseFeedForwardConfig(\n",
        "    d_model=128, d_ff=256, activation=nn.GELU(approximate=\"tanh\"), dropout=0.1, bias=True\n",
        ")\n",
        "\n",
        "add_norm_config_1 = AddNormConfig(feature_dim=128, dropout=0.1)\n",
        "add_norm_config_2 = AddNormConfig(feature_dim=128, dropout=0.1)\n",
        "\n",
        "# Create DecoderBlockConfig\n",
        "decoder_block_config = DecoderBlockConfig(\n",
        "    masked_self_attention_mha=masked_self_attention_mha_config,\n",
        "    feed_forward=feed_forward_config,\n",
        "    add_norm_1=add_norm_config_1,\n",
        "    add_norm_2=add_norm_config_2,\n",
        ")\n",
        "\n",
        "# Create the overall DecoderConfig\n",
        "model_config = DecoderConfig(\n",
        "    d_model=128,\n",
        "    vocab_size=vocab_size,\n",
        "    context_length=max_seq_len,\n",
        "    num_decoder_blocks=2,\n",
        "    dropout=0.1,\n",
        "    decoder_block=decoder_block_config,\n",
        ")\n",
        "\n",
        "model = GPTDecoder(model_config).to(DEVICE)\n",
        "\n",
        "model_size = sum([p.numel() for p in model.parameters()])\n",
        "print(f'model_size: {model_size}, train_set_size: {train_size}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "TpbLlCLRrdzc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.98)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-09\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.2\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "warmup_steps = 3 * len(train_loader)\n",
        "\n",
        "\n",
        "# lr first increases in the warmup steps, and then decays\n",
        "lr_fn        = lambda step: model_config.d_model**(-0.5) * min([(step+1)**(-0.5), (step+1)*warmup_steps**(-1.5)])\n",
        "# optimizer    = torch.optim.Adam(model.parameters(), lr=0.2, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# optimizer_config = OptimizerConfig(name=\"torch.optim.Adam\", lr=0.2, betas=(0.9, 0.98), eps=1e-9)\n",
        "# optimizer   = optimizer_config.build(params=model.parameters())\n",
        "\n",
        "# optimizer_config = OptimizerConfig(name=\"torch.optim.Adam\", lr=0.2)\n",
        "# optimizer   = optimizer_config.build(params=model.parameters(), betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "optimizer   = config.optimizer.build(params=model.parameters())\n",
        "print(optimizer)\n",
        "scheduler    = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_fn)\n",
        "criterion    = nn.CrossEntropyLoss(ignore_index=PAD, reduction=\"mean\")\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Metrics:\n",
        "    loss: Loss\n",
        "    accuracy: Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Adam (\n",
              "Parameter Group 0\n",
              "    amsgrad: False\n",
              "    betas: (0.9, 0.98)\n",
              "    capturable: False\n",
              "    differentiable: False\n",
              "    eps: 1e-09\n",
              "    foreach: None\n",
              "    fused: None\n",
              "    initial_lr: 0.2\n",
              "    lr: 2.2961808030073203e-05\n",
              "    maximize: False\n",
              "    weight_decay: 0\n",
              ")"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVvd-RL-rdzd"
      },
      "source": [
        "1. `input` is indeed `[bs, 10]` because max len is 11, so removed last token.\n",
        "2. `target` should be `[bs, 10]` but left shifted of the real original input but somehow i got 11.\n",
        "3. Think of vocab size to be num classes in my classification problem. But the"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "M5TwyxE5rdzd",
        "outputId": "3310e8c6-328e-41e5-acc8-7a4b63ed5845"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [00:04<00:00,  5.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Epoch Training Loss   : 2.41188\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [00:00<00:00, 20.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Epoch Validation Loss : 1.66422\n",
            "Epoch 2/2\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [00:03<00:00,  9.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Epoch Training Loss   : 1.36899\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [00:00<00:00, 20.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Epoch Validation Loss : 1.16084\n",
            "Training complete\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataloader=train_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    grad_norm_clip=1.0,\n",
        "    device=DEVICE,\n",
        "    valid_dataloader=valid_loader,\n",
        "    # test_dataloader=test_loader,\n",
        "    # NOTE: uncomment the above line to enable testing after each epoch\n",
        "    # but seeding will affect.\n",
        ")\n",
        "\n",
        "if DEBUG:\n",
        "    trained_model = trainer.fit(max_epochs=2) # or 15\n",
        "    # torch.save(model.state_dict(), 'model_debug.pt')\n",
        "    # model_debug = torch.load('./model_debug.pt')\n",
        "    # if are_both_models_same(model.state_dict(), model_debug):\n",
        "    #     print(\"Pass\")\n",
        "    # else:\n",
        "    #     print(\"Fail\")\n",
        "\n",
        "else:\n",
        "    trained_model = trainer.fit(max_epochs=30)\n",
        "\n",
        "    # torch.save(model.state_dict(), 'model_non_debug.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "'break' outside loop (668683560.py, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[28], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
          ]
        }
      ],
      "source": [
        "break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "Epoch 1/2\n",
        "----------\n",
        "100%|██████████| 28/28 [00:04<00:00,  5.60it/s]\n",
        "Average Epoch Training Loss   : 2.41188\n",
        "100%|██████████| 8/8 [00:00<00:00, 20.18it/s]\n",
        "Average Epoch Validation Loss : 1.66422\n",
        "Epoch 2/2\n",
        "----------\n",
        "100%|██████████| 28/28 [00:03<00:00,  9.07it/s]\n",
        "Average Epoch Training Loss   : 1.36899\n",
        "100%|██████████| 8/8 [00:00<00:00, 20.31it/s]\n",
        "Average Epoch Validation Loss : 1.16084\n",
        "Training complete\n",
        "```\n",
        "\n",
        "```\n",
        "Epoch 1/2\n",
        "----------\n",
        "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:03<00:00,  8.95it/s]\n",
        "Average Epoch Training Loss   : 2.40482\n",
        "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 15.67it/s]\n",
        "Average Epoch Validation Loss : 1.72585\n",
        "Epoch 2/2\n",
        "----------\n",
        "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:02<00:00, 12.24it/s]\n",
        "Average Epoch Training Loss   : 1.37748\n",
        "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 29.34it/s]\n",
        "Average Epoch Validation Loss : 1.15630\n",
        "Training complete\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trained_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch = next(iter(train_loader))\n",
        "pprint(batch)\n",
        "\n",
        "inputs, targets, target_padding_masks, future_masks = batch\n",
        "\n",
        "\n",
        "# Step 2: Pass the sample through the model\n",
        "trained_model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Assuming your model and sample require specific formatting, adjust as necessary\n",
        "    logits = model(inputs, target_padding_masks=target_padding_masks, future_masks=future_masks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "last_decoder_block = trained_model.decoder_blocks[-1] # take last decoder block? more feature?\n",
        "# pprint(last_decoder_block)\n",
        "\n",
        "masked_self_attention_mha = last_decoder_block.masked_self_attention_mha\n",
        "pprint(masked_self_attention_mha)\n",
        "\n",
        "context_vector, attention_weights = masked_self_attention_mha.context_vector, masked_self_attention_mha.attention_weights\n",
        "pprint(attention_weights.shape)\n",
        "# but has H=4 heads so do we take 1 head and check the heatmap?\n",
        "# torch.Size([208, 4, 10, 10])\n",
        "\n",
        "last_batch_last_sample_first_head_attention_weights = attention_weights[-1, 0:1, :, :].squeeze(0)\n",
        "pprint(last_batch_last_sample_first_head_attention_weights.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "the xy axis is keys and queries, which is correct `Q @ K.T`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Your existing setup\n",
        "last_decoder_block = trained_model.decoder_blocks[-1]\n",
        "masked_self_attention_mha = last_decoder_block.masked_self_attention_mha\n",
        "context_vector, attention_weights = masked_self_attention_mha.context_vector, masked_self_attention_mha.attention_weights\n",
        "\n",
        "# Number of heads\n",
        "num_heads = attention_weights.size(1)\n",
        "\n",
        "# Labels for each character in the sequence, including BOS\n",
        "labels = ['<BOS>'] + list('59+14=073')\n",
        "\n",
        "# Loop over each head and plot its heatmap\n",
        "for head in range(num_heads):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "\n",
        "    # Extract attention weights for the last sample in the last batch for this head\n",
        "    attention_matrix = attention_weights[-1, head, :, :].numpy()\n",
        "\n",
        "    sns.heatmap(attention_matrix, annot=True, cmap='viridis', xticklabels=labels, yticklabels=labels)\n",
        "    plt.title(f\"Attention Weights Heatmap for '<BOS>59+14=073' - Head {head+1}\")\n",
        "    plt.xlabel(\"Keys\")\n",
        "    plt.ylabel(\"Queries\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT_Nf9O4rdzd"
      },
      "source": [
        "## DEBUG\n",
        "\n",
        "### W\n",
        "\n",
        "\n",
        "```\n",
        "Epoch 1/2\n",
        "----------\n",
        "100%|██████████| 28/28 [00:04<00:00,  5.91it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 14.21it/s]\n",
        "Training Loss   : 2.08882\n",
        "Validation Loss : 1.27368\n",
        "Epoch 2/2\n",
        "----------\n",
        "  7%|▋         | 2/28 [00:00<00:04,  5.68it/s]\n",
        "100%|██████████| 28/28 [00:04<00:00,  6.42it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 21.24it/s]\n",
        "Training Loss   : 1.23194\n",
        "Validation Loss : 1.10291\n",
        "Training complete\n",
        "```\n",
        "\n",
        "CHANGED EOS and BOS SWAP POSITION\n",
        "\n",
        "```\n",
        "Epoch 1/2\n",
        "----------\n",
        " 32%|███▏      | 9/28 [00:01<00:02,  6.90it/s]\n",
        "100%|██████████| 28/28 [00:03<00:00,  8.22it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 26.60it/s]\n",
        "Training Loss   : 2.10450\n",
        "Validation Loss : 1.28284\n",
        "Epoch 2/2\n",
        "----------\n",
        "100%|██████████| 28/28 [00:02<00:00,  9.82it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 25.31it/s]\n",
        "Training Loss   : 1.23119\n",
        "Validation Loss : 1.09374\n",
        "```\n",
        "\n",
        "### M \n",
        "\n",
        "```\n",
        "Epoch 1/2\n",
        "----------\n",
        "100%|██████████| 28/28 [00:02<00:00, 13.63it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 34.32it/s]\n",
        "Training Loss   : 2.08863\n",
        "Validation Loss : 1.26961\n",
        "Epoch 2/2\n",
        "----------\n",
        "100%|██████████| 28/28 [00:01<00:00, 14.40it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 44.49it/s]\n",
        "Training Loss   : 1.23620\n",
        "Validation Loss : 1.11484\n",
        "Training complete\n",
        "\n",
        "---\n",
        "\n",
        "Epoch 29/30\n",
        "----------\n",
        "100%|██████████| 28/28 [00:01<00:00, 15.37it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 41.86it/s]\n",
        "Training Loss   : 0.01514\n",
        "Validation Loss : 0.00067\n",
        "Epoch 30/30\n",
        "----------\n",
        "100%|██████████| 28/28 [00:01<00:00, 15.38it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 42.62it/s]\n",
        "Training Loss   : 0.01448\n",
        "Validation Loss : 0.00057\n",
        "Training complete\n",
        "```\n",
        "\n",
        "```\n",
        "Epoch 1/30\n",
        "----------\n",
        "100%|██████████| 28/28 [00:02<00:00, 12.22it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 29.15it/s]\n",
        "Training Loss   : 2.10712\n",
        "Validation Loss : 1.27872\n",
        "Epoch 2/30\n",
        "----------\n",
        "100%|██████████| 28/28 [00:02<00:00, 12.78it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 29.36it/s]\n",
        "Training Loss   : 1.23133\n",
        "Validation Loss : 1.09508\n",
        "Epoch 3/30\n",
        "----------\n",
        "100%|██████████| 28/28 [00:02<00:00, 12.91it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 29.92it/s]\n",
        "Training Loss   : 1.03519\n",
        "Validation Loss : 0.87629\n",
        "Epoch 4/30\n",
        "----------\n",
        "100%|██████████| 28/28 [00:02<00:00, 13.41it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 28.30it/s]\n",
        "Training Loss   : 0.87725\n",
        "Validation Loss : 0.78361\n",
        "Epoch 5/30\n",
        "----------\n",
        "100%|██████████| 28/28 [00:02<00:00, 12.86it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 30.18it/s]\n",
        "Training Loss   : 0.79957\n",
        "Validation Loss : 0.73302\n",
        "Epoch 6/30\n",
        "----------\n",
        "100%|██████████| 28/28 [00:02<00:00, 13.35it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 28.31it/s]\n",
        "Training Loss   : 0.76029\n",
        "Validation Loss : 0.69880\n",
        "Epoch 7/30\n",
        "----------\n",
        "100%|██████████| 28/28 [00:02<00:00, 13.45it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 30.39it/s]\n",
        "Training Loss   : 0.72721\n",
        "Validation Loss : 0.68126\n",
        "Epoch 8/30\n",
        "----------\n",
        "100%|██████████| 28/28 [00:02<00:00, 13.53it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 27.81it/s]\n",
        "Training Loss   : 0.70416\n",
        "Validation Loss : 0.63668\n",
        "Epoch 9/30\n",
        "----------\n",
        "100%|██████████| 28/28 [00:02<00:00, 13.25it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 29.87it/s]\n",
        "Training Loss   : 0.64809\n",
        "Validation Loss : 0.48974\n",
        "Epoch 10/30\n",
        "----------\n",
        "100%|██████████| 28/28 [00:02<00:00, 12.47it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 22.07it/s]\n",
        "Training Loss   : 0.39999\n",
        "Validation Loss : 0.16127\n",
        "Epoch 11/30\n",
        "----------\n",
        "100%|██████████| 28/28 [00:02<00:00, 12.28it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 29.57it/s]\n",
        "Training Loss   : 0.20621\n",
        "Validation Loss : 0.08199\n",
        "Epoch 12/30\n",
        "----------\n",
        "100%|██████████| 28/28 [00:02<00:00, 12.71it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 15.80it/s]\n",
        "Training Loss   : 0.13922\n",
        "Validation Loss : 0.05043\n",
        "Epoch 13/30\n",
        "----------\n",
        "100%|██████████| 28/28 [00:02<00:00, 12.76it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 29.94it/s]\n",
        "Training Loss   : 0.11169\n",
        "Validation Loss : 0.03682\n",
        "Epoch 14/30\n",
        "----------\n",
        "100%|██████████| 28/28 [00:02<00:00, 13.03it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 30.37it/s]\n",
        "Training Loss   : 0.08848\n",
        "Validation Loss : 0.02700\n",
        "Epoch 15/30\n",
        "----------\n",
        "100%|██████████| 28/28 [00:02<00:00, 13.33it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 30.22it/s]\n",
        "Training Loss   : 0.07917\n",
        "Validation Loss : 0.02183\n",
        "Epoch 16/30\n",
        "----------\n",
        "100%|██████████| 28/28 [00:02<00:00, 13.20it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 27.96it/s]\n",
        "Training Loss   : 0.06974\n",
        "Validation Loss : 0.01599\n",
        "Epoch 17/30\n",
        "----------\n",
        "100%|██████████| 28/28 [00:02<00:00, 11.07it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 30.04it/s]\n",
        "Training Loss   : 0.05679\n",
        "Validation Loss : 0.01285\n",
        "Epoch 18/30\n",
        "----------\n",
        "100%|██████████| 28/28 [00:02<00:00, 11.82it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 18.65it/s]\n",
        "Training Loss   : 0.04896\n",
        "Validation Loss : 0.00878\n",
        "Epoch 19/30\n",
        "----------\n",
        "100%|██████████| 28/28 [00:03<00:00,  9.00it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 26.38it/s]\n",
        "Training Loss   : 0.04387\n",
        "Validation Loss : 0.00921\n",
        "Epoch 20/30\n",
        "----------\n",
        "100%|██████████| 28/28 [00:02<00:00,  9.98it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 28.77it/s]\n",
        "Training Loss   : 0.04160\n",
        "Validation Loss : 0.00447\n",
        "Epoch 21/30\n",
        "----------\n",
        "100%|██████████| 28/28 [00:02<00:00, 12.33it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 28.80it/s]\n",
        "Training Loss   : 0.03468\n",
        "Validation Loss : 0.00423\n",
        "Epoch 22/30\n",
        "----------\n",
        "100%|██████████| 28/28 [00:02<00:00, 12.97it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 29.11it/s]\n",
        "Training Loss   : 0.03085\n",
        "Validation Loss : 0.00279\n",
        "Epoch 23/30\n",
        "----------\n",
        "100%|██████████| 28/28 [00:02<00:00, 12.41it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 27.02it/s]\n",
        "Training Loss   : 0.02741\n",
        "Validation Loss : 0.00197\n",
        "Epoch 24/30\n",
        "----------\n",
        "100%|██████████| 28/28 [00:02<00:00, 13.09it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 29.83it/s]\n",
        "Training Loss   : 0.02015\n",
        "Validation Loss : 0.00132\n",
        "Epoch 25/30\n",
        "----------\n",
        "100%|██████████| 28/28 [00:02<00:00, 13.08it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 29.30it/s]\n",
        "Training Loss   : 0.01844\n",
        "Validation Loss : 0.00229\n",
        "Epoch 26/30\n",
        "----------\n",
        "100%|██████████| 28/28 [00:02<00:00, 13.23it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 29.35it/s]\n",
        "Training Loss   : 0.01913\n",
        "Validation Loss : 0.00103\n",
        "Epoch 27/30\n",
        "----------\n",
        "100%|██████████| 28/28 [00:02<00:00, 12.50it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 27.89it/s]\n",
        "Training Loss   : 0.01545\n",
        "Validation Loss : 0.00076\n",
        "Epoch 28/30\n",
        "----------\n",
        "100%|██████████| 28/28 [00:02<00:00, 12.30it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 26.09it/s]\n",
        "Training Loss   : 0.01616\n",
        "Validation Loss : 0.00104\n",
        "Epoch 29/30\n",
        "----------\n",
        "100%|██████████| 28/28 [00:02<00:00, 12.28it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 25.84it/s]\n",
        "Training Loss   : 0.01504\n",
        "Validation Loss : 0.00092\n",
        "Epoch 30/30\n",
        "----------\n",
        "100%|██████████| 28/28 [00:02<00:00, 12.59it/s]\n",
        "100%|██████████| 8/8 [00:00<00:00, 25.87it/s]\n",
        "Training Loss   : 0.01006\n",
        "Validation Loss : 0.00047\n",
        "Training complete\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDQqBukYrdze",
        "outputId": "89e6a5ed-c5f1-4b53-fac1-6528f8157c67"
      },
      "outputs": [],
      "source": [
        "break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "x -> tensor([[15,  9,  8, 10,  3,  5, 13]])\n",
        "future_mask -> 7x7\n",
        "tensor([[ True, False, False, False, False, False, False],\n",
        "│   │   [ True,  True, False, False, False, False, False],\n",
        "│   │   [ True,  True,  True, False, False, False, False],\n",
        "│   │   [ True,  True,  True,  True, False, False, False],\n",
        "│   │   [ True,  True,  True,  True,  True, False, False],\n",
        "│   │   [ True,  True,  True,  True,  True,  True, False],\n",
        "│   │   [ True,  True,  True,  True,  True,  True,  True]])\n",
        "\n",
        "logits--> 1x7x18 because 1 sample\n",
        "tensor([[[  7.8,  -0.2,  -2.3,  -1.1,  -0.1,  -3.2,  -4.4,\n",
        "          -2.4,   3.7,  -0.9,  -5.1,  -4.5,  -5.6,  -2.2,\n",
        "          -0.5,  -4.2,  -2.9,  -4.9],\n",
        "        [  0.3,   3.7,   0.9,   1.7,   0.4,  -4.0,  -6.0,\n",
        "          -2.3,   8.5,   7.3,  -6.0,  -5.1,  -6.2,  -3.0,\n",
        "         -10.9,  -3.8,  -5.3,  -5.9],\n",
        "        [-10.5,  -0.4,   4.3,   2.4,  -6.3,  -8.9,  -0.1,\n",
        "           8.2,   8.6,   0.4,   1.2,   0.9,   0.7,   0.6,\n",
        "           6.9,   0.0,   0.4,   1.2],\n",
        "        [ -2.8,   9.6,   2.0,  -6.2,  -8.2,  -2.3,   5.7,\n",
        "           6.6,  -0.3,  -4.7,  -0.5,  -0.9,  -0.9,   1.2,\n",
        "           2.3,  -0.4,   0.1,  -1.5],\n",
        "        [ -2.9,   1.6,  -1.0,  -5.8,  -0.2,   6.2,  14.1,\n",
        "           8.0,  -4.0,  -9.7,  -2.1,  -3.4,  -3.2,  -1.4,\n",
        "           0.0,  -1.7,   0.0,  -3.0],\n",
        "        [ -9.4,   1.7,   5.4,  -1.3,  -6.6,  -4.7,   6.7,\n",
        "          10.2,   1.9,  -9.6,   0.8,   0.6,   0.7,   1.2,\n",
        "          10.2,   0.4,   1.3,   1.2],\n",
        "        [  0.3,  16.1,   3.2,  -4.4,  -5.7,  -2.9,  -3.7,\n",
        "          -6.1,  -2.1,   4.0,  -0.4,   0.1,  -0.4,   0.0,\n",
        "           0.6,  -0.6,  -1.2,  -0.7]]])\n",
        "\n",
        "logits.argmax(dim=-1) -> 1x7\n",
        "tensor([[0,  8,  8,  1,  6, 14,  1]])\n",
        "```\n",
        "\n",
        "`logits.argmax(dim=-1)` basically compress 1x7x18 to 1x7 where for each row of the\n",
        "7 rows, find the index that is maximum for example, first row 7.8 is max of all\n",
        "18 elements, so index 0 is returned. `tensor([[0,  8,  8,  1,  6, 14,  1]])`\n",
        "\n",
        "There is some meaning here too, remember our input `[15, 9, 8, 10, 3, 5, 13]`\n",
        "this is basically the BOS (15) up till the equal sign, then\n",
        "`[ 0, 8, 8, 1, 6, 14, 1]` is basically the prediction of each token what comes\n",
        "next.\n",
        "\n",
        "1. **Input Sequence**: Your input sequence is `[15, 9, 8, 10, 3, 5, 13]`. In\n",
        "   this context, `15` could be a special token like BOS (Beginning of Sentence)\n",
        "   or something else depending on your encoding scheme.\n",
        "\n",
        "2. **Output Tensor Interpretation**: The output tensor\n",
        "   `tensor([[ 0, 8, 8, 1, 6, 14, 1]])` represents the model's sequential\n",
        "   predictions for each step of the input:\n",
        "\n",
        "   - The first element `0` is the prediction following the first element `15` of\n",
        "     the input.\n",
        "   - The second element `8` is the prediction after seeing the first two\n",
        "     elements `15, 9` of the input.\n",
        "   - The third element `8` is predicted after seeing `15, 9, 8`.\n",
        "   - The fourth element `1` follows after `15, 9, 8, 10`.\n",
        "   - The sequence continues in this manner, with each new prediction based on an\n",
        "     increasingly longer prefix of the input sequence.\n",
        "\n",
        "3. **Sequential Predictions**: This output suggests that the model is working in\n",
        "   an autoregressive manner. It generates predictions one token at a time, and\n",
        "   each prediction is based on the sequence of tokens it has seen up to that\n",
        "   point.\n",
        "\n",
        "4. **Specific Meanings of Output Tokens**: The actual meaning of each token in\n",
        "   your output tensor (`0`, `8`, `1`, `6`, `14`, etc.) depends on your specific\n",
        "   encoding and task. In a language model, these would correspond to specific\n",
        "   words or characters. In a numerical context, they could represent numbers or\n",
        "   operations.\n",
        "\n",
        "In summary, the output tensor reflects the model's predictions for what comes\n",
        "next in the sequence, based on the current and all previous input tokens. Each\n",
        "element in the output is the model's guess for the next token, considering the\n",
        "sequence of tokens it has seen up to that point.\n",
        "\n",
        "> Then we move on to the concat operation:\n",
        "\n",
        "\n",
        "- In our model, after processing the input `[15, 9, 8, 10, 3, 5, 13]`, it\n",
        "  predicts the next token to be `1`. This prediction is based on the entire\n",
        "  sequence seen so far.\n",
        "\n",
        "- The process of extending the input sequence with this new token (`1`) and then\n",
        "  feeding this extended sequence back into the model for further predictions is\n",
        "  indeed an example of greedy decoding. The model is iteratively building a\n",
        "  longer sequence, one token at a time, always choosing the most likely next\n",
        "  token at each step.\n",
        "\n",
        "- This process would continue until a stopping condition is met, which might be\n",
        "  the prediction of an EOS (End of Sentence) token or reaching a maximum\n",
        "  sequence length.\n",
        "\n",
        "\n",
        "> for i in range(num_digits + 2):\n",
        "> now you know why loop over 4 times in total if num digits is 2.\n",
        "> This is because, after equal sign, we will have answer of 3 digits (xyz)\n",
        "> and an EOS token, our stop condition!\n",
        "\n",
        "Lastly: `tensor([[15,  9,  8, 10,  3,  5, 13,  1,  3,  3, 14]])` is the full predicted\n",
        "after EOS is met. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config.global_.seed = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def construct_future_mask(seq_len: int) -> torch.BoolTensor:\n",
        "    future_mask = torch.triu(torch.ones((seq_len, seq_len), dtype=torch.bool), diagonal=1).to(torch.bool)\n",
        "    future_mask = future_mask.contiguous()\n",
        "    future_mask = future_mask == 0\n",
        "    return torch.BoolTensor(future_mask)\n",
        "\n",
        "def construct_padding_mask(input_sequence: torch.Tensor, pad_token_id: int) -> torch.BoolTensor:\n",
        "    padding_mask = input_sequence != pad_token_id\n",
        "    return torch.BoolTensor(padding_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def compute_sum(model, x) -> List[int]:\n",
        "    \"Function for computing the sum of two numbers.\"\n",
        "    # x=[[15,  9,  8, 10,  3,  5, 13]]\n",
        "    for _ in range(num_digits + 2):\n",
        "        # pprint(x)\n",
        "        pad_mask = (x != PAD).view(1, 1, 1, x.size(-1)).to(DEVICE)\n",
        "        future_mask = construct_future_mask(seq_len=x.size(1))\n",
        "        batch_size, seq_len = x.size()\n",
        "        future_mask = future_mask.view(1, seq_len, seq_len).expand(size=(batch_size, -1, -1)).unsqueeze(1)\n",
        "        #print(pad_mask.shape, future_mask.shape)\n",
        "        #inputs, targets, target_padding_masks, future_masks = construct_batches(x)\n",
        "        #print(target_padding_masks.shape, future_masks.shape)\n",
        "        logits = model(input_tokens=x, target_padding_masks=pad_mask, future_masks=future_mask)\n",
        "        pprint(logits.shape)\n",
        "        time.sleep(100)\n",
        "        #logits = model(inputs, target_padding_masks=target_padding_masks, future_masks=future_masks)\n",
        "\n",
        "        last_output = logits.argmax(-1)[:, -1].view(1, 1)\n",
        "        x = torch.cat((x, last_output), 1).to(DEVICE)\n",
        "        # STOPPING CONDITION!\n",
        "        if last_output.item() == EOS:\n",
        "            break\n",
        "        #return\n",
        "    return x[0]\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, num_batch=None):\n",
        "    \"\"\"\n",
        "    Function for evaluation the model.\n",
        "\n",
        "    This function take equations, and truncate them up to the equal-sign, and feed\n",
        "    them to the model to get the predictions, compare them with the correct answers,\n",
        "    and output the accuracy.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    acc, count = 0, 0\n",
        "    num_wrong_to_display = 5\n",
        "    for idx, batch in enumerate(dataloader):\n",
        "        (\n",
        "            inputs,\n",
        "            targets,\n",
        "            target_padding_masks,\n",
        "            future_masks,\n",
        "        ) = batch  # construct_batches(batch)\n",
        "        for equation in inputs:\n",
        "            # pprint(equation)\n",
        "            # add EOS behind equation\n",
        "            equation = torch.cat((equation, torch.tensor([EOS])), 0) # TODO: PLEASE DO NOT DO THIS - DO NOT MODIFY LIKE THIS.\n",
        "            # fmt: off\n",
        "            loc_equal_sign = equation.tolist().index(EQUAL)\n",
        "            loc_EOS        = equation.tolist().index(EOS)\n",
        "            input          = equation[0 : loc_equal_sign + 1].view(1, -1).to(DEVICE)\n",
        "            ans            = equation[: loc_EOS + 1].tolist()\n",
        "            ans_pred       = compute_sum(model, input)\n",
        "            count += 1\n",
        "            # fmt: on\n",
        "\n",
        "            if ans == ans_pred.tolist():\n",
        "                acc += 1\n",
        "            else:\n",
        "                if num_wrong_to_display > 0:\n",
        "                    print(\n",
        "                        f'correct equation: {decode_equation(vocab=vocab, equation=equation).replace(\"<PAD>\",\"\")}'\n",
        "                    )\n",
        "                    print(f\"wrongly predicted as:        {decode_equation(vocab=vocab, equation=ans_pred)}\")\n",
        "                    num_wrong_to_display -= 1\n",
        "        if num_batch and idx > num_batch:\n",
        "            break\n",
        "    return acc / count\n",
        "\n",
        "\n",
        "def what_is(question: str) -> str:\n",
        "    \"function for computing the sum of two numbers with input in literal string format\"\n",
        "    pred = compute_sum(model, encode_equation(question, num_digits).view(1, -1))\n",
        "    pred = decode_equation(pred)\n",
        "    pred = pred[pred.index(\"=\") + 1 :]\n",
        "    return question + pred\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The provided code implements a form of greedy decoding for sequence generation.\n",
        "Let's break down how it aligns with the principles of greedy decoding:\n",
        "\n",
        "1. **Greedy Decoding Principle**: Greedy decoding in sequence generation models\n",
        "   involves choosing the most probable next token at each step of the sequence\n",
        "   generation. This is done iteratively until a stopping condition is met (like\n",
        "   reaching an EOS token or a maximum length).\n",
        "\n",
        "2. **Implementation in Your Code**:\n",
        "\n",
        "   - The `compute_sum` function generates a sequence by repeatedly predicting\n",
        "     the next token and appending it to the input.\n",
        "   - For each iteration in `compute_sum`:\n",
        "     - The model (`model(x, pad_mask, future_mask)`) generates logits for the\n",
        "       next token based on the current sequence (`x`).\n",
        "     - `last_output = logits.argmax(-1)[:, -1].view(1, 1)` picks the most\n",
        "       probable next token (the token with the highest logit value) from the\n",
        "       logits. This is the essence of greedy decoding.\n",
        "     - This token is then appended to the sequence:\n",
        "       `x = torch.cat((x, last_output), 1)`.\n",
        "   - The process continues until the model generates an EOS token, as indicated\n",
        "     by `if last_output.item() == EOS: break`.\n",
        "\n",
        "3. **Evaluation Function**:\n",
        "\n",
        "   - The `evaluate` function further confirms this approach by feeding truncated\n",
        "     sequences (up to the equal sign) from the dataloader to the `compute_sum`\n",
        "     function and comparing the model's predictions to the correct answers.\n",
        "\n",
        "4. **Characteristics of Greedy Decoding**:\n",
        "   - Greedy decoding is computationally efficient and straightforward but may\n",
        "     not always produce the best possible sequence. It does not reconsider past\n",
        "     decisions; it always picks the most likely next token at each step without\n",
        "     considering the global context of the sequence.\n",
        "\n",
        "In summary, the provided code, especially the `compute_sum` function, implements\n",
        "a typical greedy decoding approach. It iteratively generates a sequence by\n",
        "choosing the most probable next token at each step, which is characteristic of\n",
        "greedy decoding in sequence generation tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ujaa9mSNrdze"
      },
      "outputs": [],
      "source": [
        "print('training set examples the model gives an incorrect result:')\n",
        "# rng = torch.Generator().manual_seed(config.global_.seed)\n",
        "seed_all(1992, seed_torch=True)\n",
        "\n",
        "train_acc = evaluate(model, train_loader, 2)\n",
        "pprint(train_acc) #\n",
        "# print('validataion set examples the model gives an incorrect result:')\n",
        "val_acc = evaluate(model, valid_loader)\n",
        "pprint(val_acc)\n",
        "# print('test set examples the model gives an incorrect result:')\n",
        "test_acc = evaluate(model, test_loader)\n",
        "pprint(test_acc)\n",
        "# result = f'''train_size: {train_size}, test_acc: {test_acc}, val_acc: {val_acc}, train_acc: {train_acc}\n",
        "#                 '''\n",
        "# print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "QUESTION:\n",
        "\n",
        "another not so smart question of the day: For an input sequence x1,x2,...,x_L, when it forward pass all the way through the decoder model, up till before the pre-logits/head/linear layer, and assuming for simplicity that we squeeze out the first batch dimension (only 1 sample), the the shape of the pre-logits is [L, D] where L is seq len and D the hidden embedding dimension. Am I right to say that the last row of [L, D] being the last token's representation, holds info of the full context of all previous tokens.\n",
        "\n",
        "1. This means the last token in the input sequence (the last row in [L, D]) is a function of all previous tokens, so it is not surprising why the tutorial will just use the last row/token's corresponding prediction as the next predicted token/word, given all previous tokens.\n",
        "\n",
        "> Important to know the last token or last row of [L, D] is actually a function of all previous tokens, here it is unmasked already.\n",
        "> So if confused, just remember the pre logits last row, corresponding to the last token in the input sequence, is a function of all previous tokens.\n",
        "> It just means that row holds all information, context, of all previous tokens so we can say its conditioned on all previous tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbLTWPIRrdze"
      },
      "source": [
        "train acc: 0.021484375 , 0.0185546875\n",
        "\n",
        "non debug\n",
        "\n",
        "```\n",
        "correct equation: 24+86=110\n",
        "predicted:        24+86=100\n",
        "correct equation: 84+26=110\n",
        "predicted:        84+26=100\n",
        "validataion set examples the model gives an incorrect result:\n",
        "test set examples the model gives an incorrect result:\n",
        "train_size: 7000, train_loss: 0.013642309483007662,\n",
        "                val_loss: 0.0008140208410623018, test_loss: 0.00040599027124699205,\n",
        "                test_acc: 1.0, val_acc: 1.0, train_acc: 0.9996448863636364\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_10_'></a>[Adder Decoder Walkthrough](#toc0_)\n",
        "\n",
        "In a decoder-only model like GPT, the input sequence is essentially the target.\n",
        "The model aims to generate tokens that come after the given input, treating it\n",
        "as the \"history\" or \"context\" for the task of text generation. Unlike\n",
        "encoder-decoder models like the original Transformer, where the encoder\n",
        "processes a source sequence and the decoder generates a target sequence, a\n",
        "decoder-only model works solely with what would traditionally be considered the\n",
        "target sequence. Therefore, the padding mask applied to this input sequence is\n",
        "more aptly named \"target_padding_mask\" to maintain terminological consistency\n",
        "with the original Transformer architecture.\n",
        "\n",
        "Consequently, the input (source to beginners like me) padding masks is called\n",
        "the target padding masks for the following reasons:\n",
        "\n",
        "In a decoder-only architecture like GPT, the input sequence serves as the target\n",
        "sequence for which you want to generate subsequent tokens. Despite its role as\n",
        "an input to the model, it's termed as \"target\" because in the original\n",
        "Transformer architecture, the decoder's job is to generate the target sequence.\n",
        "Therefore, the mask that works on this input sequence in a decoder-only model\n",
        "should more aptly be named \"target_padding_mask.\" This naming maintains\n",
        "consistency with the Transformer architecture and clarifies that you're working\n",
        "on what is essentially the target of the model's generation task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_10_1_'></a>[Target Padding Mask (`target_padding_mask`)](#toc0_)\n",
        "\n",
        "- Definition: An attention mask to ignore pad-tokens in the source input. But in decoder only model, the source is the target.\n",
        "- Shape     : `(B, S)` or `(B, L)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yzuvux-rdzf"
      },
      "outputs": [],
      "source": [
        "pad_token_id = 16\n",
        "target_batch = torch.tensor(\n",
        "    [\n",
        "        [5, 7, 9, 16, 16],\n",
        "        [8, 6, 16, 16, 16],\n",
        "        [3, 12, 4, 11, 16],\n",
        "        [2, 1, 4, 16, 16],\n",
        "    ]\n",
        ")\n",
        "\n",
        "batch_size, seq_len = target_batch.size()\n",
        "\n",
        "target_padding_mask = target_batch != pad_token_id\n",
        "\n",
        "pprint(target_padding_mask)\n",
        "pprint(target_padding_mask.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_10_2_'></a>[Future Mask (`future_mask`)](#toc0_)\n",
        "\n",
        "```\n",
        ":param future_mask:\n",
        ":shape            : (L, L)\n",
        ":note             : Independent of batch size?\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzGyd-9Hrdzf"
      },
      "outputs": [],
      "source": [
        "seq_len = 5\n",
        "future_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
        "future_mask = future_mask == 0\n",
        "\n",
        "pprint(future_mask)\n",
        "pprint(future_mask.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGaeKY3Drdzf"
      },
      "source": [
        "One thing we need to know is that we need to do a matmul of `attention_weights` (emphasize the weights word here because it is indeed derived\n",
        "from weights although not explicit) and the value (the input seq). This attention weights has a preceding\n",
        "`attention_scores` prior to softmax, and we need to fill the tensors in this `attention_scores` with `-inf` because\n",
        "the softmax operation on `-inf` is zero, effectively zero out masked logits.\n",
        "\n",
        "Let's consider what zeroing out these masked logits actually does. The attention\n",
        "mechanism can be thought of as a weighted average of all the tokens in the input\n",
        "sequence. Each token is assigned a weight, with higher weights indicating more\n",
        "relevance to the token under consideration. If a certain token should not be\n",
        "considered at all (e.g., it's a future token that should not be visible to the\n",
        "current decoder step, or it's a padding token), its weight should be zero.\n",
        "\n",
        "In the case of a masked self-attention mechanism, as is often used in the\n",
        "decoder of a transformer, there are two main scenarios where masking comes into\n",
        "play:\n",
        "\n",
        "1. **Padding Tokens**: You don't want the attention mechanism to consider\n",
        "   padding tokens as they carry no useful information. If it did, it could skew\n",
        "   the resulting weighted average.\n",
        "\n",
        "2. **Future Tokens in Decoding**: In autoregressive decoding, the model\n",
        "   shouldn't have access to future tokens in the sequence when making\n",
        "   predictions. Otherwise, the model would cheat by peeking ahead.\n",
        "\n",
        "By setting the corresponding positions in the attention scores tensor to `-inf`\n",
        "and then applying a softmax, you effectively get a zero at those positions in\n",
        "the attention weights tensor. This results in completely ignoring those tokens\n",
        "when taking the weighted sum of the value vectors, thus implementing the desired\n",
        "masking behavior.\n",
        "\n",
        "To summarize, zeroing out masked logits ensures that the tokens corresponding to\n",
        "those logits do not contribute to the computed context, whether because they are\n",
        "padding or because they are future tokens that should not be visible to the\n",
        "model at a given time step.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG7eD_Ucrdzg"
      },
      "source": [
        "The purpose of applying `logical_and` between `target_padding_mask` and `future_mask` is to combine the constraints from both masks when calculating self-attention scores in the transformer's decoder. The `target_padding_mask` is designed to mask out the padding tokens in the input sequence, while the `future_mask` ensures that a given position cannot attend to future positions in the sequence. By combining these masks, you can perform the necessary masking for both padding and future tokens in a single step.\n",
        "\n",
        "Here's how it works:\n",
        "\n",
        "1. `target_padding_mask`: Masks out the padding tokens so that they don't contribute to the attention calculations. True values mean \"attend to this token,\" and False values mean \"ignore this token.\"\n",
        "  \n",
        "2. `future_mask`: The future mask is created as a lower triangular matrix, where the lower triangle, including the diagonal, is filled with ones, and the upper triangle is filled with zeros. Masks out future tokens in a sequence so that a token at a given position can only attend to positions that come before it (and itself). True values mean \"attend to this token,\" and False values mean \"ignore this token.\"\n",
        "\n",
        "3. `logical_and(target_padding_mask, future_mask)`: Combines the two masks. A True in the resulting mask means that the condition for both padding and future attention is satisfied.\n",
        "\n",
        "By combining these two masks, the decoder obeys the autoregressive property, ensuring it doesn't see future tokens, while also ignoring padding tokens in the input sequence. We may term it the `target_mask`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n7fIAcdrdzg"
      },
      "source": [
        "Same mask applied to all h heads.\n",
        "Same mask applied to all h heads.\n",
        "Same mask applied to all h heads.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_10_3_'></a>[Example of Source Padding and Future Masks](#toc0_)\n",
        "\n",
        "#### <a id='toc1_10_3_1_'></a>[First Sample First Token](#toc0_)\n",
        "\n",
        "- `target_padding_mask` has size of `[4, 5]`.\n",
        "  - We zoom in to the first row (sample) which is of length 5.\n",
        "  - This length 5 is the sequence length, which is `T, T, T, F, F` indicating the last 2 tokens being padded.\n",
        "- `future_mask` has size of `[5, 5]`.\n",
        "  - We note that this is indepedent of batch size. Each sample should have the same future mask shape of `[L, L]`.\n",
        "  - This `L=5` should necessary be same for the sequence length in `target_padding_mask`.\n",
        "- First, let's consider one batch of 4 samples. What we do first is to broadcast `future_mask` to `[4, 5, 5]` because we want each sample/row in the batch to have the same future mask. As shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iV9EB14Srdzg"
      },
      "outputs": [],
      "source": [
        "pprint(future_mask)\n",
        "future_mask = future_mask.view(1, seq_len, seq_len).expand(size=(batch_size, -1, -1))\n",
        "pprint(future_mask)\n",
        "pprint(future_mask.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0Z8Cy-urdzh"
      },
      "source": [
        "- Now, we can zoom in to one particular sample since both `target_padding_mask` and `future_mask` have the same first dimension of batch size.\n",
        "- What is incomplete is that we need to broadcast `target_padding_mask`'s last dimension to have the same dimensions as `future_mask`. This means we broadcast `[4, 5]` to `[4, 5, 5]`. But why?\n",
        "- For simplicity, we slice the first same of both below.\n",
        "- The first row of the `future_mask` of the first sample is `T, F, F, F, F`. This corresponds to what? This is the future mask of the first token in the sequence. Well, that is confusing, because it apparently have 5 elements, and has \"information\" of the other 4 tokens in the sequence. Let's explain in details below:\n",
        "  - Regarding the first row of the `future_mask` in the first sample, which is `[T, F, F, F, F]`, it might initially seem confusing why there are 5 elements. Each of these elements, in fact, corresponds to whether the first token can attend to other tokens at each respective position in the sequence. Here's how to interpret it:\n",
        "    - The first element (`True`) indicates that the first token can attend to itself.\n",
        "    - The next four elements (`False`) specify that the first token should not attend to any of the future tokens in the sequence.\n",
        "- Consequently, what is the first token in the sequence of the `target_padding_mask`? Recall earlier we mentioned that the first sample's `target_padding_mask` is `T, T, T, F, F` and therefore the first token in the sequence is `T`.\n",
        "- What do we want to achieve here? We want to make sure that the model does not **attend** to tokens in the sequence that are masked with `False`.\n",
        "- In other words, the first token in the sequence of the first sample has `target_padding_mask` of `T` and `future_masks` of `T, F, F, F, F`.\n",
        "- We need to broadcast this `T` to `T, T, T, T, T` to align with `T, F, F, F, F` because? Because we need ensure that this first token in the sequence is also able to considered in relation to every other token in the sequence.\n",
        "- So the first token is not a padded token, which is `T`, similarly, the first token needs to attend to itself at the first position, hence `T` and `T` give `T`. But for the second `T` in the now broadcasted `target_padding_mask`, it is still representing the first token or?\n",
        "- Broadcasting the first token's `target_padding_mask` value of `T` to `[T, T, T, T, T]` ensures that when this first token is being considered for attention computations, it is free to attend to any position, barring any restrictions set by `future_mask`.\n",
        "- Tricky: after broadcasting, each `T` in `[T, T, T, T, T]` is still representing the first token. They indicate that when the first token is compared with *any* token in the sequence (including itself), it is not a padding token. The element-wise `AND` with the `future_mask` then further refines this by restricting it from attending to future tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqRW6JMSrdzh"
      },
      "outputs": [],
      "source": [
        "pprint(target_padding_mask)\n",
        "pprint(target_padding_mask[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDoSC5wOrdzh"
      },
      "outputs": [],
      "source": [
        "pprint(target_padding_mask)\n",
        "target_padding_mask = target_padding_mask.view(batch_size, 1, seq_len).expand(size=(batch_size, seq_len, seq_len))\n",
        "pprint(target_padding_mask)\n",
        "pprint(target_padding_mask.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcXfdJ7Frdzh"
      },
      "outputs": [],
      "source": [
        "pprint(target_padding_mask[0])\n",
        "pprint(future_mask[0])\n",
        "pprint(target_padding_mask[0] & future_mask[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <a id='toc1_10_3_2_'></a>[First Sample Fourth Token](#toc0_)\n",
        "\n",
        "Now let's look at another example—the 4th token in the sequence, where `target_padding_mask = [T, T, T, F, F]` and `future_mask` is a lower triangular matrix with `True`s.\n",
        "\n",
        "1. **4th Token's target_padding_mask**: The 4th token has a value of `F` in `target_padding_mask`, indicating it's a padding token.\n",
        "   \n",
        "2. **4th Row of future_mask**: The 4th row in `future_mask` is `[True, True, True, True, False]`. This means that if this token were not a padding token, it would be allowed to attend to all the previous tokens in the sequence and itself, but not to any future token.\n",
        "\n",
        "3. **Broadcast target_padding_mask**: To align `target_padding_mask` with `future_mask`, we'd broadcast `F` from the `target_padding_mask` to `[F, F, F, F, F]`. This way, when we consider the 4th token in relation to any other token in the sequence, it's still marked as a padding token.\n",
        "\n",
        "4. **Element-wise AND with future_mask**: After broadcasting, you'd perform an element-wise AND between `[F, F, F, F, F]` and `[True, True, True, True, False]`, resulting in `[F, F, F, F, F]`.\n",
        "\n",
        "5. **Interpretation**: This effectively means that the 4th token won't attend to any other token in the sequence, and no token will attend to it either, as it is a padding token.\n",
        "\n",
        "So, the masks are doing their jobs correctly: the `target_padding_mask` indicates whether each token is a padding token or not, and `future_mask` dictates the \"rules\" of attention regarding what each token can attend to. Combining them ensures that both conditions are met."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_10_4_'></a>[Further Add a Singleton Dimension in Masks](#toc0_)\n",
        "\n",
        "Now both masks are of shape: `(B, L, L)` but we need to add a singleton dimension to the last dimension to make it `(B, 1, L, L)`.\n",
        "\n",
        "In deep learning frameworks like PyTorch, the dimensions of the tensors involved\n",
        "in operations like matrix multiplication or attention mechanisms often have\n",
        "specific semantic meanings. In the context of attention mechanisms, especially\n",
        "in the transformer architecture, the attention mask usually has a shape that is\n",
        "compatible with the attention logits for element-wise multiplication.\n",
        "\n",
        "In the transformer model, the attention logits are often computed as a dot\n",
        "product between query and key vectors, resulting in a tensor of shape\n",
        "`(Batch size, Num heads, Sequence length, Sequence length)` or `(B, H, L, L)`.\n",
        "Here, `B` is the batch size, `H` is the number of attention heads, and `L` is\n",
        "the sequence length.\n",
        "\n",
        "To make the mask tensor compatible for element-wise operations with this 4D\n",
        "tensor, it needs to have a shape that can be broadcasted to `(B, H, L, L)`. A\n",
        "mask of shape `(B, 1, L, L)` fulfills this requirement.\n",
        "\n",
        "The singleton dimension is added so that the mask can be easily broadcast to the\n",
        "shape of the attention logits tensor during the computation. When a tensor with\n",
        "shape `(B, 1, L, L)` is element-wise multiplied with a tensor of shape\n",
        "`(B, H, L, L)`, the singleton dimension (the `1`) allows the mask to be used for\n",
        "each attention head without explicitly replicating the mask `H` times. This is\n",
        "more memory-efficient and often faster.\n",
        "\n",
        "Thus, adding a singleton dimension in masks is a preparatory step that allows\n",
        "for efficient element-wise operations later in the model's forward pass.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_8uVfTRrdzi"
      },
      "outputs": [],
      "source": [
        "target_padding_mask = target_padding_mask.unsqueeze(1)\n",
        "pprint(target_padding_mask.shape)\n",
        "\n",
        "future_mask = future_mask.unsqueeze(1)\n",
        "pprint(future_mask.shape)\n",
        "\n",
        "target_mask = target_padding_mask & future_mask\n",
        "pprint(target_mask.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_10_5_'></a>[MultiHeadAttention](#toc0_)\n",
        "\n",
        "We start off by understanding the rationale of the following block:\n",
        "\n",
        "```python\n",
        "Q = self.W_Q(query).contiguous() # Z @ W_Q -> BxLxD @ DxD = BxLxD\n",
        "K = self.W_K(key).contiguous()   # Z @ W_K\n",
        "V = self.W_V(value).contiguous() # Z @ W_V\n",
        "```\n",
        "\n",
        "#### <a id='toc1_10_5_1_'></a>[A Primer](#toc0_)\n",
        "\n",
        "In the context of the Transformer architecture and self-attention mechanism, the\n",
        "matrices $\\mathbf{W}^{Q}, \\mathbf{W}^{K},$ and $\\mathbf{W}^{V}$ are learnable\n",
        "parameters designed to project the input embeddings $\\mathbf{Z}$ into distinct\n",
        "subspaces tailored for attention calculations. Let's explore their purpose and\n",
        "their resulting transformations:\n",
        "\n",
        "1. **The Role of Weights**:\n",
        "\n",
        "   - $\\mathbf{W}^{Q}$: Projects input embeddings into a query subspace,\n",
        "     determining the type of information each token seeks from others.\n",
        "   - $\\mathbf{W}^{K}$: Positions the embeddings in a key subspace, highlighting\n",
        "     the token features that others would search for.\n",
        "   - $\\mathbf{W}^{V}$: Transforms embeddings into a value subspace, showcasing\n",
        "     the actual token content to be aggregated by the attention scores.\n",
        "\n",
        "2. **Intuitive & Mathematical Interpretations**:\n",
        "\n",
        "   - **Query Transformation** ($\\mathbf{Z} \\mathbf{W}^{Q}$): Intuitively, it\n",
        "     tailors the raw embeddings to optimally question the rest of the sequence.\n",
        "     Mathematically, it's a linear transformation of the embedding space into\n",
        "     the query space, akin to a high-dimensional rotation and scaling,\n",
        "     emphasizing aspects relevant to querying.\n",
        "\n",
        "   - **Key Transformation** ($\\mathbf{Z} \\mathbf{W}^{K}$): Intuitively, it\n",
        "     accentuates token features that other tokens might seek. Mathematically,\n",
        "     it's another linear transformation emphasizing aspects that make tokens\n",
        "     searchable.\n",
        "\n",
        "   - **Value Transformation** ($\\mathbf{Z} \\mathbf{W}^{V}$): Intuitively, it\n",
        "     prepares tokens to share their intrinsic content when beckoned by the\n",
        "     attention mechanism. Mathematically, it's a linear transformation\n",
        "     accentuating token content aspects.\n",
        "\n",
        "3. **Creating Q, K, V**:\n",
        "\n",
        "   - $\\mathbf{Q} = \\mathbf{Z} \\mathbf{W}^{Q}$\n",
        "   - $\\mathbf{K} = \\mathbf{Z} \\mathbf{W}^{K}$\n",
        "   - $\\mathbf{V} = \\mathbf{Z} \\mathbf{W}^{V}$\n",
        "\n",
        "   These operations recast the embedded tokens into roles for the attention\n",
        "   mechanism:\n",
        "\n",
        "   - $\\mathbf{Q}$: Information seekers. The queries are seeking information, and\n",
        "     the computation $Q @ K^T$ finds how much each part of the input (holder)\n",
        "     should be attended to.\n",
        "   - $\\mathbf{K}$: Information gatekeepers. The keys hold the information being\n",
        "     sought, and their arrangement in space defines the subspace that the\n",
        "     queries are projected onto to find these relevance scores.\n",
        "   - $\\mathbf{V}$: Information providers. The values contain the content that\n",
        "     needs to be retrieved, and once we have the attention weights, we know how\n",
        "     much of each value to retrieve and combine to form the output.\n",
        "\n",
        "   Mathematically, the resulting matrices ($\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}$)\n",
        "   have rows that represent different aspects (querying, key, value) of the\n",
        "   original tokens.\n",
        "\n",
        "4. **Relevance to Self-Attention**:\n",
        "\n",
        "   The transformations set the stage for attention score calculations. In this\n",
        "   step, each query vector in $\\mathbf{Q}$ computes its similarity (via dot\n",
        "   product) against all key vectors in $\\mathbf{K}$. This score matrix reveals\n",
        "   the attention weightage for each token regarding every other token in the\n",
        "   sequence.\n",
        "\n",
        "   Specifically, $\\mathbf{Q} @ \\mathbf{K}^T$ calculates how each token (query)\n",
        "   aligns with every other token (key). It's akin to measuring the relevance of\n",
        "   each word to every other word in the sequence.\n",
        "\n",
        "   After normalizing these scores (typically with softmax), we get the attention\n",
        "   weights. These weights guide how the value vectors in $\\mathbf{V}$ are\n",
        "   aggregated. The outcome is a new matrix where each row aggregates\n",
        "   contextually relevant information from the entire sequence. This enriched\n",
        "   output feeds into subsequent transformer layers for further processing.\n",
        "\n",
        "Overall, by using the $\\mathbf{W}^{Q}, \\mathbf{W}^{K},$ and $\\mathbf{W}^{V}$\n",
        "matrices, the transformer fine-tunes its focus on inter-token relationships,\n",
        "enabling the model to capture intricate contextual nuances within a given\n",
        "sequence.\n",
        "\n",
        "#### <a id='toc1_10_5_2_'></a>[An Example](#toc0_)\n",
        "\n",
        "Let's use the sentence \"The cat walks by the bank\" to walk through the\n",
        "self-attention mechanism with analogies and to clarify how it works step by\n",
        "step.\n",
        "\n",
        "**Setting the Scene (Embedding the Sentence):** Imagine each word in the\n",
        "sentence is a person at a party (our tokens). They start by telling a basic fact\n",
        "about themselves (their initial embedding).\n",
        "\n",
        "**The Roles:**\n",
        "\n",
        "- **Q (Seekers)**: Each person (word) is curious about the stories (contexts) of\n",
        "  others at the party. They have their own perspective or question (Q vector).\n",
        "- **K (Holders)**: At the same time, each person has a name tag with keywords\n",
        "  that describe their story (K vector).\n",
        "- **V (Retrievers)**: They also hold a bag of their experiences (V vector),\n",
        "  ready to share.\n",
        "\n",
        "**Transformations (Applying W Matrices):** We give each person a set of glasses\n",
        "(the matrices $W_Q, W_K, W_V$) that changes how they see the world (the space\n",
        "they project to).\n",
        "\n",
        "- With $W_Q$ glasses, they focus on what they want to know from others.\n",
        "- With $W_K$ glasses, they highlight their name tag details, making some\n",
        "  features stand out more.\n",
        "- With $W_V$ glasses, they prepare to share the contents of their bag\n",
        "  effectively.\n",
        "\n",
        "**Attention (Calculating Q @ K.T):** Now, each person looks around the room\n",
        "(sequence) with their $W_Q$ glasses and sees the highlighted name tags (after\n",
        "$W_K$ transformation) of everyone else. They measure how similar their question\n",
        "is to the others' name tags—this is the dot product $Q @ K^T$.\n",
        "\n",
        "For \"cat,\" let’s say it’s curious about the notion of \"walking\" and \"bank.\" It\n",
        "will measure the similarity (attention scores) between its curiosity and the\n",
        "name tags of \"walks,\" \"by,\" \"the,\" \"bank.\"\n",
        "\n",
        "**Normalization (Softmax):** After measuring, \"cat\" decides how much to focus on\n",
        "each story—this is softmax. Some stories are very relevant (\"walks\"), some\n",
        "moderately (\"by,\" \"the\"), and some might be highly relevant depending on context\n",
        "(\"bank\" — is it a river bank or a financial institution?).\n",
        "\n",
        "**Retrieval (Applying Attention to V):** Now \"cat\" decides to listen to the\n",
        "stories in proportion to its focus. It takes pieces (weighted by attention\n",
        "scores) from each person's experience bag (V vectors) and combines them into a\n",
        "richer, contextual understanding of itself in the sentence. This combination\n",
        "gives us the new representation of \"cat,\" informed by the entire context of the\n",
        "sentence.\n",
        "\n",
        "In essence:\n",
        "\n",
        "- **Q (Query):** What does \"cat\" want to know?\n",
        "- **K (Key):** Who has relevant information to \"cat\"’s curiosity?\n",
        "- **V (Value):** What stories does \"cat\" gather from others, and how much does\n",
        "  it take from each to understand its role in the sentence?\n",
        "\n",
        "The output of self-attention for \"cat\" now encapsulates not just \"cat\" but its\n",
        "relationship and relevance to \"walks,\" \"by,\" \"the,\" \"bank\" in a way that no\n",
        "single word could convey alone. This output then becomes the input to the next\n",
        "layer, where the process can repeat, enabling the model to develop an even more\n",
        "nuanced understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"transformer.png\" width=\"600\">\n",
        "\n",
        "\n",
        "### <a id='toc1_10_6_'></a>[AddNorm (Residual Connection + Layer Normalization)](#toc0_)\n",
        "\n",
        "- https://www.d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html#residual-connection-and-layer-normalization\n",
        "- https://nlp.seas.harvard.edu/annotated-transformer\n",
        "\n",
        "#### <a id='toc1_10_6_1_'></a>[Residual Block](#toc0_)\n",
        "\n",
        "A residual block takes an input $X$ and a sub-layer (or function) $f$, and computes $X + f(X)$.\n",
        "\n",
        "```python\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        sublayer: Callable[[torch.Tensor], torch.Tensor],\n",
        "    ) -> torch.Tensor:\n",
        "        return x + sublayer(x)\n",
        "```\n",
        "\n",
        "The intuition behind a residual block is to facilitate the training of deeper networks by providing a \"shortcut\" or \"skip connection\" that allows the gradient to be directly backpropagated to earlier layers. Essentially, in a standard deep learning model, each layer transforms its input. As the network depth increases, these transformations can degrade the network's performance, mainly due to the vanishing or exploding gradient problems. This makes it challenging to train very deep networks.\n",
        "\n",
        "The residual block aims to address this problem. It adds the original input back to the output of the network layer, forming $F(x) + x$ instead of just $F(x)$. Mathematically, if $x$ is the input and $F(x)$ is the transformed version, then the residual block computes $F(x) + x$.\n",
        "\n",
        "This architecture has a few advantages:\n",
        "\n",
        "1. **Easier Learning**: During training, if the best transformation is an identity map (i.e., the output should be the same as the input), the residual block can easily learn this. The layers in $F(x)$ only need to learn to approximate zero in this case, which is generally easier than learning an identity map in a traditional stack of layers.\n",
        "\n",
        "2. **Mitigating Vanishing/Exploding Gradients**: The skip connections provide an unobstructed path for the gradients to flow, which can help mitigate the vanishing or exploding gradient problems in very deep networks.\n",
        "\n",
        "3. **Enabling Deeper Networks**: Because of the above advantages, residual blocks make it possible to train very deep networks effectively. Deep networks can represent very complex functions, which can be advantageous for many tasks.\n",
        "\n",
        "4. **Parameter Efficiency**: Residual blocks often require fewer parameters to achieve similar performance compared to traditional deep networks, making them more parameter-efficient.\n",
        "\n",
        "In summary, the residual block is a simple yet effective idea that has enabled the training of much deeper networks, thereby pushing the boundaries of what is achievable in various machine learning tasks.\n",
        "\n",
        "#### <a id='toc1_10_6_2_'></a>[Layer Normalization](#toc0_)\n",
        "\n",
        "Layer normalization normalizes the features across the feature dimension. Given the feature $X$ with shape $[B, L, D]$ (where $B$ is the batch size, $L$ is the sequence length, and $D$ is the feature dimension), layer normalization computes:\n",
        "\n",
        "$$\n",
        "\\text{Norm}(X) = \\frac{X - \\text{mean}(X)}{\\sqrt{\\text{var}(X) + \\epsilon}} \\times \\gamma + \\beta\n",
        "$$\n",
        "\n",
        "Where $\\gamma$ and $\\beta$ are learnable parameters and $\\epsilon$ is a small constant for numerical stability.\n",
        "\n",
        "```python\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, feature_dim: int, eps: float = 1e-6) -> None:\n",
        "        super().__init__()\n",
        "        # fmt: off\n",
        "        self.gamma = nn.Parameter(torch.ones(feature_dim))\n",
        "        self.beta  = nn.Parameter(torch.zeros(feature_dim))\n",
        "        self.eps   = eps\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std  = x.std(dim=-1, keepdim=True)\n",
        "        # fmt: on\n",
        "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
        "```\n",
        "\n",
        "#### <a id='toc1_10_6_3_'></a>[Combining Both](#toc0_)\n",
        "\n",
        "Finally, you can combine these into a single block, much like the `ResidualConnection` or `AddNorm` classes you mentioned earlier.\n",
        "\n",
        "```python\n",
        "class AddNorm(nn.Module):\n",
        "    def __init__(self, feature_dim, dropout_rate):\n",
        "        super(AddNorm, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.layer_norm = LayerNorm(feature_dim)\n",
        "\n",
        "    def forward(self, x, sublayer_output):\n",
        "        return self.layer_norm(x + self.dropout(sublayer_output))\n",
        "```\n",
        "\n",
        "This `AddNorm` class applies dropout to the output of the sub-layer, adds it to the original input, and then applies layer normalization. Note that this version doesn't include an embedded layer normalization operation in the residual block; instead, it utilizes a separate layer normalization class, which is then used in the `AddNorm` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_10_7_'></a>[How Loss is Computed?](#toc0_)\n",
        "\n",
        "The unreduced loss for the Cross Entropy calculation is given by:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\mathcal{X}, \\mathcal{Y}) = \\{l_1, \\ldots, l_N\\}^\\top, \\quad l_n = -\\mathcal{W}_{\\mathcal{Y}_n} \\cdot \\log \\left( \\frac{\\exp(\\mathcal{X}_{n, \\mathcal{Y}_n})}{\\sum_{c=1}^\\mathcal{C} \\exp(\\mathcal{X}_{n, c})} \\right) \\cdot \\mathbb{1}\\{\\mathcal{Y}_n \\neq \\text{ignore\\_index}\\}\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- $\\mathcal{X}$ is the input tensor of logits, with shape \\([B, d_1, \\ldots,\n",
        "  d_K, \\mathcal{C}]\\) where $\\mathcal{C}$ is the number of classes and\n",
        "  $[d_1, \\ldots, d_K]$ represent any additional dimensions.\n",
        "- $\\mathcal{Y}$ is the target tensor of class indices, with shape \\([B, d_1,\n",
        "  \\ldots, d_K]\\).\n",
        "- $\\mathcal{W}$ is a tensor of weights corresponding to class indices.\n",
        "- $N$ is the product of the batch size and any additional dimensions, i.e.,\n",
        "  $N = B \\times d_1 \\times \\ldots \\times d_K$. It spans all elements in the\n",
        "  batch and across the additional dimensions, effectively flattening these into\n",
        "  a single dimension for the loss calculation.\n",
        "\n",
        "For the reduced loss, the calculation depends on the reduction method ('mean' or\n",
        "'sum'). The mean reduction averages the loss over all $N$ elements, while the\n",
        "sum reduction simply sums over them:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\mathcal{X}, \\mathcal{Y}) =\n",
        "\\begin{cases}\n",
        "\\sum_{n=1}^N \\left( \\frac{l_n}{\\sum_{n=1}^N \\mathcal{W}_{\\mathcal{Y}_n} \\cdot \\mathbb{1}\\{\\mathcal{Y}_n \\neq \\text{ignore\\_index}\\}} \\right), & \\text{if reduction = 'mean'}\\\\\n",
        "\\sum_{n=1}^N l_n, & \\text{if reduction = 'sum'}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "This formulation emphasizes that the loss is computed element-wise for each\n",
        "class index in the target tensor $\\mathcal{Y}$, and then either summed or\n",
        "averaged depending on the chosen reduction method. The indicator function\n",
        "$\\mathbb{1}\\{\\}$ ensures that the ignore_index is not considered in the loss\n",
        "computation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFbzkjE0zKJe"
      },
      "source": [
        "1. **Define the Loss Function**: The `nn.CrossEntropyLoss` function:\n",
        "    - `nn.CrossEntropyLoss` in PyTorch expects the input logits to be of shape\n",
        "    `[N, C, d1, d2, ..., dK]` (where `N` is the batch size, `C` is the number of\n",
        "    classes, and `d1` to `dK` are optional additional dimensions) and the target\n",
        "    to be of shape `[N, d1, d2, ..., dK]`.\n",
        "    - Let's look a simplified example in image classification. The target is a\n",
        "    single integer representing the class label, and the input logits are a\n",
        "    vector of length `C` (the number of classes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25BMnAW5zKJe"
      },
      "outputs": [],
      "source": [
        "rng = torch.Generator().manual_seed(config.global_.seed)\n",
        "criterion = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "targets = torch.tensor([1, 0, 0, 0]) # indicating sample 1 is class 1 and sample 2 is class 0\n",
        "logits  = torch.tensor([[0.1, 0.9], [0.9, 0.1], [0.8, 0.2], [0.3, 0.7]])\n",
        "loss   = criterion(logits, targets)\n",
        "pprint(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFIW_XfMzKJe"
      },
      "source": [
        "Here things are simple, because the target is a single integer representing the class label, and the input logits are a vector of length `C` (the number of classes).\n",
        "\n",
        "The confusion arises when the target is a sequence of integers, as in the case of sequence-to-sequence prediction. In this case, the target is a sequence of integers representing the class labels, and the input logits are a sequence of vectors of length `C` (the number of classes).\n",
        "\n",
        "Let's walk through an example for concrete understanding.\n",
        "\n",
        "Consider the following example:\n",
        "\n",
        "- Batch size: 2\n",
        "- Sequence length: 3\n",
        "- Number of classes/Vocab size: 4\n",
        "- Targets is of shape: `[B, L] = [2, 3]`\n",
        "- Logits is of shape: `[B, L, V] = [2, 3, 4]` where `V` is `C` in the above definition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASUDASA_zKJe"
      },
      "outputs": [],
      "source": [
        "# fmt: off\n",
        "rng        = torch.Generator().manual_seed(config.global_.seed)\n",
        "\n",
        "B, L, V    = 2, 3, 4                                                   # Assuming we have B = batch size, L = sequence length, V = vocab size\n",
        "\n",
        "logits     = torch.randn(B, L, V, generator=rng)                       # logits from the head\n",
        "targets    = torch.randint(low=0, high=V, size=(B, L), generator=rng)  # targets are the labels\n",
        "# fmt: on\n",
        "\n",
        "pprint(logits)\n",
        "pprint(targets)\n",
        "pprint(logits[0]) # logits for the first sequence [L=10, V=18]\n",
        "pprint(targets[0]) # target for the first sequence [L=10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTPmANxxzKJe"
      },
      "source": [
        "We establish some conceptual understanding first:\n",
        "\n",
        "- Each sample in the batch has the following characteristics:\n",
        "    - Denote `target` and `logit` as the target and logits for a particular sample in the batch.\n",
        "    - The `target` is of shape `[L] = [3]` and each element is the class/vocab label for each token in the sequence.\n",
        "    - The `logit` is of shape `[L, V] = [3, 4]` and each row is the logits for each token in the sequence.\n",
        "    - Therefore, we want to compare each row in `logit` with each element in `target` to compute the loss.\n",
        "    - We can think of each row in `logit` as the prediction for each token in the sequence, and each element in `target` as the ground truth for each token in the sequence.\n",
        "    - Intuitively this means that within each sample, there are many \"sub-samples\" where each sub-sample is a token in the sequence. If you can visualize this, then there should be no confusion.\n",
        "- In code, we can do so with the following manner:\n",
        "    - Calculate loss for each token in each sample individually and then sum them up.\n",
        "    - Reduction by mean will mean we need to divide our `total_loss` by the total number\n",
        "        of samples in the batch. But remember that even though technically we have\n",
        "        2 samples in the batch, we are actually treating each token in each sample\n",
        "        as a sub-sample, so the total samples is `B * L` where `B` is the batch size\n",
        "        and `L` is the sequence length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHWXzJ59zKJe"
      },
      "outputs": [],
      "source": [
        "criterion  = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "\n",
        "total_loss = 0\n",
        "for b in range(B):\n",
        "    for l in range(L):\n",
        "        logit      = logits[b, l].unsqueeze(0)\n",
        "        target     = targets[b, l].unsqueeze(0)\n",
        "        total_loss += criterion(logit, target)\n",
        "\n",
        "pprint(total_loss)\n",
        "total_loss  = total_loss / (B * L)\n",
        "pprint(total_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhQrjELfzKJf"
      },
      "source": [
        "In PyTorch however, if you have a logits tensor of shape `[B, S, V]`, you need to permute it to\n",
        "  `[B, V, S]` to align with the format that `CrossEntropyLoss` wants, so that `V` (vocab size) is\n",
        "  treated as `C` (number of classes), and `S` (sequence length) is treated as\n",
        "  one of the additional dimensions `d1, d2, ..., dK`.\n",
        "\n",
        "But all in all, if you understood the previous loop to calculate the loss for each token in each sample individually and then sum them up, then dividing to fulfill reduction of mean, then you should be fine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lwkq0QDOzKJf"
      },
      "outputs": [],
      "source": [
        "# Permute logits to shape [B, V, S]\n",
        "logits_permuted = logits.permute(0, 2, 1)\n",
        "\n",
        "# Instantiate the CrossEntropyLoss\n",
        "# By default, it reduces by averaging the losses over each observation in the input\n",
        "criterion = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "\n",
        "loss = criterion(logits_permuted, targets)\n",
        "pprint(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Masking and Ignore Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fmt: off\n",
        "rng        = torch.Generator().manual_seed(config.global_.seed)\n",
        "\n",
        "B, L, V    = 2, 3, 4                                                   # Assuming we have B = batch size, L = sequence length, V = vocab size\n",
        "\n",
        "logits     = torch.randn(B, L, V, generator=rng)                       # logits from the head\n",
        "targets    = torch.randint(low=0, high=V, size=(B, L), generator=rng)  # targets are the labels\n",
        "# fmt: on\n",
        "\n",
        "pprint(logits)\n",
        "pprint(targets)\n",
        "pprint(logits[0]) # logits for the first sequence [L=10, V=18]\n",
        "pprint(targets[0]) # target for the first sequence [L=10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "targets[:, 0] = -123"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PAD_ = -123"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion  = nn.CrossEntropyLoss(reduction=\"mean\", ignore_index=PAD_)\n",
        "\n",
        "NON_IGNORE_COUNT = 0\n",
        "\n",
        "total_loss = 0\n",
        "for b in range(B):\n",
        "    for l in range(L):\n",
        "        logit      = logits[b, l].unsqueeze(0)\n",
        "        target     = targets[b, l].unsqueeze(0)\n",
        "        if target == torch.tensor([PAD_]):\n",
        "            continue\n",
        "        total_loss += criterion(logit, target)\n",
        "        NON_IGNORE_COUNT += 1\n",
        "\n",
        "pprint(total_loss)\n",
        "total_loss  = total_loss / NON_IGNORE_COUNT\n",
        "pprint(total_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTE: `NON_IGNORE_COUNT` is used instead of `BxL`, why? Cause we are averaging over\n",
        "all non-ignored guys!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Permute logits to shape [B, V, S]\n",
        "logits_permuted = logits.permute(0, 2, 1)\n",
        "\n",
        "# Instantiate the CrossEntropyLoss\n",
        "# By default, it reduces by averaging the losses over each observation in the input\n",
        "criterion  = nn.CrossEntropyLoss(reduction=\"mean\", ignore_index=PAD_)\n",
        "\n",
        "loss = criterion(logits_permuted, targets)\n",
        "pprint(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Why mask our target in Adder?\n",
        "\n",
        "Well simply put, we do not care what the model predict for anything before the equal sign.\n",
        "\n",
        "For example\n",
        "\n",
        "```\n",
        "12+97=109\n",
        "```\n",
        "\n",
        "and still the \n",
        "\n",
        "```\n",
        "x = [BOS,1,2,+,9,7,=,1,0,9]\n",
        "y = [1  ,2,+,9,7,=,1,0,9,EOS]\n",
        "```\n",
        "\n",
        "requires us to predict tokens given say, BOS, given say, 1,2,+,9, 7. What we want is\n",
        "for it to predict what is next after `=`, so earlier guys all ignore.\n",
        "\n",
        "By masking out (or ignoring) the tokens before the =, you are guiding the model to focus on learning to predict the result of the addition operation, starting from the = sign.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_11_'></a>[Potential to use Module Dict?](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taBKOEM1rdzm"
      },
      "outputs": [],
      "source": [
        "class ModelModuleDict(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleDict({\n",
        "            'fc1': nn.Linear(2, 5),\n",
        "            'relu': nn.ReLU(),\n",
        "            'fc2': nn.Linear(5, 1)\n",
        "        })\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "# Initialize a random tensor as input\n",
        "input_tensor = torch.randn(1, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpkfNuQMrdzm"
      },
      "outputs": [],
      "source": [
        "seed_all(1, seed_torch=True)\n",
        "model_sequential = nn.Sequential(\n",
        "    nn.Linear(2, 5),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(5, 1)\n",
        ")\n",
        "# Forward pass using nn.Sequential model\n",
        "model_sequential(input_tensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_HGZqyDrdzm"
      },
      "outputs": [],
      "source": [
        "seed_all(1, seed_torch=True)\n",
        "model_moduledict   = ModelModuleDict()\n",
        "model_moduledict(input_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_12_'></a>[Training with GPT-like Model](#toc0_)\n",
        "\n",
        "If you're working with a GPT-like model, which is a decoder-only architecture, the training mechanics differ slightly compared to the encoder-decoder models like seq2seq. In a GPT-style model, the entire sequence (input and output) is provided to the model at once, and each token is predicted based on the tokens that came before it. The model is still autoregressive, but there's no separate encoder to produce an intermediate representation; the \"encoding\" is effectively built into the ongoing autoregressive decoding process.\n",
        "\n",
        "In your case, if the equations are like `90+38=128`, during training you'd provide `90+38=` as the input and then use the remaining part `128` as the expected output, potentially along with special tokens to demarcate sequence boundaries or to flag the equation/result parts. However, unlike an encoder-decoder model where the decoder gets to \"peek\" at the correct output during training (also known as \"teacher forcing\"), here every token in the output is predicted one by one, based solely on the preceding tokens.\n",
        "\n",
        "In such a setup, you can definitely feed the entire equation to the model and try to predict each subsequent token based on the preceding tokens. For example, given `90+38=`, the model should predict `1`, `2`, `8` in succession.\n",
        "\n",
        "### <a id='toc1_12_1_'></a>[Loss Computation](#toc0_)\n",
        "\n",
        "For training a GPT-like model, you'd usually use a standard loss function like cross-entropy loss for each token's prediction. You'd compare the token predicted by the model to the actual token in the target sequence to compute the loss. This is calculated for each token and then averaged over the sequence or batch, depending on your implementation.\n",
        "\n",
        "### <a id='toc1_12_2_'></a>[Example](#toc0_)\n",
        "\n",
        "In a GPT-like model, each token in the sequence is used to predict the next token. The model takes a sequence of tokens and produces a new sequence of the same length where each new token is predicted based on all the preceding tokens in the input sequence. The loss is then computed between the predicted sequence and the target sequence.\n",
        "\n",
        "Let's take a closer look at an example:\n",
        "\n",
        "- The original tensor: `[15, 9, 0, 10, 3, 8, 13, 1, 2, 8, 14]` which corresponds to `<SOS>90+38=128<EOS>`\n",
        "- Input tensor:  `[15, 9,  0,  10, 3,  8,  13, 1,  2, 8]`, which corresponds to `<SOS>90+38=128` without `EOS`\n",
        "- Target tensor:     `[9,  0,  10, 3,  8,  13, 1,  2,  8, 14]`\n",
        "                     `[16, 16, 16, 16, 16, 16, 1,  2,  8, 14]`\n",
        "\n",
        "During training:\n",
        "\n",
        "1. **First Timestep**: The model takes `[15]` (or `[<BOS>]` if 15 is your BOS token) and tries to predict the next token. Ideally, it should predict `9`. But here, your target sequence starts with masked tokens (`16`, if 16 is your masking token). So the loss is computed between the predicted token and the masked token `16`. But since `CrossEntropyLoss` has an `ignore_index` (now you know what they are right!), you can set it to say `16` or (default `-1` but you would need to change padding number) and tell the model that whenever the ground truth is `16`, the loss\n",
        "is zeroed out so it is not counted? This allows the model to focus on learning from the relevant parts of the sequence while ignoring the masked portions.\n",
        "\n",
        "2. **Second Timestep**: The model takes `[15, 9]` and predicts the next token, which should be `0`. Again, the target is a masked token `16`.\n",
        "\n",
        "3. **...**\n",
        "\n",
        "4. **Eighth Timestep**: The model takes `[15, 9,  0,  10, 3,  8,  13]` (which is `90+38=`) and predicts the next token. Now the target is `1`, so the loss is computed between the predicted token and `1`. There is no mask anymore here, so the loss will be computed.\n",
        "5. **Ninth Timestep**: The model takes `[15, 9,  0,  10, 3,  8,  13, 1]` (which is `90+38=1`) and predicts the next token. Now the target is `2`, so the loss is computed between the predicted token and `2`.\n",
        "   1. Here's an important thing for beginners (me), In a typical GPT-like architecture used for sequence-to-sequence tasks like this one, the model doesn't use its own predictions as input during training. Instead, it uses the original, ground-truth input sequence. This is known as \"teacher forcing.\" In teacher forcing, even if the model predicts a wrong token at some timestep, it doesn't affect the input sequence for subsequent timesteps. The model continues to get the original input sequence for the entire training epoch.\n",
        "   2. So if model predicts a `3` during the eighth timestep, where the ground trut is `1`, the model would simply incur a higher loss for that prediction. However, the input for the ninth timestep would still be the ground truth sequence up to that point, regardless of what the model predicted at the eighth timestep.\n",
        "   3. But it is noted that this behaviour is still autoregressive.\n",
        "6. **Tenth**: The model takes `[15, 9,  0,  10, 3,  8,  13, 1, 2]` and predicts the next token which is `8`.\n",
        "7. **Last**: The model takes `[15, 9,  0,  10, 3,  8,  13, 1, 2, 8]` and predicts the next token which is `14` the `EOS`.\n",
        "   1. The reason you need to predict `EOS` is simple intuitively, consider the case where there's no need for `EOS`, then the model will not know when to stop.\n",
        "\n",
        "This goes on until the entire sequence is processed. Note that the model never actually \"sees\" the target tokens during the prediction. It is solely relying on the tokens that came before the current token in the input sequence. After the model makes its prediction, then the predicted tokens are compared to the target tokens to compute the loss, which is then backpropagated to update the model weights.\n",
        "\n",
        "### <a id='toc1_12_3_'></a>[Confusion: Training versus Inference](#toc0_)\n",
        "\n",
        "The statement \"it generates one token at a time and uses its own previously generated tokens as context for generating subsequent tokens\" is generally true for GPT-like models during the inference stage, not during training. During inference (or generation), the model does indeed use its own previously generated tokens to produce the next token, since there is no ground truth sequence to rely on. In that case, if the model makes an incorrect prediction at a certain timestep, that incorrect token is used as part of the context for the following timestep.\n",
        "\n",
        "During training, however, the model typically uses the ground truth tokens for the preceding sequence as context for predicting each next token, as described in your example. This resembles teacher forcing, in that the ground truth, rather than the model's own predictions, is used to guide training.\n",
        "\n",
        "So there's no contradiction, but the behavior is context-dependent:\n",
        "\n",
        "- During training, the ground truth sequence is used for context.\n",
        "- During inference, the model's own previously generated tokens are used for context.\n",
        "\n",
        "Both approaches are consistent with the autoregressive nature of the model: in both cases, the token at each position is generated based on the tokens at all previous positions. The difference lies in whether those preceding tokens come from the ground truth (during training) or from the model's own previous outputs (during inference).\n",
        "\n",
        "### <a id='toc1_12_4_'></a>[Training vs Inference](#toc0_)\n",
        "\n",
        "In an autoregressive model like a Transformer decoder, the concept of \"learning\n",
        "the representation of the sequence as it goes\" does not refer to the model\n",
        "processing one token at a time during actual forward passes. Instead, it refers\n",
        "to the model's ability to generate or predict one token at a time during\n",
        "inference, while training on a full sequence in a batched manner.\n",
        "\n",
        "During training:\n",
        "\n",
        "- All tokens are processed in parallel for efficiency. This is possible because\n",
        "  the entire sequence is known beforehand (it's the training data).\n",
        "- The \"autoregressive\" property is enforced by using masks in the self-attention\n",
        "  mechanism. This masking ensures that the prediction for each token can only\n",
        "  depend on previously generated tokens, not on future tokens which the model\n",
        "  has no access to during inference. This is how the model learns the\n",
        "  conditional probability distribution of each token given the previous tokens,\n",
        "  despite the parallel processing of tokens.\n",
        "\n",
        "During inference:\n",
        "\n",
        "- The model starts with an initial token (such as a start-of-sequence token) and\n",
        "  generates the next token based on this single input.\n",
        "- Then, the model uses both the initial token and the newly generated token to\n",
        "  predict the third token, and so on.\n",
        "- This process is sequential and each new token is predicted based on the\n",
        "  previously generated tokens, creating a sequence one token at a time.\n",
        "\n",
        "So, when we say that the model learns the representation of the sequence as it\n",
        "goes, we mean that the model is trained to handle sequences in such a way that\n",
        "it can generate them one piece at a time, respecting the causal order inherent\n",
        "to the task (e.g., language modeling). The parallel processing during training\n",
        "does not contradict the autoregressive nature of the model; it is simply a\n",
        "computational efficiency that is enabled by knowing the full sequence in\n",
        "advance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_13_'></a>[Questions](#toc0_)\n",
        "\n",
        "### <a id='toc1_13_1_'></a>[Why Masked == 0 in some?](#toc0_)\n",
        "\n",
        "The use of `mask == 0` in the `masked_fill` operation is a result of how the mask is constructed. Essentially, different implementations may represent masks differently:\n",
        "\n",
        "1. **Boolean Masking with True/False**: In some implementations, the mask might be a Boolean tensor where `True` denotes the positions to mask (set to negative infinity) and `False` for the positions to keep. In such cases, you can directly use the mask in `masked_fill` as in your provided code:\n",
        "\n",
        "    ```python\n",
        "    attention_scores = attention_scores.masked_fill(mask, float(\"-inf\"))\n",
        "    ```\n",
        "\n",
        "    Here, if `mask[i][j]` is `True`, `attention_scores[i][j]` would be set to `-inf`.\n",
        "\n",
        "2. **Integer Masking with 1/0**: In other implementations, the mask might be an integer tensor where `1` denotes the positions to keep and `0` denotes the positions to mask. In such cases, you'll often find the mask is inverted (`mask == 0`) before using `masked_fill`:\n",
        "\n",
        "    ```python\n",
        "    attention_scores = attention_scores.masked_fill(mask == 0, float(\"-inf\"))\n",
        "    ```\n",
        "\n",
        "    Here, if `mask[i][j]` is `0`, `attention_scores[i][j]` would be set to `-inf`.\n",
        "\n",
        "The core functionality—masking certain positions in the attention scores—is the same in both cases. The difference lies in how the mask tensor is constructed and interpreted. So, if you find an implementation using `mask == 0`, it's likely using an integer mask where `0` signifies positions to mask, whereas if it's directly using `mask`, it's probably a Boolean mask where `True` signifies positions to mask.\n",
        "\n",
        "### <a id='toc1_13_2_'></a>[what is the reason of setting the attention scores's mask indexes to negative infinity](#toc0_)\n",
        "\n",
        "\n",
        "In the attention mechanism, particularly in the Scaled Dot-Product Attention, attention scores are computed for each query-key pair and then passed through a softmax function to obtain attention weights. These weights are used to take a weighted sum of the value vectors, resulting in the final output or the context vectors. The purpose of the mask is to prevent certain tokens (like padding tokens) from being attended to.\n",
        "\n",
        "The reason for setting masked attention scores to negative infinity (`-inf`) lies in the properties of the softmax function:\n",
        "\n",
        "1. **Softmax Behavior**: The softmax function transforms its input (the attention scores in this case) into a probability distribution. Mathematically, the softmax function for a given vector $x$ is defined as:\n",
        "\n",
        "$$\n",
        "\\text{Softmax}(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{N} e^{x_j}}\n",
        "$$\n",
        "\n",
        "2. **Impact of Negative Infinity**: When you pass negative infinity through the softmax function, $e^{-\\infty}$ approaches zero. As a result, the masked positions get a near-zero weight in the attention mechanism.\n",
        "\n",
        "$$\n",
        "\\text{Softmax}(-\\infty) = \\frac{e^{-\\infty}}{\\sum_{j=1}^{N} e^{x_j}} \\approx 0\n",
        "$$\n",
        "\n",
        "3. **Avoiding Unwanted Attention**: The point of setting these specific positions to `-inf` is to ensure that when softmax is applied, these positions get zero attention weights. This is a way of making sure that the model does not attend to the positions we've masked (like padding tokens or future tokens in the sequence, depending on the mask).\n",
        "\n",
        "In summary, setting the masked attention scores to `-inf` and then passing them through a softmax effectively nullifies the contribution of the masked positions in the resulting attention-weighted sum of the value vectors. This is a commonly used trick to impose a certain structure (like masking out future information in the decoder) or to handle variable-length sequences with padding.\n",
        "\n",
        "### <a id='toc1_13_3_'></a>[Why do we need both ignore index in Loss and also negative infinity mask](#toc0_)\n",
        "\n",
        "Using an \"ignore index\" in the `CrossEntropyLoss` function in PyTorch can ignore the effect of certain tokens (like padding tokens) during the loss computation. However, the purpose of the mask in the attention mechanism and the \"ignore index\" in the loss function serve different roles in the model, and they operate at different stages of the computational graph.\n",
        "\n",
        "1. **Ignore Index in Loss Function**: The \"ignore index\" in the loss function ensures that the model's output at certain positions (typically corresponding to padding tokens) does not contribute to the loss. This happens at the very end of the forward pass, just before backpropagation begins.\n",
        "\n",
        "2. **Mask in Attention Mechanism**: The mask in the attention mechanism, on the other hand, operates during the forward pass at the time when attention scores are computed. This is a more \"internal\" operation and ensures that certain positions do not contribute to the output at all, not just during the loss computation but actually in the intermediate representations (i.e., context vectors) that the model computes.\n",
        "\n",
        "To put it another way, even if you're ignoring certain tokens in your loss calculation, those tokens can still influence the model's output unless they're masked out in the attention mechanism itself.\n",
        "\n",
        "For example, consider a decoder in a sequence-to-sequence model:\n",
        "- If you don't use a mask in the attention mechanism, future tokens could influence the output at the current timestep, which is not desirable.\n",
        "- Even if you use an \"ignore index\" in your loss function, it doesn't prevent the model from \"cheating\" by peeking at the future tokens if they are not masked in the attention mechanism.\n",
        "\n",
        "So in summary, using an \"ignore index\" in `CrossEntropyLoss` is not a replacement for using attention masks. Both have specific roles in the model, and they are often used together to ensure both that the model attends to the right tokens and that it is trained properly.\n",
        "\n",
        "### <a id='toc1_13_4_'></a>[Target and Preds/Logits Shape](#toc0_)\n",
        "\n",
        "The target tensor for the cross-entropy loss function should typically have a shape of `[batch_size, sequence_length]` where each entry in the tensor is an integer representing the index of the true class (i.e., the actual word/token from the vocabulary) for that position in the sequence. Here `batch_size` refers to the number of sequences in each batch, and `sequence_length` is the length of each sequence.\n",
        "\n",
        "Let's break it down step-by-step:\n",
        "\n",
        "1. **Last Linear Layer of Decoder**: When you say that the last linear layer of your decoder has shape `[bs, vocab_size]`, it means that for each example in the batch, you're outputting a distribution over the vocabulary. The values can be logit scores that represent the likelihood of each word in your vocabulary being the next word in the sequence.\n",
        "\n",
        "2. **Target Shape**: In comparison, your target tensor should contain the actual words (as integers) that appear at each position in your sequence for each example in the batch. The target tensor does not need to have a `vocab_size` dimension because it is not a distribution; it contains the indices of the actual next words. Thus, it should have a shape `[bs, sequence_length]`.\n",
        "\n",
        "3. **Cross-Entropy Loss**: When using the cross-entropy loss, the logits (i.e., the output from your linear layer) should have a shape `[bs, sequence_length, vocab_size]`, while the target should have a shape `[bs, sequence_length]`. The cross-entropy loss function will internally apply a softmax to the logits, and then compute the log-likelihood between the predicted distribution and the target class.\n",
        "\n",
        "To sum up, if your decoder's last linear layer has shape `[bs, vocab_size]` for each time step, make sure that your target tensor has the shape `[bs, sequence_length]`, and your logits should be `[bs, sequence_length, vocab_size]` when you feed them into the cross-entropy loss function.\n",
        "\n",
        "### <a id='toc1_13_5_'></a>[Why do we flatten prediction and target (logits)?](#toc0_)\n",
        "\n",
        "Flattening both the predicted logits and the target labels serves a specific purpose when using the cross-entropy loss function for sequence data. Let's dig into each component to understand why this is done:\n",
        "\n",
        "#### <a id='toc1_13_5_1_'></a>[Background](#toc0_)\n",
        "\n",
        "1. **Logits Tensor**: In a sequence-to-sequence model, you usually generate a sequence of logits for each item in your batch. The logits for each position in the sequence form a vector of size `vocab_size`, which gives you a probability distribution across all possible tokens.\n",
        "  \n",
        "   Shape: `[batch_size, sequence_length, vocab_size]`\n",
        "\n",
        "2. **Targets Tensor**: Your ground truth data, the `targets`, are integers representing the correct class labels (or tokens) at each sequence position.\n",
        "\n",
        "   Shape: `[batch_size, sequence_length]`\n",
        "\n",
        "#### <a id='toc1_13_5_2_'></a>[Traditional Loss Computation](#toc0_)\n",
        "\n",
        "Typically, the cross-entropy loss between predicted probabilities and target labels for one data point is computed, and then you average over all data points. In sequence-to-sequence models, you can think of each position in the sequence as a separate data point.\n",
        "\n",
        "#### <a id='toc1_13_5_3_'></a>[Why Flatten?](#toc0_)\n",
        "1. **Batch and Sequence Unification**: The idea of flattening both logits and targets is to treat each `(batch, sequence_position)` pair as an independent data point. Instead of having a batch of sequences, you have a \"flattened\" batch of tokens. This simplifies the application of the loss function by converting the 3D logits tensor and 2D targets tensor into 2D and 1D tensors, respectively.\n",
        "\n",
        "2. **Efficiency**: Loss computations often benefit from vectorization for computational efficiency. By flattening the tensors, you enable a more efficient matrix operation, which is generally faster than using nested loops over each sequence and batch.\n",
        "\n",
        "3. **Alignment**: The key is to ensure that each row in the flattened logits corresponds to the same position in the flattened targets. This alignment is crucial for the correct computation of the loss.\n",
        "\n",
        "#### <a id='toc1_13_5_4_'></a>[Step-by-step Flattening](#toc0_)\n",
        "\n",
        "1. **Logits Flattening**: `logits.view(-1, logits.size(-1))` will take the 3D tensor `[batch_size, seq_length, vocab_size]` and reshape it into a 2D tensor of shape `[batch_size * seq_length, vocab_size]`.\n",
        "\n",
        "2. **Targets Flattening**: `targets.view(-1)` will take the 2D tensor `[batch_size, seq_length]` and convert it into a 1D tensor of shape `[batch_size * seq_length]`.\n",
        "\n",
        "3. **Loss Calculation**: Both flattened tensors are then used in the cross-entropy loss function. The loss between each row in the flattened logits and the corresponding element in the flattened targets is computed.\n",
        "\n",
        "By flattening the tensors this way, you maintain the correspondence between each logit and its corresponding target, enabling you to correctly compute the loss for each token across all sequences and batches.\n",
        "\n",
        "### <a id='toc1_13_6_'></a>[Why sometimes unsqueeze masks?](#toc0_)\n",
        "\n",
        "The `unsqueeze` operation is used to add an additional dimension to the tensor. In attention mechanisms, particularly the scaled dot-product attention used in models like the Transformer, the masks usually need to have the same number of dimensions as the attention logits for proper broadcasting.\n",
        "\n",
        "For instance, let's say your source tensor (`src`) has a shape of $B \\times L$ where $B$ is the batch size and $L$ is the sequence length. The attention logit tensor resulting from the query-key dot product would then have shape $B \\times N \\times L \\times L$, where $N$ is the number of attention heads.\n",
        "\n",
        "The mask needs to align with the $L \\times L$ dimensions of this 4D tensor. In order to accomplish that, you add singleton dimensions to make it compatible with the attention logit tensor. By unsqueezing the mask tensor from $B \\times L$ to $B \\times 1 \\times 1 \\times L$, you enable broadcasting such that the mask effectively gets expanded to $B \\times N \\times L \\times L$ during the attention calculation, perfectly aligning with the attention logits.\n",
        "\n",
        "That's why the line:\n",
        "```python\n",
        "self.src_mask = (src != pad).unsqueeze(-2)\n",
        "```\n",
        "adds a singleton dimension, converting the shape from $B \\times L$ to $B \\times 1 \\times 1 \\times L$ for proper broadcasting during the attention computations.\n",
        "\n",
        "### <a id='toc1_13_7_'></a>[Why does sequence length differ for source and target, usually I thought it is just all L, same.](#toc0_)\n",
        "\n",
        "In many sequence-to-sequence tasks, the source and target sequences can have different lengths. Here are a few scenarios where this happens:\n",
        "\n",
        "1. **Machine Translation**: A sentence in one language may require more or fewer words when translated into another language.\n",
        "  \n",
        "2. **Text Summarization**: The source text is usually much longer than the summarized target text.\n",
        "\n",
        "3. **Question Answering**: The source document could be quite lengthy, while the target answer might be a short sentence or even a single word.\n",
        "\n",
        "4. **Code Generation**: Given a natural language query, the corresponding code snippet could be of varying length that doesn't directly correlate with the length of the query.\n",
        "\n",
        "5. **Dialogue Systems**: The system's response may not be of the same length as the user's query.\n",
        "\n",
        "So, in general, source and target sequence lengths (`S` and `T` in the function signature) could be different, and the attention mechanism accommodates that by allowing for `key` and `query` tensors with different sequence lengths. Although\n",
        "lilian weng use `L` for seq length.\n",
        "\n",
        "### <a id='toc1_13_8_'></a>[Am i right to assume that the core idea of autoregressive model like decoder only (GPT like) is that for a given sample, there will eventually be L rows where L is the seq length, and therefore I can intuitively view it as 1 sample having L samples, since for each row, we will compute the loss. Am I right in my understanding? Do not hesistate to correct me.](#toc0_)\n",
        "\n",
        "\n",
        "Your description captures an important aspect of autoregressive models like decoder-only Transformers (e.g., GPT). Specifically, you're talking about how the model treats a sequence during training. Let's break down your understanding step by step.\n",
        "\n",
        "1. **Sequence Length**: When you mention \"L rows,\" where \"L\" is the sequence length, you're essentially pointing out that each sequence is divided into \"L\" time steps (or tokens). Each time step becomes an input-output pair for training the model.\n",
        "\n",
        "2. **One Sequence as Multiple Samples**: You're correct to intuit that a single sequence of length \"L\" can be treated like \"L\" samples, at least in the context of loss calculation. This is because, during training, the model computes the loss at each time step by comparing the predicted token with the actual next token in the sequence.\n",
        "\n",
        "3. **Loss Computation**: The loss is often computed at each position and then averaged over the sequence length or summed up, depending on the specific loss function or training regime.\n",
        "\n",
        "However, it's crucial to clarify that although a single sequence may contribute \"L\" terms to the loss function, this is not equivalent to having \"L\" independent samples. The key difference lies in the autoregressive property: the prediction at each time step is conditioned on the preceding tokens. This introduces a temporal dependency across the \"L\" positions, making them not entirely independent samples.\n",
        "\n",
        "In other words, while it's accurate to say that a single sequence contributes multiple terms to the loss function, these terms are correlated because they come from the same sequence and are generated in an autoregressive manner.\n",
        "\n",
        "To summarize, you're mostly correct in your understanding that a single sequence is broken down into multiple steps for the purpose of loss computation, but it's important to remember that these steps are not independent samples due to the autoregressive nature of the model.\n",
        "\n",
        "### <a id='toc1_13_9_'></a>[QKV Again](#toc0_)\n",
        "\n",
        "#### <a id='toc1_13_9_1_'></a>[Background and Assumptions](#toc0_)\n",
        "\n",
        "The Transformer architecture, introduced by Vaswani et al. in 2017, has become a cornerstone in NLP and many other machine learning tasks. It is built around the concept of self-attention, which allows the model to weigh different parts of the input when making predictions or transformations. The context vector, as well as Q, K, and V vectors, play a crucial role in this architecture.\n",
        "\n",
        "#### <a id='toc1_13_9_2_'></a>[Context Vector](#toc0_)\n",
        "\n",
        "The term \"context vector\" is commonly used to refer to the weighted sum of value vectors (`V`), after the attention scores have been computed. The purpose of this vector is to encode information from different parts of the input sequence in a way that is most useful for the task at hand. In the attention mechanism, each word (or token) is represented as a context vector that aggregates information from all the other words in the sentence, weighted by their relevance or \"attention score\".\n",
        "\n",
        "#### <a id='toc1_13_9_3_'></a>[Query (Q), Key (K), and Value (V)](#toc0_)\n",
        "\n",
        "1. **Query (Q):** This is a representation of the element for which we are calculating the context. The query is used to find relevant keys, which in turn helps in identifying relevant values. Mathematically, we take the dot product of the Query with each Key to get an attention score.\n",
        "\n",
        "   $$\n",
        "   \\text{Attention Score} = Q \\cdot K^T\n",
        "   $$\n",
        "\n",
        "2. **Key (K):** Keys serve as a set of indicators, helping the model identify which values should be attended to when forming the context vector for each query.\n",
        "\n",
        "3. **Value (V):** Values hold the actual content that will be used to form the context vector. Once the attention scores have been computed using Q and K, these scores are used to weigh the Value vectors before summing them up to get the final context vector.\n",
        "\n",
        "#### <a id='toc1_13_9_4_'></a>[Mathematical Description](#toc0_)\n",
        "\n",
        "To obtain the context vector, we first calculate the attention scores for each Query-Key pair:\n",
        "\n",
        "$$\n",
        "\\text{Attention Score} = \\frac{Q \\cdot K^T}{\\sqrt{d_k}}\n",
        "$$\n",
        "\n",
        "We then take the softmax of these scores:\n",
        "\n",
        "$$\n",
        "\\text{Softmax Score} = \\text{Softmax}(\\text{Attention Score})\n",
        "$$\n",
        "\n",
        "Finally, we use these softmax scores to compute a weighted sum of the Value vectors:\n",
        "\n",
        "$$\n",
        "\\text{Context Vector} = \\text{Softmax Score} \\cdot V\n",
        "$$\n",
        "\n",
        "In summary, Q, K, and V vectors are instrumental in the computation of the context vector. The Query helps to identify relevant Keys, which in turn are used to weigh the Values, culminating in a context vector that holds the contextual representation useful for a given task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Some Implementation Details\n",
        "\n",
        "```\n",
        "Performs one decoder forward pass given encoder hidden states, the decoder input tokens and attention masks.\n",
        "B = batch size\n",
        "S = source sequence length\n",
        "T = target sequence length\n",
        "E = embedding dimensionality\n",
        "V = vocabulary size\n",
        "```\n",
        "\n",
        "### Input\n",
        "\n",
        "Let's view input's first two samples:\n",
        "\n",
        "```\n",
        "tensor([[15,  4,  9, 10,  1,  3, 13,  0,  6,  2],\n",
        "│   │   [15,  3,  5, 10,  4,  6, 13,  0,  8,  1]])\n",
        "```\n",
        "\n",
        "which is\n",
        "\n",
        "-   shape is `[2, 10]` which is `BxL`.\n",
        "-   `49+13=62` but no `EOS` as we truncated last token.\n",
        "-   `35+46=81` but no `EOS` as we truncated last token.\n",
        "\n",
        "### Positional Encodings\n",
        "\n",
        "#### Why do we hardcode batch size of 1 when creating P?\n",
        "\n",
        "The tensor $P$ for positional encoding is initialized with a batch size of 1.\n",
        "This makes it easy to add to the actual input sequences later, during the\n",
        "forward pass. Positional encodings are not dependent on the specific input\n",
        "sequence but are a function of the position within the sequence. Therefore, they\n",
        "can be precomputed and stored. When you look at the forward pass:\n",
        "\n",
        "```python\n",
        "def forward(self, Z: torch.Tensor) -> torch.Tensor:\n",
        "    Z = self._add_positional_encoding(Z)\n",
        "    return self.dropout(Z)\n",
        "```\n",
        "\n",
        "and the `_add_positional_encoding` method:\n",
        "\n",
        "```python\n",
        "def _add_positional_encoding(self, Z: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Add the positional encoding tensor to the input tensor.\"\"\"\n",
        "    return Z + self.P[:, : Z.shape[1], :].to(Z.device)\n",
        "```\n",
        "\n",
        "You'll see that $P$ is sliced to match the sequence length of $Z$ and then added\n",
        "to $Z$. Because of broadcasting rules in PyTorch, $P$ will automatically be\n",
        "broadcasted to the batch size of $Z$ during this addition. This is why $P$ is\n",
        "initialized with a batch size of 1; it keeps the implementation flexible while\n",
        "making the broadcasting implicit.\n",
        "\n",
        "#### Why do we register P as a buffer in PyTorch?\n",
        "\n",
        "In your `PositionalEncoding` class, the tensor `self.P` holds the pre-computed\n",
        "positional encodings. If you intend for this tensor to be automatically moved to\n",
        "the correct device when the module is moved, and if it should not be a learnable\n",
        "parameter, then registering it as a buffer would be a good idea. This ensures\n",
        "that `self.P` is part of the module's state but is not updated during\n",
        "backpropagation.\n",
        "\n",
        "You could register `self.P` as a buffer right after you initialize it in the\n",
        "`_init_positional_encoding` method:\n",
        "\n",
        "```python\n",
        "def _init_positional_encoding(self) -> torch.Tensor:\n",
        "    \"\"\"Initialize the positional encoding tensor.\"\"\"\n",
        "    P = torch.zeros((1, self.max_seq_len, self.d_model))\n",
        "    position = self._get_position_vector()\n",
        "    div_term = self._get_div_term_vector()\n",
        "    P[:, :, 0::2] = torch.sin(position / div_term)\n",
        "    P[:, :, 1::2] = torch.cos(position / div_term)\n",
        "    self.register_buffer(\"P\", P, persistent=True)\n",
        "    return P\n",
        "```\n",
        "\n",
        "Using `register_buffer` ensures that:\n",
        "\n",
        "1. `self.P` is automatically moved to the device the model is moved to (e.g.,\n",
        "   from CPU to GPU).\n",
        "2. `self.P` is saved when you save the model using `torch.save` or `torch.load`.\n",
        "\n",
        "The `persistent=False` argument indicates that the buffer should not be part of\n",
        "the model's `state_dict`, meaning it won't be saved or loaded with the model. If\n",
        "you do want it to be part of the `state_dict`, you can simply omit this\n",
        "argument.\n",
        "\n",
        "### Attention\n",
        "\n",
        "#### Why do we call contiguous on Q, K and V?\n",
        "\n",
        "D2L's code uses `reshape` to reshape the `Q`, `K` and `V`, where other code such\n",
        "as from the Annotated Transformer uses `view`. When you use `view`, this assumes\n",
        "the tensor is `contiguous`, so it is better to call `contiguous` first.\n",
        "\n",
        "#### Why do we want to transpose Q, K, and V?\n",
        "\n",
        "The transposition of $Q$, $K$, and $V$ in multi-head attention serves a specific\n",
        "purpose: to allow for parallel computation across multiple attention heads. In\n",
        "the original shape, the \"heads\" dimension does not exist; the tensor is simply\n",
        "$B \\times L \\times D$, where $B$ is the batch size, $L$ is the sequence length,\n",
        "and $D$ is the model dimension. By transposing, we create a new shape\n",
        "$B \\times H \\times L \\times (D/H)$, where $H$ is the number of heads. This\n",
        "enables the following:\n",
        "\n",
        "1. **Parallelization**: Each head can now be computed in parallel since each\n",
        "   head operates independently of the others.\n",
        "2. **Optimization**: Modern hardware accelerators like GPUs are optimized for\n",
        "   certain tensor operations, and having a shape that aligns well with these\n",
        "   optimizations can result in faster computation.\n",
        "3. **Readability and Maintainability**: It's easier to understand and debug the\n",
        "   operations for each head when they're isolated like this.\n",
        "\n",
        "#### Why do we want to reverse transpose Q, K, and V?\n",
        "\n",
        "After the attention scores are computed and used to weight $V$, we get a new\n",
        "tensor for each head. However, these tensors are still in the transposed shape\n",
        "$B \\times H \\times L \\times (D/H)$, and they need to be concatenated and\n",
        "linearly transformed to continue through the network. The reverse transposition\n",
        "essentially does the following:\n",
        "\n",
        "1. **Concatenation**: Converts the multiple heads back into a single tensor.\n",
        "   This is required because subsequent layers (like feed-forward neural\n",
        "   networks) expect input in the original $D$-dimensional space.\n",
        "\n",
        "2. **Compatibility**: The rest of the neural network architecture often expects\n",
        "   input tensors to have a specific shape (usually $B \\times L \\times D$).\n",
        "   Reverse transposing ensures that the output of the multi-head attention block\n",
        "   can be fed into subsequent layers without issue.\n",
        "\n",
        "3. **Resource Efficiency**: By reducing the tensor back to its original\n",
        "   dimensions, we can save memory and computational resources, which is\n",
        "   beneficial when you're training large models or operating under hardware\n",
        "   constraints.\n",
        "\n",
        "In summary, the initial transposition is done to facilitate parallel computation\n",
        "across heads, and the reverse transposition is done to concatenate these heads\n",
        "and prepare the tensor for subsequent layers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why we need Positional Vector\n",
        "\n",
        "Positional encoding is critical cause the cat ate the mouse is the same as the\n",
        "mouse ate the cat without it\n",
        "\n",
        "Without it the attention Q and K matmul would result in a permutation invariant\n",
        "matrix. So adding position info makes the last token in the attention matrix\n",
        "(say mouse from the cat ate the mouse) would allow the word mouse to hold info\n",
        "for every other word in the sentence as well as knowing every other token\n",
        "position (including knowing it's the last token)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_14_'></a>[TODO](#toc0_)\n",
        "\n",
        "1. Add Positional Encoding\n",
        "2. Add LR Scheduler\n",
        "3. Check why need to use `torch.nn.utils.clip_grad_norm_` to clip gradients\n",
        "4. Why unsqueeze mask?\n",
        "5. Can you init weights inside Encoder instead of outside?\n",
        "6. Add Epoch and Batch State see my old code.\n",
        "7. Important use `Vocab` class like in https://github.com/jsbaan/transformer-from-scratch/blob/main/vocabulary.py.\n",
        "\n",
        "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_15_'></a>[References and Further Readings](#toc0_)\n",
        "\n",
        "- https://slds-lmu.github.io/seminar_nlp_ss20/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
