{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Shakespeare](#toc1_)    \n",
    "  - [Vocabulary and Tokenization](#toc1_1_)    \n",
    "  - [Dataset and Dataloader](#toc1_2_)    \n",
    "    - [Why is the length of the dataset defined as `len(self.corpus) - self.context_length`?](#toc1_2_1_)    \n",
    "    - [Understanding the Dataset Length in the Context of Language Models:](#toc1_2_2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Shakespeare](#toc0_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from rich.pretty import pprint\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "import requests\n",
    "from typing import List, Tuple, Dict, Any, Union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_root_dir(current_path: Path = Path.cwd(), marker: str = '.git') -> Path | None:\n",
    "    \"\"\"\n",
    "    Find the root directory by searching for a directory or file that serves as a\n",
    "    marker.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    current_path : Path\n",
    "        The starting path to search from.\n",
    "    marker : str\n",
    "        The name of the file or directory that signifies the root.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Path or None\n",
    "        The path to the root directory. Returns None if the marker is not found.\n",
    "    \"\"\"\n",
    "    current_path = current_path.resolve()\n",
    "    for parent in current_path.parents:\n",
    "        if (parent / marker).exists():\n",
    "            return parent\n",
    "    return None\n",
    "\n",
    "current_file_path = Path(os.getcwd())\n",
    "root_dir          = find_root_dir(current_file_path, marker='omnivault')\n",
    "\n",
    "if root_dir is not None:\n",
    "    sys.path.append(str(root_dir))\n",
    "    from omnivault.transformer.utils.reproducibility import seed_all\n",
    "    from omnivault.transformer.core.vocabulary import TextCharacterVocabulary\n",
    "    from omnivault.transformer.core.dataset import TextCharacterDataset, collate_fn\n",
    "    from omnivault.transformer.core.tokenizer import TextCharacterTokenizer\n",
    "    from omnivault.transformer.config.composer import Composer, DataConfig\n",
    "    from omnivault.transformer.config.optim import OptimizerConfig, AdamWConfig\n",
    "    from omnivault.transformer.config.constants import MaybeConstant\n",
    "    from omnivault.transformer.config.global_ import MaybeGlobal\n",
    "    from omnivault.transformer.decoder.core import GPTDecoder, GPTDecoderBlock\n",
    "    from omnivault.transformer.config.decoder import *\n",
    "    from omnivault.transformer.modules.attention.core import ScaledDotProductAttention, MultiHeadedAttention\n",
    "    from omnivault.transformer.core.trainer import Trainer\n",
    "\n",
    "else:\n",
    "    raise ImportError(\"Root directory not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Vocabulary and Tokenization](#toc0_)\n",
    "\n",
    "See https://huggingface.co/learn/nlp-course/chapter2/4?fw=pt.\n",
    "\n",
    "Especially:\n",
    "\n",
    "> The second step is to convert those tokens into numbers, so we can build a tensor out of them and feed them to the model. To do this, the tokenizer has a vocabulary, which is the part we download when we instantiate it with the from_pretrained() method. Again, we need to use the same vocabulary used when the model was pretrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# sequence = \"Using a Transformer network is simple\"\n",
    "# tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('../../../data/tinyshakespeare/input.txt', 'r').read() # don't worry we won't run out of file handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(data)))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "vocabulary = TextCharacterVocabulary.from_file('../../../data/tinyshakespeare/input.txt')\n",
    "vocabulary_2 = TextCharacterVocabulary.from_url(url)\n",
    "assert vocabulary.index_to_token == vocabulary_2.index_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 46)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.vocab_size, vocabulary.token_to_index['h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TextCharacterTokenizer(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['h', 'e', 'l', 'l', 'o'], [46, 43, 50, 50, 53], 'hello')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('hello'), tokenizer.encode('hello'), tokenizer.decode(tokenizer.encode('hello'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Dataset and Dataloader](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "          53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
       "           1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
       "          57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
       "           6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
       "          58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
       "           1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
       "          53,  1]),\n",
       "  tensor([47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44, 53,\n",
       "          56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,  1,\n",
       "          44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1, 57,\n",
       "          54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,  6,\n",
       "           1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
       "          47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,  1,\n",
       "          56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58, 53,\n",
       "           1, 42]),\n",
       "  tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True]),\n",
       "  tensor([[ True, False, False,  ..., False, False, False],\n",
       "          [ True,  True, False,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          ...,\n",
       "          [ True,  True,  True,  ...,  True, False, False],\n",
       "          [ True,  True,  True,  ...,  True,  True, False],\n",
       "          [ True,  True,  True,  ...,  True,  True,  True]])),\n",
       " 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to ',\n",
       " 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to d')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = open('../../../data/tinyshakespeare/input.txt', 'r').read()\n",
    "dataset = TextCharacterDataset(corpus=corpus, context_length=128, tokenizer=tokenizer)\n",
    "\n",
    "dataset[0], tokenizer.decode(dataset[0][0].tolist()), tokenizer.decode(dataset[0][1].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_1_'></a>[Why is the length of the dataset defined as `len(self.corpus) - self.context_length`?](#toc0_)\n",
    "\n",
    "Why the length of the dataset is defined as\n",
    "`len(self.corpus) - self.context_length`. This design is common in datasets used\n",
    "for training language models, particularly autoregressive models like GPT. Let\n",
    "me elaborate further:\n",
    "\n",
    "### <a id='toc1_2_2_'></a>[Understanding the Dataset Length in the Context of Language Models:](#toc0_)\n",
    "\n",
    "1. **Training Samples Formation**:\n",
    "\n",
    "    - In an autoregressive model, each training sample typically consists of a\n",
    "      sequence of tokens used as input and a subsequent token (or tokens) used\n",
    "      as the target for prediction.\n",
    "    - If `context_length` is the size of the input sequence, then for any\n",
    "      starting point in the corpus, you need enough tokens following it to form\n",
    "      a complete input sequence.\n",
    "\n",
    "2. **Avoiding Out-of-Bounds Access**:\n",
    "\n",
    "    - As you approach the end of the corpus, there are fewer tokens available to\n",
    "      form a complete input sequence of `context_length`.\n",
    "    - For example, if the corpus length is 1000 tokens and `context_length` is\n",
    "      128, trying to form a sequence starting at token 900 would result in an\n",
    "      out-of-bounds access, as you would need tokens up to index 1027 (which\n",
    "      doesn't exist in the corpus).\n",
    "\n",
    "3. **Dataset Length Calculation**:\n",
    "\n",
    "    - To prevent this out-of-bounds issue, the length of the dataset is\n",
    "      restricted to `len(self.corpus) - self.context_length`. This ensures that\n",
    "      for any index `i` in the dataset, you can safely access the sequence\n",
    "      `self.corpus[i:i + context_length]` without exceeding the bounds of the\n",
    "      corpus.\n",
    "    - This adjustment means the dataset will not generate sequences that extend\n",
    "      beyond the end of the corpus.\n",
    "\n",
    "4. **Practical Example**:\n",
    "    - If `self.corpus` has 1000 characters and `self.context_length` is 128, the\n",
    "      last index accessed by the dataset (for the start of a sequence) will be\n",
    "      `1000 - 128 = 872`. The corresponding sequence will run from index 872 to\n",
    "      999, which is precisely 128 characters.\n",
    "\n",
    "In summary, the length of the dataset is calculated as\n",
    "`len(self.corpus) - self.context_length` to ensure that every training sample\n",
    "has a complete input sequence of the desired context length, without attempting\n",
    "to access data beyond the end of the corpus. This approach is a standard\n",
    "practice in preparing datasets for training language models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Composer</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">constants</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MaybeConstant</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">NUM_DIGITS</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">TOKENS</span>=<span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'0'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'1'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'3'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'4'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'5'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'6'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'7'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'8'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'9'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'+'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'*'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'-'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'='</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;BOS&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;EOS&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;PAD&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;UNK&gt;'</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">global_</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MaybeGlobal</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">seed</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">42</span>, <span style=\"color: #808000; text-decoration-color: #808000\">debug</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">data</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DataConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">dataset_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">split</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">collate_fn</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'batch_first'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'pad_token_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"font-weight: bold\">}</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">train_loader</span>=<span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'shuffle'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'num_workers'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'pin_memory'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'drop_last'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">}</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">val_loader</span>=<span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'shuffle'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'num_workers'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'pin_memory'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'drop_last'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">}</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">test_loader</span>=<span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'shuffle'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'num_workers'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'pin_memory'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'drop_last'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">optimizer</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AdamWConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'torch.optim.AdamW'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">lr</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0005</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">betas</span>=<span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.999</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">eps</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-08</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">weight_decay</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.01</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">amsgrad</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mComposer\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mconstants\u001b[0m=\u001b[1;35mMaybeConstant\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33mNUM_DIGITS\u001b[0m=\u001b[1;36m2\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33mTOKENS\u001b[0m=\u001b[1m[\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'0'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'1'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'2'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'3'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'4'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'5'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'6'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'7'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'8'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'9'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'+'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'*'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'-'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'='\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32mBOS\u001b[0m\u001b[32m>'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'<EOS>'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'<PAD>'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'<UNK\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mglobal_\u001b[0m=\u001b[1;35mMaybeGlobal\u001b[0m\u001b[1m(\u001b[0m\u001b[33mseed\u001b[0m=\u001b[1;36m42\u001b[0m, \u001b[33mdebug\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mdata\u001b[0m=\u001b[1;35mDataConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33mdataset_size\u001b[0m=\u001b[1;36m2\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33msplit\u001b[0m=\u001b[1m[\u001b[0m\u001b[1;36m0.7\u001b[0m, \u001b[1;36m0.1\u001b[0m, \u001b[1;36m0.2\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33mcollate_fn\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'batch_first'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \u001b[32m'pad_token_id'\u001b[0m: \u001b[1;36m16\u001b[0m\u001b[1m}\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33mtrain_loader\u001b[0m=\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'batch_size'\u001b[0m: \u001b[1;36m32\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'shuffle'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'num_workers'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'pin_memory'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'drop_last'\u001b[0m: \u001b[3;91mFalse\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m}\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33mval_loader\u001b[0m=\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'batch_size'\u001b[0m: \u001b[1;36m32\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'shuffle'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'num_workers'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'pin_memory'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'drop_last'\u001b[0m: \u001b[3;91mFalse\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m}\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33mtest_loader\u001b[0m=\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'batch_size'\u001b[0m: \u001b[1;36m32\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'shuffle'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'num_workers'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'pin_memory'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[32m'drop_last'\u001b[0m: \u001b[3;91mFalse\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33moptimizer\u001b[0m=\u001b[1;35mAdamWConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33mname\u001b[0m=\u001b[32m'torch.optim.AdamW'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33mlr\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0005\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33mbetas\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m0.9\u001b[0m, \u001b[1;36m0.999\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-08\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33mweight_decay\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.01\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33mamsgrad\u001b[0m=\u001b[3;91mFalse\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "constants: MaybeConstant = MaybeConstant()\n",
    "global_: MaybeGlobal = MaybeGlobal(seed=42, dataset_size=2)\n",
    "data_config: DataConfig = DataConfig()\n",
    "optimizer_config = AdamWConfig(name=\"torch.optim.AdamW\", lr=5e-4)\n",
    "\n",
    "config = Composer(constants=constants, global_=global_, data=data_config, optimizer=optimizer_config)\n",
    "pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gaohn/omniverse/omniverse/omnivault/transformer/utils/reproducibility.py:69: UserWarning: Deterministic mode is activated. This will negatively impact performance.\n",
      "  configure_deterministic_mode()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEBUG  = True\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "seed_all(config.global_.seed, seed_torch=True, set_torch_deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[47, 57, 46,  ..., 59, 58, 46],\n",
       "         [46, 43,  1,  ...,  1, 30, 47],\n",
       "         [47, 57,  1,  ..., 63,  1, 61],\n",
       "         ...,\n",
       "         [30, 24, 17,  ...,  8,  0,  0],\n",
       "         [47, 57,  1,  ..., 15, 20, 13],\n",
       "         [53, 59, 50,  ..., 47, 52, 45]]),\n",
       " tensor([[57, 46, 51,  ..., 58, 46,  1],\n",
       "         [43,  1, 57,  ..., 30, 47, 41],\n",
       "         [57,  1, 40,  ...,  1, 61, 53],\n",
       "         ...,\n",
       "         [24, 17, 10,  ...,  0,  0, 20],\n",
       "         [57,  1, 44,  ..., 20, 13, 30],\n",
       "         [59, 50, 42,  ..., 52, 45,  1]]),\n",
       " tensor([[[[True, True, True,  ..., True, True, True],\n",
       "           [True, True, True,  ..., True, True, True],\n",
       "           [True, True, True,  ..., True, True, True],\n",
       "           ...,\n",
       "           [True, True, True,  ..., True, True, True],\n",
       "           [True, True, True,  ..., True, True, True],\n",
       "           [True, True, True,  ..., True, True, True]]],\n",
       " \n",
       " \n",
       "         [[[True, True, True,  ..., True, True, True],\n",
       "           [True, True, True,  ..., True, True, True],\n",
       "           [True, True, True,  ..., True, True, True],\n",
       "           ...,\n",
       "           [True, True, True,  ..., True, True, True],\n",
       "           [True, True, True,  ..., True, True, True],\n",
       "           [True, True, True,  ..., True, True, True]]],\n",
       " \n",
       " \n",
       "         [[[True, True, True,  ..., True, True, True],\n",
       "           [True, True, True,  ..., True, True, True],\n",
       "           [True, True, True,  ..., True, True, True],\n",
       "           ...,\n",
       "           [True, True, True,  ..., True, True, True],\n",
       "           [True, True, True,  ..., True, True, True],\n",
       "           [True, True, True,  ..., True, True, True]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[True, True, True,  ..., True, True, True],\n",
       "           [True, True, True,  ..., True, True, True],\n",
       "           [True, True, True,  ..., True, True, True],\n",
       "           ...,\n",
       "           [True, True, True,  ..., True, True, True],\n",
       "           [True, True, True,  ..., True, True, True],\n",
       "           [True, True, True,  ..., True, True, True]]],\n",
       " \n",
       " \n",
       "         [[[True, True, True,  ..., True, True, True],\n",
       "           [True, True, True,  ..., True, True, True],\n",
       "           [True, True, True,  ..., True, True, True],\n",
       "           ...,\n",
       "           [True, True, True,  ..., True, True, True],\n",
       "           [True, True, True,  ..., True, True, True],\n",
       "           [True, True, True,  ..., True, True, True]]],\n",
       " \n",
       " \n",
       "         [[[True, True, True,  ..., True, True, True],\n",
       "           [True, True, True,  ..., True, True, True],\n",
       "           [True, True, True,  ..., True, True, True],\n",
       "           ...,\n",
       "           [True, True, True,  ..., True, True, True],\n",
       "           [True, True, True,  ..., True, True, True],\n",
       "           [True, True, True,  ..., True, True, True]]]]),\n",
       " tensor([[[[ True, False, False,  ..., False, False, False],\n",
       "           [ True,  True, False,  ..., False, False, False],\n",
       "           [ True,  True,  True,  ..., False, False, False],\n",
       "           ...,\n",
       "           [ True,  True,  True,  ...,  True, False, False],\n",
       "           [ True,  True,  True,  ...,  True,  True, False],\n",
       "           [ True,  True,  True,  ...,  True,  True,  True]]],\n",
       " \n",
       " \n",
       "         [[[ True, False, False,  ..., False, False, False],\n",
       "           [ True,  True, False,  ..., False, False, False],\n",
       "           [ True,  True,  True,  ..., False, False, False],\n",
       "           ...,\n",
       "           [ True,  True,  True,  ...,  True, False, False],\n",
       "           [ True,  True,  True,  ...,  True,  True, False],\n",
       "           [ True,  True,  True,  ...,  True,  True,  True]]],\n",
       " \n",
       " \n",
       "         [[[ True, False, False,  ..., False, False, False],\n",
       "           [ True,  True, False,  ..., False, False, False],\n",
       "           [ True,  True,  True,  ..., False, False, False],\n",
       "           ...,\n",
       "           [ True,  True,  True,  ...,  True, False, False],\n",
       "           [ True,  True,  True,  ...,  True,  True, False],\n",
       "           [ True,  True,  True,  ...,  True,  True,  True]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ True, False, False,  ..., False, False, False],\n",
       "           [ True,  True, False,  ..., False, False, False],\n",
       "           [ True,  True,  True,  ..., False, False, False],\n",
       "           ...,\n",
       "           [ True,  True,  True,  ...,  True, False, False],\n",
       "           [ True,  True,  True,  ...,  True,  True, False],\n",
       "           [ True,  True,  True,  ...,  True,  True,  True]]],\n",
       " \n",
       " \n",
       "         [[[ True, False, False,  ..., False, False, False],\n",
       "           [ True,  True, False,  ..., False, False, False],\n",
       "           [ True,  True,  True,  ..., False, False, False],\n",
       "           ...,\n",
       "           [ True,  True,  True,  ...,  True, False, False],\n",
       "           [ True,  True,  True,  ...,  True,  True, False],\n",
       "           [ True,  True,  True,  ...,  True,  True,  True]]],\n",
       " \n",
       " \n",
       "         [[[ True, False, False,  ..., False, False, False],\n",
       "           [ True,  True, False,  ..., False, False, False],\n",
       "           [ True,  True,  True,  ..., False, False, False],\n",
       "           ...,\n",
       "           [ True,  True,  True,  ...,  True, False, False],\n",
       "           [ True,  True,  True,  ...,  True,  True, False],\n",
       "           [ True,  True,  True,  ...,  True,  True,  True]]]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAD=100\n",
    "collate_fn_config = {\"batch_first\": True, \"pad_token_id\": PAD}\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=128, shuffle=True, collate_fn=lambda batch: collate_fn(batch, **collate_fn_config))\n",
    "sample_batch = next(iter(train_loader))\n",
    "x, y, padding_mask, future_mask = sample_batch\n",
    "x, y, padding_mask, future_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"ishment, then let them\\nIf I say fine, cry 'Fine;' if death, cry 'Death.'\\nInsisting on the old prerogative\\nAnd power i' the truth\",\n",
       " \"shment, then let them\\nIf I say fine, cry 'Fine;' if death, cry 'Death.'\\nInsisting on the old prerogative\\nAnd power i' the truth \")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(x[0]), tokenizer.decode(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create individual component configurations\n",
    "masked_self_attention_mha_config = MultiHeadedAttentionConfig(\n",
    "     attention=ScaledDotProductAttention(),\n",
    "    d_model=128, H=8, dropout=0.1\n",
    ")\n",
    "\n",
    "feed_forward_config = PositionwiseFeedForwardConfig(\n",
    "    d_model=128, d_ff=256, activation=nn.GELU(approximate=\"tanh\"), dropout=0.1, bias=True\n",
    ")\n",
    "\n",
    "add_norm_config_1 = AddNormConfig(feature_dim=128, dropout=0.1)\n",
    "add_norm_config_2 = AddNormConfig(feature_dim=128, dropout=0.1)\n",
    "\n",
    "# Create DecoderBlockConfig\n",
    "decoder_block_config = DecoderBlockConfig(\n",
    "    masked_self_attention_mha=masked_self_attention_mha_config,\n",
    "    feed_forward=feed_forward_config,\n",
    "    add_norm_1=add_norm_config_1,\n",
    "    add_norm_2=add_norm_config_2,\n",
    ")\n",
    "\n",
    "# Create the overall DecoderConfig\n",
    "model_config = DecoderConfig(\n",
    "    d_model=128,\n",
    "    vocab_size=vocab_size,\n",
    "    context_length=128,\n",
    "    num_decoder_blocks=5,\n",
    "    dropout=0.1,\n",
    "    decoder_block=decoder_block_config,\n",
    ")\n",
    "\n",
    "model = GPTDecoder(model_config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warmup_steps = 3*len(dataloaders.train_loader)\n",
    "warmup_steps = 3 * len(train_loader)\n",
    "\n",
    "\n",
    "# lr first increases in the warmup steps, and then descreases\n",
    "lr_fn        = lambda step: model_config.d_model**(-0.5) * min([(step+1)**(-0.5), (step+1)*warmup_steps**(-1.5)])\n",
    "# optimizer    = torch.optim.Adam(model.parameters(), lr=0.2, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# optimizer_config = OptimizerConfig(name=\"torch.optim.Adam\", lr=0.2, betas=(0.9, 0.98), eps=1e-9)\n",
    "# optimizer   = optimizer_config.build(params=model.parameters())\n",
    "\n",
    "# optimizer_config = OptimizerConfig(name=\"torch.optim.Adam\", lr=0.2)\n",
    "# optimizer   = optimizer_config.build(params=model.parameters(), betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "optimizer   = config.optimizer.build(params=model.parameters())\n",
    "\n",
    "scheduler    = None\n",
    "criterion    = nn.CrossEntropyLoss(ignore_index=PAD, reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0, This Batch Train Loss: 2.626, This Batch Average Train Loss: 0.021, LR: LR: N/A:   1%|          | 79/8714 [01:13<2:13:03,  1.08it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 16\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      3\u001b[0m     train_dataloader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# but seeding will affect.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DEBUG:\n\u001b[0;32m---> 16\u001b[0m     trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# or 15\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# torch.save(model.state_dict(), 'model_debug.pt')\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# model_debug = torch.load('./model_debug.pt')\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# if are_both_models_same(model.state_dict(), model_debug):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     trained_model \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mfit(num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n",
      "File \u001b[0;32m~/omniverse/omniverse/omnivault/transformer/core/trainer.py:275\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, num_epochs)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_epoch()\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Loss   : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/omniverse/omniverse/omnivault/transformer/core/trainer.py:240\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Loss:\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_norm_clip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_norm_clip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/omniverse/omniverse/omnivault/transformer/core/trainer.py:90\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, scheduler, grad_norm_clip, device)\u001b[0m\n\u001b[1;32m     87\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_padding_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_padding_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuture_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m loss   \u001b[38;5;241m=\u001b[39m criterion(logits\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous(), targets\u001b[38;5;241m.\u001b[39mcontiguous())\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# model vs optimizer zero grad, the former is safer if you have >=2 optimizers\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/omniverse/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/omniverse/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/omniverse/omniverse/omnivault/transformer/decoder/core.py:241\u001b[0m, in \u001b[0;36mGPTDecoder.forward\u001b[0;34m(self, input_tokens, target_padding_masks, future_masks, encoder_hidden_states, encoder_hidden_states_masks)\u001b[0m\n\u001b[1;32m    238\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(z)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decoder_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_blocks:\n\u001b[0;32m--> 241\u001b[0m     z  \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m z      \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(z)\n\u001b[1;32m    244\u001b[0m logits: torch\u001b[38;5;241m.\u001b[39mFloatTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(z)\n",
      "File \u001b[0;32m~/miniconda3/envs/omniverse/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/omniverse/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/omniverse/omniverse/omnivault/transformer/decoder/core.py:73\u001b[0m, in \u001b[0;36mGPTDecoderBlock.forward\u001b[0;34m(self, z, encoder_hidden_states, encoder_hidden_states_masks, target_masks)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m target_masks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m NOT_GIVEN\n\u001b[1;32m     69\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_norm_1(\n\u001b[1;32m     70\u001b[0m     z,\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m z: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasked_self_attention_mha(query\u001b[38;5;241m=\u001b[39mz, key\u001b[38;5;241m=\u001b[39mz, value\u001b[38;5;241m=\u001b[39mz, mask\u001b[38;5;241m=\u001b[39mtarget_masks),\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_norm_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m z\n",
      "File \u001b[0;32m~/miniconda3/envs/omniverse/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/omniverse/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/omniverse/omniverse/omnivault/transformer/modules/layers/addnorm.py:27\u001b[0m, in \u001b[0;36mAddNorm.forward\u001b[0;34m(self, x, sublayer)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"G(F(x) + x) where G = layer norm and F = sublayer\"\"\"\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# TODO: to check if this is correct and use my own custom layer.\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m output: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(x \u001b[38;5;241m+\u001b[39m \u001b[43msublayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/omniverse/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/omniverse/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/omniverse/omniverse/omnivault/transformer/modules/layers/mlp.py:65\u001b[0m, in \u001b[0;36mPositionwiseFeedForward.forward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     63\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext_fc\u001b[39m\u001b[38;5;124m\"\u001b[39m](z)\n\u001b[1;32m     64\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m\"\u001b[39m](z)\n\u001b[0;32m---> 65\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffn\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdropout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext_projection\u001b[39m\u001b[38;5;124m\"\u001b[39m](z)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m z\n",
      "File \u001b[0;32m~/miniconda3/envs/omniverse/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/omniverse/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/omniverse/lib/python3.9/site-packages/torch/nn/modules/dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/omniverse/lib/python3.9/site-packages/torch/nn/functional.py:1266\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_loader,\n",
    "    valid_dataloader=train_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    grad_norm_clip=1.0,\n",
    "    device=DEVICE,\n",
    "    # test_dataloader=test_loader,\n",
    "    # NOTE: uncomment the above line to enable testing after each epoch\n",
    "    # but seeding will affect.\n",
    ")\n",
    "\n",
    "if DEBUG:\n",
    "    trained_model = trainer.fit(num_epochs=2) # or 15\n",
    "    # torch.save(model.state_dict(), 'model_debug.pt')\n",
    "    # model_debug = torch.load('./model_debug.pt')\n",
    "    # if are_both_models_same(model.state_dict(), model_debug):\n",
    "    #     print(\"Pass\")\n",
    "    # else:\n",
    "    #     print(\"Fail\")\n",
    "\n",
    "else:\n",
    "    trained_model = trainer.fit(num_epochs=30)\n",
    "\n",
    "    # torch.save(model.state_dict(), 'model_non_debug.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omniverse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
