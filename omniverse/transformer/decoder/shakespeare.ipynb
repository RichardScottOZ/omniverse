{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import requests\n",
    "from typing import List, Tuple, Dict, Any, Union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_root_dir(current_path: Path = Path.cwd(), marker: str = '.git') -> Path | None:\n",
    "    \"\"\"\n",
    "    Find the root directory by searching for a directory or file that serves as a\n",
    "    marker.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    current_path : Path\n",
    "        The starting path to search from.\n",
    "    marker : str\n",
    "        The name of the file or directory that signifies the root.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Path or None\n",
    "        The path to the root directory. Returns None if the marker is not found.\n",
    "    \"\"\"\n",
    "    current_path = current_path.resolve()\n",
    "    for parent in current_path.parents:\n",
    "        if (parent / marker).exists():\n",
    "            return parent\n",
    "    return None\n",
    "\n",
    "current_file_path = Path(os.getcwd())\n",
    "root_dir          = find_root_dir(current_file_path, marker='omnivault')\n",
    "\n",
    "if root_dir is not None:\n",
    "    sys.path.append(str(root_dir))\n",
    "    from omnivault.transformer.utils.reproducibility import seed_all\n",
    "    from omnivault.transformer.core.vocabulary import TextCharacterVocabulary\n",
    "    # from omnivault.transformer.core.dataset import TextCharacterDataset\n",
    "    from omnivault.transformer.core.tokenizer import TextCharacterTokenizer\n",
    "\n",
    "else:\n",
    "    raise ImportError(\"Root directory not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TinyShakespeareDataset(Dataset):\n",
    "#     def __init__(self, root_dir='data/tinyshakespeare'):\n",
    "#         self.root_dir = root_dir\n",
    "#         self.file_path = os.path.join(root_dir, 'input.txt')\n",
    "#         self._download()\n",
    "\n",
    "#         # Load and preprocess data\n",
    "#         self.data = self._load_data()\n",
    "\n",
    "#     def _download(self):\n",
    "#         \"\"\"Download the dataset if it's not already present.\"\"\"\n",
    "#         if not os.path.exists(self.root_dir):\n",
    "#             os.makedirs(self.root_dir)\n",
    "\n",
    "#         if not os.path.isfile(self.file_path):\n",
    "#             url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "#             response = requests.get(url)\n",
    "#             with open(self.file_path, 'wb') as f:\n",
    "#                 f.write(response.content)\n",
    "\n",
    "#     def _load_data(self):\n",
    "#         \"\"\"Load and preprocess the data.\"\"\"\n",
    "#         with open(self.file_path, 'r') as file:\n",
    "#             data = file.read()\n",
    "#         # Implement any specific preprocessing here\n",
    "#         return data\n",
    "\n",
    "#     def __len__(self):\n",
    "#         # Define the length of the dataset\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         # Implement how to get a single item (e.g., a character or a sequence)\n",
    "#         return self.data[idx]\n",
    "\n",
    "# # Example usage\n",
    "# dataset = TinyShakespeareDataset()\n",
    "# print(dataset[0])  # Access the first character/sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of characters\n",
    "    \"\"\"\n",
    "    def __init__(self, config, data):\n",
    "        self.config = config\n",
    "\n",
    "        chars = sorted(list(set(data)))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "\n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.config.block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.config.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx:idx + self.config.block_size + 1]\n",
    "        # encode every character to an integer\n",
    "        dix = [self.stoi[s] for s in chunk]\n",
    "        # return as tensors\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('../../../data/tinyshakespeare/input.txt', 'r').read() # don't worry we won't run out of file handles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115389 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(data)))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TextCharacterTokenizer:\n",
    "#     def __init__(self, vocabulary: List[str]):\n",
    "#         self.vocabulary = vocabulary\n",
    "#         self.vocab_size = len(self.vocabulary)\n",
    "#         self.str_to_int = {s: i for i, s in enumerate(self.vocabulary)}\n",
    "#         self.int_to_str = {i: s for i, s in enumerate(self.vocabulary)}\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_corpus(cls, text_corpus: str):\n",
    "#         vocabulary = sorted(set(text_corpus))\n",
    "#         return cls(vocabulary)\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_file(cls, file_path: Union[str, Path]):\n",
    "#         with open(file_path, \"r\") as f:\n",
    "#             text_corpus = f.read()\n",
    "#         return cls.from_corpus(text_corpus)\n",
    "\n",
    "#     def encode(self, text: str):\n",
    "#         return [self.str_to_int[s] for s in text]\n",
    "\n",
    "#     def decode(self, tokens: List[int]):\n",
    "#         return \"\".join([self.int_to_str[t] for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = TextCharacterVocabulary.from_file('../../../data/tinyshakespeare/input.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 46)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.vocab_size, vocabulary.token_to_index['h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TextCharacterTokenizer(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[46, 43, 50, 50, 53]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is the length of the dataset defined as `len(self.corpus) - self.context_length`?\n",
    "\n",
    "Why the length of the dataset is defined as\n",
    "`len(self.corpus) - self.context_length`. This design is common in datasets used\n",
    "for training language models, particularly autoregressive models like GPT. Let\n",
    "me elaborate further:\n",
    "\n",
    "### Understanding the Dataset Length in the Context of Language Models:\n",
    "\n",
    "1. **Training Samples Formation**:\n",
    "\n",
    "    - In an autoregressive model, each training sample typically consists of a\n",
    "      sequence of tokens used as input and a subsequent token (or tokens) used\n",
    "      as the target for prediction.\n",
    "    - If `context_length` is the size of the input sequence, then for any\n",
    "      starting point in the corpus, you need enough tokens following it to form\n",
    "      a complete input sequence.\n",
    "\n",
    "2. **Avoiding Out-of-Bounds Access**:\n",
    "\n",
    "    - As you approach the end of the corpus, there are fewer tokens available to\n",
    "      form a complete input sequence of `context_length`.\n",
    "    - For example, if the corpus length is 1000 tokens and `context_length` is\n",
    "      128, trying to form a sequence starting at token 900 would result in an\n",
    "      out-of-bounds access, as you would need tokens up to index 1027 (which\n",
    "      doesn't exist in the corpus).\n",
    "\n",
    "3. **Dataset Length Calculation**:\n",
    "\n",
    "    - To prevent this out-of-bounds issue, the length of the dataset is\n",
    "      restricted to `len(self.corpus) - self.context_length`. This ensures that\n",
    "      for any index `i` in the dataset, you can safely access the sequence\n",
    "      `self.corpus[i:i + context_length]` without exceeding the bounds of the\n",
    "      corpus.\n",
    "    - This adjustment means the dataset will not generate sequences that extend\n",
    "      beyond the end of the corpus.\n",
    "\n",
    "4. **Practical Example**:\n",
    "    - If `self.corpus` has 1000 characters and `self.context_length` is 128, the\n",
    "      last index accessed by the dataset (for the start of a sequence) will be\n",
    "      `1000 - 128 = 872`. The corresponding sequence will run from index 872 to\n",
    "      999, which is precisely 128 characters.\n",
    "\n",
    "In summary, the length of the dataset is calculated as\n",
    "`len(self.corpus) - self.context_length` to ensure that every training sample\n",
    "has a complete input sequence of the desired context length, without attempting\n",
    "to access data beyond the end of the corpus. This approach is a standard\n",
    "practice in preparing datasets for training language models.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omniverse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
