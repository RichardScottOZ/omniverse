{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Shakespeare](#toc1_)    \n",
    "  - [Vocabulary and Tokenization](#toc1_1_)    \n",
    "  - [Dataset and Dataloader](#toc1_2_)    \n",
    "    - [Why is the length of the dataset defined as `len(self.corpus) - self.context_length`?](#toc1_2_1_)    \n",
    "    - [Understanding the Dataset Length in the Context of Language Models:](#toc1_2_2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Shakespeare](#toc0_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "from rich.pretty import pprint\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "import requests\n",
    "from typing import List, Tuple, Dict, Any, Union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"She read the book under the tree.\",\n",
    "    \"He enjoys walking in the park on sunny days.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"I like to eat apples and bananas.\",\n",
    "    \"She writes a letter every Sunday.\",\n",
    "    \"They are playing soccer in the field.\",\n",
    "    \"The sun rises in the east.\",\n",
    "    \"She sells seashells by the seashore.\",\n",
    "    \"He learned to play the guitar.\",\n",
    "    \"The little boy lost his toy.\",\n",
    "    \"Birds fly south in the winter.\",\n",
    "    \"I saw a beautiful rainbow today.\",\n",
    "    \"She baked a chocolate cake for his birthday.\",\n",
    "    \"The museum opens at ten in the morning.\",\n",
    "    \"He fixed the leaky faucet.\",\n",
    "    \"They celebrated their anniversary at the beach.\",\n",
    "    \"The train arrives at six in the evening.\",\n",
    "    \"She planted roses in her garden.\",\n",
    "    \"The library has many books.\"\n",
    "]\n",
    "\n",
    "# Let's write these sentences to a text file\n",
    "with open(\"simple_english_dataset.txt\", \"w\") as file:\n",
    "    for sentence in sentences:\n",
    "        file.write(sentence + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_root_dir(current_path: Path = Path.cwd(), marker: str = '.git') -> Path | None:\n",
    "    \"\"\"\n",
    "    Find the root directory by searching for a directory or file that serves as a\n",
    "    marker.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    current_path : Path\n",
    "        The starting path to search from.\n",
    "    marker : str\n",
    "        The name of the file or directory that signifies the root.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Path or None\n",
    "        The path to the root directory. Returns None if the marker is not found.\n",
    "    \"\"\"\n",
    "    current_path = current_path.resolve()\n",
    "    for parent in current_path.parents:\n",
    "        if (parent / marker).exists():\n",
    "            return parent\n",
    "    return None\n",
    "\n",
    "current_file_path = Path(os.getcwd())\n",
    "root_dir          = find_root_dir(current_file_path, marker='omnivault')\n",
    "\n",
    "if root_dir is not None:\n",
    "    sys.path.append(str(root_dir))\n",
    "    from omnivault.transformer.utils.reproducibility import seed_all\n",
    "    from omnivault.transformer.core.vocabulary import TextCharacterVocabulary\n",
    "    from omnivault.transformer.core.dataset import TextCharacterDataset, collate_fn\n",
    "    from omnivault.transformer.core.tokenizer import TextCharacterTokenizer\n",
    "    from omnivault.transformer.config.composer import Composer, DataConfig\n",
    "    from omnivault.transformer.config.optim import OptimizerConfig, AdamWConfig\n",
    "    from omnivault.transformer.config.constants import MaybeConstant\n",
    "    from omnivault.transformer.config.global_ import MaybeGlobal\n",
    "    from omnivault.transformer.decoder.core import GPTDecoder, GPTDecoderBlock\n",
    "    from omnivault.transformer.config.decoder import *\n",
    "    from omnivault.transformer.modules.attention.core import ScaledDotProductAttention, MultiHeadedAttention\n",
    "    from omnivault.transformer.core.trainer import Trainer\n",
    "\n",
    "else:\n",
    "    raise ImportError(\"Root directory not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Vocabulary and Tokenization](#toc0_)\n",
    "\n",
    "See https://huggingface.co/learn/nlp-course/chapter2/4?fw=pt.\n",
    "\n",
    "Especially:\n",
    "\n",
    "> The second step is to convert those tokens into numbers, so we can build a tensor out of them and feed them to the model. To do this, the tokenizer has a vocabulary, which is the part we download when we instantiate it with the from_pretrained() method. Again, we need to use the same vocabulary used when the model was pretrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# sequence = \"Using a Transformer network is simple\"\n",
    "# tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('../../../data/tinyshakespeare/input.txt', 'r').read() # don't worry we won't run out of file handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115389 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(data)))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "vocabulary = TextCharacterVocabulary.from_file('../../../data/tinyshakespeare/input.txt')\n",
    "vocabulary_2 = TextCharacterVocabulary.from_url(url)\n",
    "assert vocabulary.index_to_token == vocabulary_2.index_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 46)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.vocab_size, vocabulary.token_to_index['h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TextCharacterTokenizer(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence hello\n",
      "sequence hello\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['h', 'e', 'l', 'l', 'o'], [46, 43, 50, 50, 53], 'hello')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('hello'), tokenizer.encode('hello'), tokenizer.decode(tokenizer.encode('hello'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Dataset and Dataloader](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('../../../data/tinyshakespeare/input.txt', 'r').read()\n",
    "# subset corpus to first 100000 characters\n",
    "corpus = corpus[:100000]\n",
    "\n",
    "\n",
    "dataset = TextCharacterDataset(corpus=corpus, context_length=128, tokenizer=tokenizer)\n",
    "\n",
    "dataset[0], tokenizer.decode(dataset[0][0].tolist()), tokenizer.decode(dataset[0][1].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_1_'></a>[Why is the length of the dataset defined as `len(self.corpus) - self.context_length`?](#toc0_)\n",
    "\n",
    "Why the length of the dataset is defined as\n",
    "`len(self.corpus) - self.context_length`. This design is common in datasets used\n",
    "for training language models, particularly autoregressive models like GPT. Let\n",
    "me elaborate further:\n",
    "\n",
    "### <a id='toc1_2_2_'></a>[Understanding the Dataset Length in the Context of Language Models:](#toc0_)\n",
    "\n",
    "1. **Training Samples Formation**:\n",
    "\n",
    "    - In an autoregressive model, each training sample typically consists of a\n",
    "      sequence of tokens used as input and a subsequent token (or tokens) used\n",
    "      as the target for prediction.\n",
    "    - If `context_length` is the size of the input sequence, then for any\n",
    "      starting point in the corpus, you need enough tokens following it to form\n",
    "      a complete input sequence.\n",
    "\n",
    "2. **Avoiding Out-of-Bounds Access**:\n",
    "\n",
    "    - As you approach the end of the corpus, there are fewer tokens available to\n",
    "      form a complete input sequence of `context_length`.\n",
    "    - For example, if the corpus length is 1000 tokens and `context_length` is\n",
    "      128, trying to form a sequence starting at token 900 would result in an\n",
    "      out-of-bounds access, as you would need tokens up to index 1027 (which\n",
    "      doesn't exist in the corpus).\n",
    "\n",
    "3. **Dataset Length Calculation**:\n",
    "\n",
    "    - To prevent this out-of-bounds issue, the length of the dataset is\n",
    "      restricted to `len(self.corpus) - self.context_length`. This ensures that\n",
    "      for any index `i` in the dataset, you can safely access the sequence\n",
    "      `self.corpus[i:i + context_length]` without exceeding the bounds of the\n",
    "      corpus.\n",
    "    - This adjustment means the dataset will not generate sequences that extend\n",
    "      beyond the end of the corpus.\n",
    "\n",
    "4. **Practical Example**:\n",
    "    - If `self.corpus` has 1000 characters and `self.context_length` is 128, the\n",
    "      last index accessed by the dataset (for the start of a sequence) will be\n",
    "      `1000 - 128 = 872`. The corresponding sequence will run from index 872 to\n",
    "      999, which is precisely 128 characters.\n",
    "\n",
    "In summary, the length of the dataset is calculated as\n",
    "`len(self.corpus) - self.context_length` to ensure that every training sample\n",
    "has a complete input sequence of the desired context length, without attempting\n",
    "to access data beyond the end of the corpus. This approach is a standard\n",
    "practice in preparing datasets for training language models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constants: MaybeConstant = MaybeConstant()\n",
    "global_: MaybeGlobal = MaybeGlobal(seed=42, debug=False)\n",
    "data_config: DataConfig = DataConfig()\n",
    "optimizer_config = AdamWConfig(name=\"torch.optim.AdamW\", lr=5e-4)\n",
    "\n",
    "config = Composer(constants=constants, global_=global_, data=data_config, optimizer=optimizer_config)\n",
    "pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG  = True\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "seed_all(config.global_.seed, seed_torch=True, set_torch_deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD=100\n",
    "collate_fn_config = {\"batch_first\": True, \"pad_token_id\": PAD}\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=128, shuffle=True, collate_fn=lambda batch: collate_fn(batch, **collate_fn_config))\n",
    "sample_batch = next(iter(train_loader))\n",
    "x, y, padding_mask, future_mask = sample_batch\n",
    "x, y, padding_mask, future_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(x[0]), tokenizer.decode(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create individual component configurations\n",
    "\n",
    "masked_self_attention_mha_config = MultiHeadedAttentionConfig(\n",
    "     attention=ScaledDotProductAttention(),\n",
    "    d_model=128, H=8, dropout=0.1\n",
    ")\n",
    "feed_forward_config = PositionwiseFeedForwardConfig(\n",
    "    d_model=128, d_ff=256, activation=nn.GELU(approximate=\"tanh\"), dropout=0.1, bias=True\n",
    ")\n",
    "\n",
    "add_norm_config_1 = AddNormConfig(feature_dim=128, dropout=0.1)\n",
    "add_norm_config_2 = AddNormConfig(feature_dim=128, dropout=0.1)\n",
    "\n",
    "# Create DecoderBlockConfig\n",
    "decoder_block_config = DecoderBlockConfig(\n",
    "    masked_self_attention_mha=masked_self_attention_mha_config,\n",
    "    feed_forward=feed_forward_config,\n",
    "    add_norm_1=add_norm_config_1,\n",
    "    add_norm_2=add_norm_config_2,\n",
    ")\n",
    "\n",
    "# Create the overall DecoderConfig\n",
    "model_config = DecoderConfig(\n",
    "    d_model=128,\n",
    "    vocab_size=vocab_size,\n",
    "    context_length=128,\n",
    "    num_decoder_blocks=5,\n",
    "    dropout=0.1,\n",
    "    decoder_block=decoder_block_config,\n",
    ")\n",
    "\n",
    "model = GPTDecoder(model_config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warmup_steps = 3*len(dataloaders.train_loader)\n",
    "warmup_steps = 3 * len(train_loader)\n",
    "\n",
    "\n",
    "# lr first increases in the warmup steps, and then descreases\n",
    "lr_fn        = lambda step: model_config.d_model**(-0.5) * min([(step+1)**(-0.5), (step+1)*warmup_steps**(-1.5)])\n",
    "# optimizer    = torch.optim.Adam(model.parameters(), lr=0.2, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# optimizer_config = OptimizerConfig(name=\"torch.optim.Adam\", lr=0.2, betas=(0.9, 0.98), eps=1e-9)\n",
    "# optimizer   = optimizer_config.build(params=model.parameters())\n",
    "\n",
    "# optimizer_config = OptimizerConfig(name=\"torch.optim.Adam\", lr=0.2)\n",
    "# optimizer   = optimizer_config.build(params=model.parameters(), betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "optimizer   = config.optimizer.build(params=model.parameters())\n",
    "\n",
    "scheduler    = None\n",
    "criterion    = nn.CrossEntropyLoss(ignore_index=PAD, reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_loader,\n",
    "    valid_dataloader=train_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    grad_norm_clip=1.0,\n",
    "    device=DEVICE,\n",
    "    # test_dataloader=test_loader,\n",
    "    # NOTE: uncomment the above line to enable testing after each epoch\n",
    "    # but seeding will affect.\n",
    ")\n",
    "\n",
    "if DEBUG:\n",
    "    trained_model = trainer.fit(max_epochs=5) # or 15\n",
    "    torch.save(model.state_dict(), 'model_debug.pt')\n",
    "    # model_debug = torch.load('./model_debug.pt')\n",
    "    # if are_both_models_same(model.state_dict(), model_debug):\n",
    "    #     print(\"Pass\")\n",
    "    # else:\n",
    "    #     print(\"Fail\")\n",
    "\n",
    "else:\n",
    "    trained_model = trainer.fit(max_epochs=30)\n",
    "\n",
    "    # torch.save(model.state_dict(), 'model_non_debug.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GPT.from_checkpoint(\"best_model.ckpt\")\n",
    "\n",
    "context = \"KING JOHNNY:\\nTo be, or not to be: that is the question.\"\n",
    "\n",
    "context = \"The word is 'mildly.' Pray you, let us go:\"\n",
    "# with open(\"generated-shakespeare.txt\", \"w\") as f:\n",
    "#     f.write(tokenizer.decode(generation.flatten().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from typing import Union, List, Optional\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(\n",
    "    model: nn.Module,\n",
    "    idx: Union[List[int], torch.LongTensor],\n",
    "    num_generated_tokens: int,\n",
    "    temperature: float = 1.0,\n",
    "    do_sample: bool = False,\n",
    "    top_k: Optional[int] = None,\n",
    "    as_list: bool = False,\n",
    "):\n",
    "    device = next(model.parameters()).device\n",
    "    idx = torch.as_tensor(idx, dtype=torch.long, device=device)[None, ...]\n",
    "    changed_training_mode = False\n",
    "\n",
    "    if model.training:\n",
    "        model.eval()\n",
    "        changed_training_mode = True\n",
    "\n",
    "    for _ in range(num_generated_tokens):\n",
    "        idx_cond = idx if idx.size(1) <= model.config.context_length else idx[:, -model.config.context_length :]\n",
    "        seq_len = idx_cond.size(1)\n",
    "        target_padding_masks = torch.ones((idx_cond.size(0), 1, seq_len, seq_len), dtype=torch.bool, device=idx_cond.device)\n",
    "\n",
    "        logits = model(idx_cond, target_padding_masks=target_padding_masks)\n",
    "        print(logits.shape)\n",
    "        print(logits)\n",
    "        print(logits[:, -1, :].shape)\n",
    "        print(logits[:, -1, :])\n",
    "        time.sleep(1000)\n",
    "        logits = logits[:, -1, :] / (temperature + 1e-8)\n",
    "\n",
    "        if top_k is not None:\n",
    "            logits[logits < torch.topk(logits, top_k)[0][:, [-1]]] = -float(\"Inf\")\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1) if do_sample else torch.topk(probs, k=1, dim=-1)[1]\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    if changed_training_mode:\n",
    "        model.train()\n",
    "\n",
    "    return idx.cpu().squeeze().tolist() if as_list else idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation = generate(model=trained_model,\n",
    "                      idx=tokenizer.encode(context),\n",
    "                        num_generated_tokens=3000,\n",
    "                        do_sample=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(generation.flatten().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omniverse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
