{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shakespeare\n",
    "\n",
    "\n",
    "```{contents}\n",
    ":local:\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Literal, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "from numpy.typing import ArrayLike, NDArray\n",
    "import requests\n",
    "import tiktoken\n",
    "import torch\n",
    "from rich.pretty import pprint\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn\n",
    "\n",
    "from pydantic import BaseModel, Field, model_validator, computed_field\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composing the Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Composer</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">seed</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">debug</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">dataset_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'tinyshakespeare'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">data_folder</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'./data/tinyshakespeare'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">train_path</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PosixPath</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'data/tinyshakespeare/train.txt'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">valid_path</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PosixPath</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'data/tinyshakespeare/valid.txt'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">encoding_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt2'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">batch_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">block_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">device_type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'cpu'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">device</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">device</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'cpu'</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mComposer\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mseed\u001b[0m=\u001b[1;36m2024\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mdebug\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33murl\u001b[0m=\u001b[32m'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mdataset_name\u001b[0m=\u001b[32m'tinyshakespeare'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mdata_folder\u001b[0m=\u001b[32m'./data/tinyshakespeare'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mtrain_path\u001b[0m=\u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'data/tinyshakespeare/train.txt'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mvalid_path\u001b[0m=\u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'data/tinyshakespeare/valid.txt'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mencoding_name\u001b[0m=\u001b[32m'gpt2'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mbatch_size\u001b[0m=\u001b[1;36m2\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mblock_size\u001b[0m=\u001b[1;36m8\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mdevice_type\u001b[0m=\u001b[32m'cpu'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mdevice\u001b[0m=\u001b[1;35mdevice\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtype\u001b[0m=\u001b[32m'cpu'\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Composer(BaseModel):\n",
    "    seed: int = 2024\n",
    "    debug: bool = False\n",
    "\n",
    "    url: str = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    dataset_name: str = \"tinyshakespeare\"\n",
    "    data_folder: str = Field(default=\"./data/tinyshakespeare\", description=\"Path to the data folder\")\n",
    "\n",
    "    train_path: Path = Field(None, description=\"Path to the train file\")\n",
    "    valid_path: Path = Field(None, description=\"Path to the valid file\")\n",
    "\n",
    "    encoding_name: Literal['gpt2', 'r50k_base', 'p50k_base', 'p50k_edit', 'cl100k_base'] = \"gpt2\"\n",
    "\n",
    "    batch_size: int = Field(default=64, description=\"Batch size\")\n",
    "    block_size: int = Field(default=128, description=\"Block size, an alias for max length/context window size.\", alias=\"context_length\")\n",
    "    device_type: Literal[\"cpu\", \"cuda\"] = \"cpu\"\n",
    "    device: torch.device = Field(None, description=\"Device to use\")\n",
    "\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def set_train_valid_paths(self) -> Composer:\n",
    "        self.train_path = Path(self.data_folder) / \"train.txt\"\n",
    "        self.valid_path = Path(self.data_folder) / \"valid.txt\"\n",
    "        return self\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def set_device(self) -> Composer:\n",
    "        self.device = torch.device(self.device_type)\n",
    "        return self\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def set_debug_fields(self) -> Composer:\n",
    "        if self.debug:\n",
    "            self.batch_size = 2\n",
    "            self.block_size = 8\n",
    "        return self\n",
    "\n",
    "    class Config:\n",
    "        extra = \"forbid\"\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "composer = Composer(debug=True)\n",
    "pprint(composer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_deterministic_mode() -> None:\n",
    "    \"\"\"\n",
    "    See https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html\n",
    "    and https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility\n",
    "    \"\"\"\n",
    "    # fmt: off\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    torch.backends.cudnn.benchmark        = False\n",
    "    torch.backends.cudnn.deterministic    = True\n",
    "    torch.backends.cudnn.enabled          = False\n",
    "\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "    # fmt: on\n",
    "    warnings.warn(\n",
    "        \"Deterministic mode is activated. This will negatively impact performance and may cause increase in CUDA memory footprint.\",\n",
    "        category=UserWarning,\n",
    "        stacklevel=2,\n",
    "    )\n",
    "\n",
    "\n",
    "def seed_all(\n",
    "    seed: int = 1992,\n",
    "    seed_torch: bool = True,\n",
    "    set_torch_deterministic: bool = True,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Seed all random number generators.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seed : int\n",
    "        Seed number to be used, by default 1992.\n",
    "    seed_torch : bool\n",
    "        Whether to seed PyTorch or not, by default True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    seed: int\n",
    "        The seed number.\n",
    "    \"\"\"\n",
    "    # fmt: off\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)       # set PYTHONHASHSEED env var at fixed value\n",
    "    np.random.default_rng(seed)                    # numpy pseudo-random generator\n",
    "    random.seed(seed)                              # python's built-in pseudo-random generator\n",
    "\n",
    "    if seed_torch:\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)           # pytorch (both CPU and CUDA)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.enabled = False\n",
    "\n",
    "        if set_torch_deterministic:\n",
    "            configure_deterministic_mode()\n",
    "    # fmt: on\n",
    "    return seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2024"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_all(composer.seed, seed_torch=True, set_torch_deterministic=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Vocabulary\n",
    "\n",
    "- https://huggingface.co/learn/nlp-course/chapter2/4?fw=pt\n",
    "- https://github.com/openai/tiktoken\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "\n",
    "Language models don't see text like you and I, instead they see a sequence of numbers (known as tokens). Byte pair encoding (BPE) is a way of converting text into tokens. It has a couple desirable properties[^1]:\n",
    "\n",
    "- It's reversible and lossless, so you can convert tokens back into the original text\n",
    "- It works on arbitrary text, even text that is not in the tokeniser's training data\n",
    "- It compresses the text: the token sequence is shorter than the bytes corresponding to the original text. On average, in practice, each token corresponds to about 4 bytes.\n",
    "- It attempts to let the model see common subwords. For instance, \"ing\" is a common subword in English, so BPE encodings will often split \"encoding\" into tokens like \"encod\" and \"ing\" (instead of e.g. \"enc\" and \"oding\"). Because the model will then see the \"ing\" token again and again in different contexts, it helps models generalise and better understand grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def am_i_in_jupyter() -> bool:\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        if \"IPKernelApp\" not in get_ipython().config:\n",
    "            return False\n",
    "    except ImportError:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "IN_JUPYTER = am_i_in_jupyter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url: str, dataset_name: str, dest_folder: Path | str) -> Path:\n",
    "    dest_folder_path = Path(dest_folder)\n",
    "\n",
    "    dest_folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    filepath = dest_folder_path / f\"{dataset_name}.txt\"\n",
    "\n",
    "    response = requests.get(url, stream=True)\n",
    "    corpus = response.text\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "    return filepath, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PosixPath</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'data/tinyshakespeare/tinyshakespeare.txt'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'data/tinyshakespeare/tinyshakespeare.txt'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "filepath, corpus = download(composer.url, composer.dataset_name, composer.data_folder)\n",
    "pprint(filepath)\n",
    "print(corpus[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt2', 'r50k_base', 'p50k_base', 'p50k_edit', 'cl100k_base']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(corpus)\n",
    "train_data = corpus[: int(N * 0.9)]\n",
    "valid_data = corpus[int(N * 0.9) :]\n",
    "\n",
    "# encode with tiktoken gpt2 bpe\n",
    "tokenizer = tiktoken.get_encoding(composer.encoding_name)\n",
    "tiktoken.list_encoding_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 301,966 tokens\n",
      "val has 36,059 tokens\n"
     ]
    }
   ],
   "source": [
    "train_ids = tokenizer.encode_ordinary(train_data)\n",
    "valid_ids = tokenizer.encode_ordinary(valid_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(valid_ids):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen\n",
      "--------------------------------------------------------------------------------\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(train_ids[:2]))\n",
    "print(\"-\" * 80)\n",
    "print(tokenizer.decode(train_ids[:100]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to bin files\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "valid_ids = np.array(valid_ids, dtype=np.uint16)\n",
    "\n",
    "train_ids.tofile(composer.train_path)\n",
    "valid_ids.tofile(composer.valid_path)\n",
    "\n",
    "# train.bin has 301,966 tokens\n",
    "# val.bin has 36,059 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloading (Poor Man's Dataloader)\n",
    "\n",
    "As Karpathy puts it, he implemented a poor man's\n",
    "[dataloader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n",
    "We will start by dissecting the code and understanding how it works and finally,\n",
    "show that everything can be done with PyTorch's `Dataset` and `Dataloader`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Mapping\n",
    "\n",
    "Firstly, Kaparthy uses `numpy`'s\n",
    "[memory mapping](https://numpy.org/doc/stable/reference/generated/numpy.memmap.html)\n",
    "(`numpy.memmap`) to load the data. Memory mapping is used to create a\n",
    "memory-mapped array from a binary file. This involves mapping the contents of a\n",
    "file directly into the virtual memory space of the calling process. This allows\n",
    "applications to access the file data as if it were loaded in memory, using\n",
    "pointer operations or array indexing, without the need for explicit read or\n",
    "write operations.\n",
    "\n",
    "This essentially means that you can access small segments of a large file\n",
    "without having to load the entire file into memory. The concept draws\n",
    "similarities to the use of [generators](https://wiki.python.org/moin/Generators)\n",
    "in Python, where you can iterate over a large dataset without having to load the\n",
    "entire dataset into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_dtype: uint16, data_shape: (301966,)\n"
     ]
    }
   ],
   "source": [
    "train_data = np.memmap(composer.train_path, dtype=np.uint16, mode=\"r\")\n",
    "train_data_dtype = train_data.dtype\n",
    "train_data_shape = train_data.shape\n",
    "\n",
    "print(f\"data_dtype: {train_data_dtype}, data_shape: {train_data_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the shape of train data is `(301966,)`, which means that it is a 1D (flattened) array \n",
    "with $301966$ elements - this is basically the length of the entire train corpus, in terms of\n",
    "tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation, Context Length, Shuffling and Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we are not going to pass the entire training corpus as is to the model.\n",
    "Instead, we are going to pass a **batch** of sequences (each sequence of length\n",
    "`context_length`) to the model at a time.\n",
    "\n",
    "#### Notation\n",
    "\n",
    "More formally, let's consider a sequence $\\mathcal{S}$ of tokens\n",
    "$(t_1, t_2, \\ldots, t_L)$, where $L$ is the length of the sequence. We are going\n",
    "to pass a batch of sequences $\\mathcal{B}$ to the model at a time. Consequently,\n",
    "the input sequence to the model can be more concisely defined as a matrix $\\mathbf{X}$\n",
    "residing in the space $\\mathbb{R}^{\\mathcal{B} \\times L}$, where $\\mathcal{B}$ is the batch size and\n",
    "$L$ is the length of the sequence.\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "t_{1,1} & t_{1,2} & \\ldots & t_{1,L} \\\\\n",
    "t_{2,1} & t_{2,2} & \\ldots & t_{2,L} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "t_{B,1} & t_{B,2} & \\ldots & t_{B,L} \\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{B \\times L}\n",
    "$$\n",
    "\n",
    "where $t_{i,j}$ is the $j$-th token in the $i$-th sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context Length / Block Size\n",
    "\n",
    "$L$ is often referred to as the sequence length, or in the context of GPT, it is\n",
    "the `block_size` or `context_length` or `max_seq_len`.\n",
    "\n",
    "It is the length of the sequence that the model will be trained on and is also\n",
    "the context length/context window that we often hear about.\n",
    "\n",
    "For example,\n",
    "[Gemini 1.5](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024)\n",
    "was announced to have a standard $128,000$ token context window, up to a maximum\n",
    "of $1$ million max length.\n",
    "\n",
    "Typically, I think that if your model is trained on a certain context length, it\n",
    "is not trivial to change it. For example, if you train a model on a context\n",
    "length of $128$, you cannot simply change it to $256$ without retraining the\n",
    "model. But it seems that it is increasingly possible to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example, if we define our $L$ to be $32$, then we would expect each\n",
    "sequence to be of length $32$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">memmap</span><span style=\"font-weight: bold\">([</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5962</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22307</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8421</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">356</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5120</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">597</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2252</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │      </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3285</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">502</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2740</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3237</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5248</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">461</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2740</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5962</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22307</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1639</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">389</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">uint16</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mmemmap\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m5962\u001b[0m, \u001b[1;36m22307\u001b[0m,    \u001b[1;36m25\u001b[0m,   \u001b[1;36m198\u001b[0m,  \u001b[1;36m8421\u001b[0m,   \u001b[1;36m356\u001b[0m,  \u001b[1;36m5120\u001b[0m,   \u001b[1;36m597\u001b[0m,  \u001b[1;36m2252\u001b[0m,\n",
       "\u001b[2;32m│   │      \u001b[0m\u001b[1;36m11\u001b[0m,  \u001b[1;36m3285\u001b[0m,   \u001b[1;36m502\u001b[0m,  \u001b[1;36m2740\u001b[0m,    \u001b[1;36m13\u001b[0m,   \u001b[1;36m198\u001b[0m,   \u001b[1;36m198\u001b[0m,  \u001b[1;36m3237\u001b[0m,    \u001b[1;36m25\u001b[0m,\n",
       "\u001b[2;32m│   │     \u001b[0m\u001b[1;36m198\u001b[0m,  \u001b[1;36m5248\u001b[0m,   \u001b[1;36m461\u001b[0m,    \u001b[1;36m11\u001b[0m,  \u001b[1;36m2740\u001b[0m,    \u001b[1;36m13\u001b[0m,   \u001b[1;36m198\u001b[0m,   \u001b[1;36m198\u001b[0m,  \u001b[1;36m5962\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1;36m22307\u001b[0m,    \u001b[1;36m25\u001b[0m,   \u001b[1;36m198\u001b[0m,  \u001b[1;36m1639\u001b[0m,   \u001b[1;36m389\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35muint16\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are\n"
     ]
    }
   ],
   "source": [
    "first_sequence = train_data[0:0+32]\n",
    "pprint(first_sequence)\n",
    "\n",
    "first_sequence_decoded = tokenizer.decode(first_sequence)\n",
    "print(first_sequence_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example is just extracting $1$ such sequence $\\mathcal{S}$ from the train\n",
    "corpus. To leverage the prowess of linear algebra operations in CUDA, we would\n",
    "typically pass a batch of sequences $\\mathcal{B}$ to the model at a time.\n",
    "\n",
    "Furthermore, we would require some level of randomness in the sequences that we\n",
    "pass to the model to enable generalisation. You really do not want the model to\n",
    "overfit to an ordered sequence of tokens in the training corpus.\n",
    "\n",
    "To this end, let's see how Karpathy implements batching and shuffling of the\n",
    "sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffling and Discrete Uniform Sampling\n",
    "\n",
    "To enable shuffling, Karpathy generates a tensor of random integers (essentially a list of\n",
    "random integers), which serve as indices. These indices are used to select\n",
    "random sequences from the training (and validation) data.\n",
    "\n",
    "For simplicity, let's look at the case where batch size is reduced to $\\mathcal{B} = 1$.\n",
    "This means we only need to sample $1$ sequence from the training data - and consequently\n",
    "we only need $1$ random index.\n",
    "\n",
    "We can easily achieve this via `torch.randint` which generates random integers\n",
    "from a discrete uniform distribution over the half-open interval $[l, h)$,\n",
    "and since we only want to sample $1$ sequence, we set `size=(1,)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">136016</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m136016\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = torch.Generator(device=composer.device)\n",
    "low, high = 0, len(train_data) - composer.block_size\n",
    "size = (1,)\n",
    "indices: torch.Tensor = torch.randint(low=low, high=high, size=size, generator=generator)\n",
    "pprint(indices)\n",
    "pprint(indices.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mathematical operation performed by `torch.randint(low, high, size, generator)` can be described as drawing samples from a uniform discrete distribution. Each element of the resulting tensor is an independent and identically distributed {cite}`radford2019language` (i.i.d.) random variable $X_i$ with the following probability mass function (PMF):\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(X_i = k) = \\frac{1}{h - l} \\quad \\text{for} \\, k = l, \\ldots, h-1 \n",
    "$$\n",
    "\n",
    "This PMF implies that each integer in the range $[l, h-1]$ has an equal probability of being selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, we randomly sampled an index $136,016$ from the training data. We can then\n",
    "construct a sequence $\\mathcal{S}$ by selecting the tokens at the index $136,016$ and the\n",
    "next $L$ tokens defined by the block size. For simplcity, we will set $L = 8$, which \n",
    "we have conveniently defined it in our `composer` configuration when `debug` is set to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">memmap</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3194</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">612</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">290</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">284</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">606</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">910</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">uint16</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mmemmap\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m3194\u001b[0m,  \u001b[1;36m612\u001b[0m,   \u001b[1;36m11\u001b[0m,  \u001b[1;36m290\u001b[0m,  \u001b[1;36m284\u001b[0m,  \u001b[1;36m606\u001b[0m,  \u001b[1;36m910\u001b[0m,   \u001b[1;36m11\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35muint16\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m8\u001b[0m,\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "' written there, and to them say,'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_sequence = train_data[indices:indices+composer.block_size]\n",
    "pprint(random_sequence)\n",
    "pprint(random_sequence.shape)\n",
    "\n",
    "random_sequence_decoded = tokenizer.decode(random_sequence)\n",
    "tokenizer.decode(random_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One might wonder why the highest value of the random integers is\n",
    "`len(self.train_data) - self.block_size`. This is mostly to prevent index out of\n",
    "range errors. As we shall soon see, we are using these `indices` to slice a\n",
    "sequence of length `block_size` from the data where you start slicing from the\n",
    "index `index` and end at `index + block_size`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching\n",
    "\n",
    "Now that we understand how to sample a single sequence from the training data,\n",
    "let's look at how we can sample a batch of sequences.\n",
    "PyTorch made it easy for you, as we can just simply change the `size` parameter\n",
    "to `(batch_size,)`.\n",
    "\n",
    "In our case, if we set $\\mathcal{B} = 2$, we would expect to sample $2$ sequences\n",
    "from the training data - and consequently we would need $2$ random indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">136016</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">197976</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m136016\u001b[0m, \u001b[1;36m197976\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = torch.Generator(device=composer.device)\n",
    "low, high = 0, len(train_data) - composer.block_size\n",
    "size = (composer.batch_size,)\n",
    "indices: torch.Tensor = torch.randint(low=low, high=high, size=size, generator=generator)\n",
    "pprint(indices)\n",
    "pprint(indices.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then construct a batch of sequences $\\mathcal{B}$ by selecting the tokens at the\n",
    "indices $136,016$ and $197,976$ and the next $L$ tokens via a for loop - and using `torch.stack`\n",
    "to stack the sequences into a tensor of shape $\\mathbb{R}^{\\mathcal{B} \\times L}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3194</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">612</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">290</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">284</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">606</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">910</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4713</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5127</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11486</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">304</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">260</span><span style=\"font-weight: bold\">]])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m3194\u001b[0m,   \u001b[1;36m612\u001b[0m,    \u001b[1;36m11\u001b[0m,   \u001b[1;36m290\u001b[0m,   \u001b[1;36m284\u001b[0m,   \u001b[1;36m606\u001b[0m,   \u001b[1;36m910\u001b[0m,    \u001b[1;36m11\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m4713\u001b[0m,  \u001b[1;36m5127\u001b[0m,    \u001b[1;36m13\u001b[0m,   \u001b[1;36m198\u001b[0m, \u001b[1;36m11486\u001b[0m,    \u001b[1;36m11\u001b[0m,   \u001b[1;36m304\u001b[0m,   \u001b[1;36m260\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m8\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = torch.stack([torch.from_numpy((train_data[index : index + composer.block_size]).astype(np.int64)) for index in indices])\n",
    "pprint(x)\n",
    "pprint(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth reconciling the fact that the slicing uses `[index:index + block_size]` and\n",
    "therefore completes the reasoning behind the `len(self.train_data) - self.block_size` in\n",
    "the `torch.randint` function call - to prevent index out of range errors. Consider\n",
    "that if we do not subtract `block_size` from the length of the training data, we might\n",
    "end up with an index that is the last index of the training data, and when we add\n",
    "`block_size` to it, we would end up with an index that is out of range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3936093827.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[30], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    **check makemore notes**\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**check makemore notes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to fill in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">228358</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m228358\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">290</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">262</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27636</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2405</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5211</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5587</span><span style=\"font-weight: bold\">]])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m   \u001b[1;36m26\u001b[0m,   \u001b[1;36m290\u001b[0m,   \u001b[1;36m262\u001b[0m, \u001b[1;36m27636\u001b[0m,  \u001b[1;36m2405\u001b[0m,   \u001b[1;36m198\u001b[0m,  \u001b[1;36m5211\u001b[0m,  \u001b[1;36m5587\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">290</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">262</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27636</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2405</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5211</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5587</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">379</span><span style=\"font-weight: bold\">]])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m  \u001b[1;36m290\u001b[0m,   \u001b[1;36m262\u001b[0m, \u001b[1;36m27636\u001b[0m,  \u001b[1;36m2405\u001b[0m,   \u001b[1;36m198\u001b[0m,  \u001b[1;36m5211\u001b[0m,  \u001b[1;36m5587\u001b[0m,   \u001b[1;36m379\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_batch(\n",
    "    composer: Composer,\n",
    "    *,\n",
    "    split: Literal[\"train\", \"valid\"],\n",
    "    batch_size: int,\n",
    "    block_size: int,\n",
    "    device: torch.device,\n",
    "    device_type: Literal[\"cpu\", \"cuda\"] = \"cpu\",\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
    "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
    "    if split == \"train\":\n",
    "        data = np.memmap(composer.train_path, dtype=np.uint16, mode=\"r\")\n",
    "    else:\n",
    "        data = np.memmap(composer.valid_path, dtype=np.uint16, mode=\"r\")\n",
    "\n",
    "\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    pprint(ix)\n",
    "    x_list_of_numpy: List[NDArray[np.int64]] = [data[i : i + block_size] for i in ix]\n",
    "    y_list_of_numpy: List[NDArray[np.int64]] = [data[i + 1 : i + 1 + block_size] for i in ix]\n",
    "\n",
    "    x = torch.stack([torch.from_numpy((data[i : i + block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i + 1 : i + 1 + block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == \"cuda\":\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "seed_all(composer.seed, seed_torch=True, set_torch_deterministic=False)\n",
    "train_batch = get_batch(composer, split=\"train\", batch_size=1, block_size=8, device=composer.device)\n",
    "x, y = train_batch\n",
    "pprint(x)\n",
    "pprint(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is relatively simple to understand - and since there is not a need to\n",
    "[collate](https://pytorch.org/docs/stable/data.html#dataloader-collate-fn) the\n",
    "data, which makes things a bit easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m8\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m8\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(x.shape)\n",
    "pprint(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^1]: [OpenAI tiktoken](https://github.com/openai/tiktoken)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omniverse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
