{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shakespeare\n",
    "\n",
    "\n",
    "```{contents}\n",
    ":local:\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Literal, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "from numpy.typing import ArrayLike, NDArray\n",
    "import requests\n",
    "import tiktoken\n",
    "import torch\n",
    "from rich.pretty import pprint\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn\n",
    "\n",
    "from pydantic import BaseModel, Field, model_validator, computed_field\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composing the Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Composer</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">seed</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">debug</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">dataset_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'tinyshakespeare'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">data_folder</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'./data/tinyshakespeare'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">train_path</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PosixPath</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'data/tinyshakespeare/train.txt'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">valid_path</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PosixPath</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'data/tinyshakespeare/valid.txt'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">encoding_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt2'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">batch_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">block_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">device_type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'cpu'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">device</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">device</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'cpu'</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mComposer\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mseed\u001b[0m=\u001b[1;36m2024\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mdebug\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33murl\u001b[0m=\u001b[32m'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mdataset_name\u001b[0m=\u001b[32m'tinyshakespeare'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mdata_folder\u001b[0m=\u001b[32m'./data/tinyshakespeare'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mtrain_path\u001b[0m=\u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'data/tinyshakespeare/train.txt'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mvalid_path\u001b[0m=\u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'data/tinyshakespeare/valid.txt'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mencoding_name\u001b[0m=\u001b[32m'gpt2'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mbatch_size\u001b[0m=\u001b[1;36m2\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mblock_size\u001b[0m=\u001b[1;36m8\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mdevice_type\u001b[0m=\u001b[32m'cpu'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mdevice\u001b[0m=\u001b[1;35mdevice\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtype\u001b[0m=\u001b[32m'cpu'\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Composer(BaseModel):\n",
    "    seed: int = 2024\n",
    "    debug: bool = False\n",
    "\n",
    "    url: str = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    dataset_name: str = \"tinyshakespeare\"\n",
    "    data_folder: str = Field(default=\"./data/tinyshakespeare\", description=\"Path to the data folder\")\n",
    "\n",
    "    train_path: Path = Field(None, description=\"Path to the train file\")\n",
    "    valid_path: Path = Field(None, description=\"Path to the valid file\")\n",
    "\n",
    "    encoding_name: Literal['gpt2', 'r50k_base', 'p50k_base', 'p50k_edit', 'cl100k_base'] = \"gpt2\"\n",
    "\n",
    "    batch_size: int = Field(default=64, description=\"Batch size\")\n",
    "    block_size: int = Field(default=128, description=\"Block size, an alias for max length/context window size.\", alias=\"context_length\")\n",
    "    device_type: Literal[\"cpu\", \"cuda\"] = \"cpu\"\n",
    "    device: torch.device = Field(None, description=\"Device to use\")\n",
    "\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def set_train_valid_paths(self) -> Composer:\n",
    "        self.train_path = Path(self.data_folder) / \"train.txt\"\n",
    "        self.valid_path = Path(self.data_folder) / \"valid.txt\"\n",
    "        return self\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def set_device(self) -> Composer:\n",
    "        self.device = torch.device(self.device_type)\n",
    "        return self\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def set_debug_fields(self) -> Composer:\n",
    "        if self.debug:\n",
    "            self.batch_size = 2\n",
    "            self.block_size = 8\n",
    "        return self\n",
    "\n",
    "    class Config:\n",
    "        extra = \"forbid\"\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "composer = Composer(debug=True)\n",
    "pprint(composer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_deterministic_mode() -> None:\n",
    "    \"\"\"\n",
    "    See https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html\n",
    "    and https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility\n",
    "    \"\"\"\n",
    "    # fmt: off\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    torch.backends.cudnn.benchmark        = False\n",
    "    torch.backends.cudnn.deterministic    = True\n",
    "    torch.backends.cudnn.enabled          = False\n",
    "\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "    # fmt: on\n",
    "    warnings.warn(\n",
    "        \"Deterministic mode is activated. This will negatively impact performance and may cause increase in CUDA memory footprint.\",\n",
    "        category=UserWarning,\n",
    "        stacklevel=2,\n",
    "    )\n",
    "\n",
    "\n",
    "def seed_all(\n",
    "    seed: int = 1992,\n",
    "    seed_torch: bool = True,\n",
    "    set_torch_deterministic: bool = True,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Seed all random number generators.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seed : int\n",
    "        Seed number to be used, by default 1992.\n",
    "    seed_torch : bool\n",
    "        Whether to seed PyTorch or not, by default True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    seed: int\n",
    "        The seed number.\n",
    "    \"\"\"\n",
    "    # fmt: off\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)       # set PYTHONHASHSEED env var at fixed value\n",
    "    np.random.default_rng(seed)                    # numpy pseudo-random generator\n",
    "    random.seed(seed)                              # python's built-in pseudo-random generator\n",
    "\n",
    "    if seed_torch:\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)           # pytorch (both CPU and CUDA)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.enabled = False\n",
    "\n",
    "        if set_torch_deterministic:\n",
    "            configure_deterministic_mode()\n",
    "    # fmt: on\n",
    "    return seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2024"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_all(composer.seed, seed_torch=True, set_torch_deterministic=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Vocabulary\n",
    "\n",
    "- https://huggingface.co/learn/nlp-course/chapter2/4?fw=pt\n",
    "- https://github.com/openai/tiktoken\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "\n",
    "Language models don't see text like you and I, instead they see a sequence of numbers (known as tokens). Byte pair encoding (BPE) is a way of converting text into tokens. It has a couple desirable properties[^1]:\n",
    "\n",
    "- It's reversible and lossless, so you can convert tokens back into the original text\n",
    "- It works on arbitrary text, even text that is not in the tokeniser's training data\n",
    "- It compresses the text: the token sequence is shorter than the bytes corresponding to the original text. On average, in practice, each token corresponds to about 4 bytes.\n",
    "- It attempts to let the model see common subwords. For instance, \"ing\" is a common subword in English, so BPE encodings will often split \"encoding\" into tokens like \"encod\" and \"ing\" (instead of e.g. \"enc\" and \"oding\"). Because the model will then see the \"ing\" token again and again in different contexts, it helps models generalise and better understand grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def am_i_in_jupyter() -> bool:\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        if \"IPKernelApp\" not in get_ipython().config:\n",
    "            return False\n",
    "    except ImportError:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "IN_JUPYTER = am_i_in_jupyter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url: str, dataset_name: str, dest_folder: Path | str) -> Path:\n",
    "    dest_folder_path = Path(dest_folder)\n",
    "\n",
    "    dest_folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    filepath = dest_folder_path / f\"{dataset_name}.txt\"\n",
    "\n",
    "    response = requests.get(url, stream=True)\n",
    "    corpus = response.text\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "    return filepath, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PosixPath</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'data/tinyshakespeare/tinyshakespeare.txt'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'data/tinyshakespeare/tinyshakespeare.txt'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "filepath, corpus = download(composer.url, composer.dataset_name, composer.data_folder)\n",
    "pprint(filepath)\n",
    "print(corpus[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt2', 'r50k_base', 'p50k_base', 'p50k_edit', 'cl100k_base']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(corpus)\n",
    "train_data = corpus[: int(N * 0.9)]\n",
    "valid_data = corpus[int(N * 0.9) :]\n",
    "\n",
    "# encode with tiktoken gpt2 bpe\n",
    "tokenizer = tiktoken.get_encoding(composer.encoding_name)\n",
    "tiktoken.list_encoding_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 301,966 tokens\n",
      "val has 36,059 tokens\n"
     ]
    }
   ],
   "source": [
    "train_ids = tokenizer.encode_ordinary(train_data)\n",
    "valid_ids = tokenizer.encode_ordinary(valid_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(valid_ids):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen\n",
      "--------------------------------------------------------------------------------\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(train_ids[:2]))\n",
    "print(\"-\" * 80)\n",
    "print(tokenizer.decode(train_ids[:100]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to bin files\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "valid_ids = np.array(valid_ids, dtype=np.uint16)\n",
    "\n",
    "train_ids.tofile(composer.train_path)\n",
    "valid_ids.tofile(composer.valid_path)\n",
    "\n",
    "# train.bin has 301,966 tokens\n",
    "# val.bin has 36,059 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloading (Poor Man's Dataloader)\n",
    "\n",
    "**TODO: to bridge the gap between the previous section and this one, we need to\n",
    "explain why we need to construct a dataset.**\n",
    "\n",
    "---\n",
    "\n",
    "As Karpathy puts it, he implemented a poor man's\n",
    "[dataloader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n",
    "We will start by dissecting the code and understanding how it works and finally,\n",
    "show that everything can be done with PyTorch's `Dataset` and `Dataloader`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Mapping\n",
    "\n",
    "Firstly, Karpathy uses `numpy`'s\n",
    "[memory mapping](https://numpy.org/doc/stable/reference/generated/numpy.memmap.html)\n",
    "(`numpy.memmap`) to load the data. Memory mapping is used to create a\n",
    "memory-mapped array from a binary file. This involves mapping the contents of a\n",
    "file directly into the virtual memory space of the calling process. This allows\n",
    "applications to access the file data as if it were loaded in memory, using\n",
    "pointer operations or array indexing, without the need for explicit read or\n",
    "write operations.\n",
    "\n",
    "This essentially means that you can access small segments of a large file\n",
    "without having to load the entire file into memory. The concept draws\n",
    "similarities to the use of [generators](https://wiki.python.org/moin/Generators)\n",
    "in Python, where you can iterate over a large dataset without having to load the\n",
    "entire dataset into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_dtype: uint16, data_shape: (301966,)\n"
     ]
    }
   ],
   "source": [
    "train_data = np.memmap(composer.train_path, dtype=np.uint16, mode=\"r\")\n",
    "train_data_dtype = train_data.dtype\n",
    "train_data_shape = train_data.shape\n",
    "\n",
    "print(f\"data_dtype: {train_data_dtype}, data_shape: {train_data_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the shape of train data is `(301966,)`, which means that it is a 1D (flattened) array \n",
    "with $301966$ elements - this is basically the length of the entire train corpus, in terms of\n",
    "tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation, Context Length, Shuffling and Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we are not going to pass the entire training corpus as is to the model.\n",
    "Instead, we are going to pass a **batch** of sequences (each sequence of length\n",
    "`context_length`) to the model at a time.\n",
    "\n",
    "#### Notation\n",
    "\n",
    "Let's consider a sequence $\\mathbf{x} = (x_1, x_2, \\ldots, x_T)$, where:\n",
    "\n",
    "-   $x_t$ represents the $t$-th token in the sequence,\n",
    "-   Each token $x_t$ is an element of a predefined vocabulary $\\mathcal{V} := \\mathcal{X}$,\n",
    "-   $T$ denotes the total number of tokens in the sequence, i.e., the sequence\n",
    "    length.\n",
    "\n",
    "In practice, we handle multiple sequences at once by grouping them into a batch.\n",
    "This batch, denoted as $\\mathcal{B}$, is then presented to the model for\n",
    "parallel processing.\n",
    "\n",
    "A batch of sequences is represented as a matrix $\\mathbf{X}$, where each row\n",
    "corresponds to a sequence in the batch. If the batch size is $\\mathcal{B}$ and\n",
    "each sequence within the batch has a fixed length $T$, then $\\mathbf{X}$ can be\n",
    "expressed as:\n",
    "\n",
    "$$\n",
    "    \\mathbf{X} = \\begin{bmatrix} x_{1,1} & x_{1,2} & \\ldots & x_{1,T} \\\\ x_{2,1}\n",
    "        & x_{2,2} & \\ldots & x_{2,T} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{\\mathcal{B},1} &\n",
    "        x_{\\mathcal{B},2} & \\ldots & x_{\\mathcal{B},T} \\\\\\end{bmatrix} \\in \\mathbb{Z}^{\\mathcal{B} \\times T}\n",
    "$$\n",
    "\n",
    "Here, $x_{i,j}$ denotes the $j$-th token in the $i$-th sequence of the batch.\n",
    "It's important to note that while we represent the sequences in a real-valued\n",
    "space $\\mathbb{Z}^{\\mathcal{B} \\times T}$ for mathematical convenience, in\n",
    "practice, each $x_{i,j}$ corresponds to a discrete token from the vocabulary\n",
    "$\\mathcal{X}$ so using $\\mathbb{Z}^{+}$ would be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context Length / Block Size\n",
    "\n",
    "$T$ is often referred to as the sequence length, or in the context of GPT, it is\n",
    "the `block_size` or `context_length` or `max_seq_len`.\n",
    "\n",
    "It is the length of the sequence that the model will be trained on and is also\n",
    "the context length/context window that we often hear about.\n",
    "\n",
    "For example,\n",
    "[Gemini 1.5](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024)\n",
    "was announced to have a standard $128,000$ token context window, up to a maximum\n",
    "of $1$ million max length.\n",
    "\n",
    "Typically, I think that if your model is trained on a certain context length, it\n",
    "is not trivial to change it. For example, if you train a model on a context\n",
    "length of $128$, you cannot simply change it to $256$ without retraining the\n",
    "model. But it seems that it is increasingly possible to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example, if we define our $L$ to be $32$, then we would expect each\n",
    "sequence to be of length $32$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">memmap</span><span style=\"font-weight: bold\">([</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5962</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22307</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8421</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">356</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5120</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">597</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2252</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │      </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3285</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">502</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2740</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3237</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5248</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">461</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2740</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5962</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22307</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1639</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">389</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">uint16</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mmemmap\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m5962\u001b[0m, \u001b[1;36m22307\u001b[0m,    \u001b[1;36m25\u001b[0m,   \u001b[1;36m198\u001b[0m,  \u001b[1;36m8421\u001b[0m,   \u001b[1;36m356\u001b[0m,  \u001b[1;36m5120\u001b[0m,   \u001b[1;36m597\u001b[0m,  \u001b[1;36m2252\u001b[0m,\n",
       "\u001b[2;32m│   │      \u001b[0m\u001b[1;36m11\u001b[0m,  \u001b[1;36m3285\u001b[0m,   \u001b[1;36m502\u001b[0m,  \u001b[1;36m2740\u001b[0m,    \u001b[1;36m13\u001b[0m,   \u001b[1;36m198\u001b[0m,   \u001b[1;36m198\u001b[0m,  \u001b[1;36m3237\u001b[0m,    \u001b[1;36m25\u001b[0m,\n",
       "\u001b[2;32m│   │     \u001b[0m\u001b[1;36m198\u001b[0m,  \u001b[1;36m5248\u001b[0m,   \u001b[1;36m461\u001b[0m,    \u001b[1;36m11\u001b[0m,  \u001b[1;36m2740\u001b[0m,    \u001b[1;36m13\u001b[0m,   \u001b[1;36m198\u001b[0m,   \u001b[1;36m198\u001b[0m,  \u001b[1;36m5962\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1;36m22307\u001b[0m,    \u001b[1;36m25\u001b[0m,   \u001b[1;36m198\u001b[0m,  \u001b[1;36m1639\u001b[0m,   \u001b[1;36m389\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35muint16\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are\n"
     ]
    }
   ],
   "source": [
    "first_sequence = train_data[0:0+32]\n",
    "pprint(first_sequence)\n",
    "\n",
    "first_sequence_decoded = tokenizer.decode(first_sequence)\n",
    "print(first_sequence_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example is just extracting $1$ such sequence $\\mathbf{x}$ from the train\n",
    "corpus. To leverage the prowess of linear algebra operations in CUDA, we would\n",
    "typically pass a batch of sequences $\\mathcal{B}$ to the model at a time.\n",
    "\n",
    "Furthermore, we would require some level of randomness in the sequences that we\n",
    "pass to the model to enable generalisation. You really do not want the model to\n",
    "overfit to an ordered sequence of tokens in the training corpus.\n",
    "\n",
    "To this end, let's see how Karpathy implements batching and shuffling of the\n",
    "sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffling and Discrete Uniform Sampling\n",
    "\n",
    "To enable shuffling, Karpathy generates a tensor of random integers (essentially a list of\n",
    "random integers), which serve as indices. These indices are used to select\n",
    "random sequences from the training (and validation) data.\n",
    "\n",
    "For simplicity, let's look at the case where batch size is reduced to $\\mathcal{B} = 1$.\n",
    "This means we only need to sample $1$ sequence from the training data - and consequently\n",
    "we only need $1$ random index.\n",
    "\n",
    "We can easily achieve this via `torch.randint` which generates random integers\n",
    "from a discrete uniform distribution over the half-open interval $[l, h)$,\n",
    "and since we only want to sample $1$ sequence, we set `size=(1,)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">136016</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m136016\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = torch.Generator(device=composer.device)\n",
    "low, high = 0, len(train_data) - composer.block_size\n",
    "size = (1,)\n",
    "indices: torch.Tensor = torch.randint(low=low, high=high, size=size, generator=generator)\n",
    "pprint(indices)\n",
    "pprint(indices.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mathematical operation performed by `torch.randint(low, high, size, generator)` can be described as drawing samples from a uniform discrete distribution. Each element of the resulting tensor is an independent and identically distributed {cite}`radford2019language` (i.i.d.) random variable $X_i$ with the following probability mass function (PMF):\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(X_i = k) = \\frac{1}{h - l} \\quad \\text{for} \\, k = l, \\ldots, h-1 \n",
    "$$\n",
    "\n",
    "This PMF implies that each integer in the range $[l, h-1]$ has an equal probability of being selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our demonstration, we selected a random index, specifically $136,016$, from\n",
    "our training dataset. This serves as a starting point for constructing a\n",
    "sequence, denoted as $\\mathbf{x}$. This sequence consists of the token found at\n",
    "the chosen index and extends to include the subsequent $T$ tokens, where $T$\n",
    "represents the block size. For the sake of simplicity, and to align with our\n",
    "predefined settings, we have chosen $T = 8$. This block size is predetermined in\n",
    "our `composer` configuration, activated specifically under a `debug` mode.\n",
    "\n",
    "In code, we can achieve this by slicing the training data from the random index\n",
    "to the random index plus the block size. This is done by `train_data[random_index:random_index+block_size]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">memmap</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3194</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">612</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">290</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">284</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">606</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">910</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">uint16</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mmemmap\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m3194\u001b[0m,  \u001b[1;36m612\u001b[0m,   \u001b[1;36m11\u001b[0m,  \u001b[1;36m290\u001b[0m,  \u001b[1;36m284\u001b[0m,  \u001b[1;36m606\u001b[0m,  \u001b[1;36m910\u001b[0m,   \u001b[1;36m11\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35muint16\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m8\u001b[0m,\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "' written there, and to them say,'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_sequence = train_data[indices:indices+composer.block_size]\n",
    "pprint(random_sequence)\n",
    "pprint(random_sequence.shape)\n",
    "\n",
    "random_sequence_decoded = tokenizer.decode(random_sequence)\n",
    "tokenizer.decode(random_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One might wonder why the highest value of the random integers is\n",
    "`len(self.train_data) - self.block_size`. This is mostly to prevent index out of\n",
    "range errors. As we shall soon see, we are using these `indices` to slice a\n",
    "sequence of length `block_size` from the data where you start slicing from the\n",
    "index `index` and end at `index + block_size`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching\n",
    "\n",
    "Now that we understand how to sample a single sequence from the training data,\n",
    "let's look at how we can sample a batch of sequences.\n",
    "PyTorch made it easy for you, as we can just simply change the `size` parameter\n",
    "to `(batch_size,)` so we can sample $\\mathcal{B}$ number of indices - and\n",
    "consequently $\\mathcal{B}$ number of sequences.\n",
    "\n",
    "In our case, if we set $\\mathcal{B} = 2$, we would expect to get $2$ random\n",
    "indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">136016</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">197976</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m136016\u001b[0m, \u001b[1;36m197976\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = torch.Generator(device=composer.device)\n",
    "low, high = 0, len(train_data) - composer.block_size\n",
    "size = (composer.batch_size,)\n",
    "indices: torch.Tensor = torch.randint(low=low, high=high, size=size, generator=generator)\n",
    "pprint(indices)\n",
    "pprint(indices.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then construct a batch of \n",
    "input sequences $\\mathcal{B}$ by selecting the tokens at the\n",
    "indices $136,016$ and $197,976$ and the next $T$ tokens via a for loop - and using `torch.stack`\n",
    "to stack the sequences into a tensor of shape $\\mathbb{Z}^{\\mathcal{B} \\times L}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3194</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">612</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">290</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">284</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">606</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">910</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4713</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5127</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11486</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">304</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">260</span><span style=\"font-weight: bold\">]])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m3194\u001b[0m,   \u001b[1;36m612\u001b[0m,    \u001b[1;36m11\u001b[0m,   \u001b[1;36m290\u001b[0m,   \u001b[1;36m284\u001b[0m,   \u001b[1;36m606\u001b[0m,   \u001b[1;36m910\u001b[0m,    \u001b[1;36m11\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m4713\u001b[0m,  \u001b[1;36m5127\u001b[0m,    \u001b[1;36m13\u001b[0m,   \u001b[1;36m198\u001b[0m, \u001b[1;36m11486\u001b[0m,    \u001b[1;36m11\u001b[0m,   \u001b[1;36m304\u001b[0m,   \u001b[1;36m260\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m8\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = torch.stack([torch.from_numpy((train_data[index : index + composer.block_size]).astype(np.int64)) for index in indices])\n",
    "pprint(x)\n",
    "pprint(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth reconciling the fact that the slicing uses `[index:index + block_size]` and\n",
    "therefore completes the reasoning behind the `len(self.train_data) - self.block_size` in\n",
    "the `torch.randint` function call - to prevent index out of range errors. Consider\n",
    "that if we do not subtract `block_size` from the length of the training data, we might\n",
    "end up with an index that is the last index of the training data, and when we add\n",
    "`block_size` to it, we would end up with an index that is out of range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction of Input and Target Sequences\n",
    "\n",
    "As we will define more formally later, GPT model is an autoregressive\n",
    "self-supervised learning model{cite}`math11112451` that directly learns a\n",
    "conditional probability distribution $\\mathbb{P}(x_t | x_{<t} ; \\Theta)$ over\n",
    "the vocabulary $\\mathcal{V}$ of tokens, which is conditioned on the entire\n",
    "history of tokens $x_{<t} = (x_1, x_2, \\ldots, x_{t-1})$.\n",
    "\n",
    "We have seen earlier how to construct an input sequence $\\mathbf{x}$ from the\n",
    "training data. To put things into perspective, we consider again the first\n",
    "sequence that we constructed from the training data:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\begin{bmatrix} 3194 & 612 & 11 & 290 & 284 & 606 & 910 & 11 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "representing the sentence `' written there, and to them say,'`.\n",
    "\n",
    "Given the autoregressive and self-supervised nature, in order to construct the\n",
    "target sequence $\\mathbf{y}$, we simply shift the input sequence by one token to\n",
    "the left. This means that the target sequence $\\mathbf{y}$ is:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\begin{bmatrix} 612 & 11 & 290 & 284 & 606 & 910 & 11 & 198 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "representing the sentence `' there, and to them say,\\n'`.\n",
    "\n",
    "This behaviour is autoregressive because we are using the context tokens\n",
    "$x_{<t}$ to predict the next token $x_t$, and self-supervised because we are\n",
    "using the input sequence $\\mathbf{x}$ to construct the target sequence\n",
    "$\\mathbf{y}$ without any external labels.\n",
    "\n",
    "To illustrate further, the prediction process during training is cumulative:\n",
    "\n",
    "-   For predicting $x_2$, the model uses $x_1$ as context:\n",
    "    $\\mathbb{P}\\left(x_2 \\mid x_1\\right)$.\n",
    "-   For predicting $x_3$, the model uses both $x_1$ and $x_2$ as context:\n",
    "    $\\mathbb{P}\\left(x_3 \\mid x_1, x_2\\right)$.\n",
    "-   This pattern continues, such that for predicting $x_t$, the model uses\n",
    "    $x_1, x_2, \\ldots, x_{t-1}$ as context:\n",
    "    $\\mathbb{P}\\left(x_t \\mid x_1, x_2, \\ldots, x_{t-1}\\right)$\n",
    "\n",
    "In code, we can achieve this by simply slicing the adding a `1` to the `index`\n",
    "in the `train_data` slicing operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">612</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">290</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">284</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">606</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">910</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5127</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11486</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">304</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">260</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14210</span><span style=\"font-weight: bold\">]])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m  \u001b[1;36m612\u001b[0m,    \u001b[1;36m11\u001b[0m,   \u001b[1;36m290\u001b[0m,   \u001b[1;36m284\u001b[0m,   \u001b[1;36m606\u001b[0m,   \u001b[1;36m910\u001b[0m,    \u001b[1;36m11\u001b[0m,   \u001b[1;36m198\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m5127\u001b[0m,    \u001b[1;36m13\u001b[0m,   \u001b[1;36m198\u001b[0m, \u001b[1;36m11486\u001b[0m,    \u001b[1;36m11\u001b[0m,   \u001b[1;36m304\u001b[0m,   \u001b[1;36m260\u001b[0m, \u001b[1;36m14210\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m8\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "' there, and to them say,\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.stack([torch.from_numpy((train_data[index + 1: index + 1 + composer.block_size]).astype(np.int64)) for index in indices])\n",
    "pprint(y)\n",
    "pprint(y.shape)\n",
    "\n",
    "tokenizer.decode(y[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous Data Loading and Prefetching\n",
    "\n",
    "As we approach the last part of the code, Karpathy moves `x` and `y` to the\n",
    "device and returns them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "if composer.device_type == \"cuda\":\n",
    "    # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "    x, y = x.pin_memory().to(composer.device, non_blocking=True), y.pin_memory().to(composer.device, non_blocking=True)\n",
    "else:\n",
    "    x, y = x.to(composer.device), y.to(composer.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a common operation in PyTorch, where we move the data to the underlying\n",
    "device (CPU or GPU or MPS) to leverage the processing capabilities of the\n",
    "device. It goes without saying that modern deep learning models are trained on\n",
    "GPUs - and CUDA is the de facto standard for GPU-accelerated computing.\n",
    "\n",
    "CUDA allows a `pin_memory` and `non_blocking` parameter to be set when transferring\n",
    "tensor data from CPU to GPU. The `pin_memory` parameter is used to allow `.to(\"cuda\")`\n",
    "to be more [performant](https://devblogs.nvidia.com/how-optimize-data-transfers-cuda-cc/)\n",
    "as it avoids some implicit CPU-to-CPU copies. Tensors which are pinned in memory\n",
    "also allow the transfer from CPU to GPU to be done asynchronously via `non_blocking` with respect to\n",
    "the host[^2].\n",
    "\n",
    "It can be useful because we can do some other work in CPU while the data is being\n",
    "transferred to GPU. Consider the below scenario:\n",
    "\n",
    "- `tensor.pin_memory().to(\"cuda\", non_blocking=True)` will transfer the tensor\n",
    "  to the GPU asynchronously, and the CPU can continue doing some other work.\n",
    "- While waiting, CPU can do some other operations without waiting for the\n",
    "  transfer to complete,\n",
    "- Once `tensor` is transferred to the GPU, then we can do some other operations\n",
    "  on the GPU.\n",
    "\n",
    "What is worth noting is that CUDA manages the synchronization such that\n",
    "operations on the GPU will not start until the transfer is complete. However, CUDA\n",
    "programming is complex and is out of the scope of this post. Interested readers\n",
    "can see the reference section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collating Everything Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3194</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">612</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">290</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">284</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">606</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">910</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4713</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5127</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11486</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">304</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">260</span><span style=\"font-weight: bold\">]])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m3194\u001b[0m,   \u001b[1;36m612\u001b[0m,    \u001b[1;36m11\u001b[0m,   \u001b[1;36m290\u001b[0m,   \u001b[1;36m284\u001b[0m,   \u001b[1;36m606\u001b[0m,   \u001b[1;36m910\u001b[0m,    \u001b[1;36m11\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m4713\u001b[0m,  \u001b[1;36m5127\u001b[0m,    \u001b[1;36m13\u001b[0m,   \u001b[1;36m198\u001b[0m, \u001b[1;36m11486\u001b[0m,    \u001b[1;36m11\u001b[0m,   \u001b[1;36m304\u001b[0m,   \u001b[1;36m260\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">612</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">290</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">284</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">606</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">910</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5127</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11486</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">304</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">260</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14210</span><span style=\"font-weight: bold\">]])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m  \u001b[1;36m612\u001b[0m,    \u001b[1;36m11\u001b[0m,   \u001b[1;36m290\u001b[0m,   \u001b[1;36m284\u001b[0m,   \u001b[1;36m606\u001b[0m,   \u001b[1;36m910\u001b[0m,    \u001b[1;36m11\u001b[0m,   \u001b[1;36m198\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m5127\u001b[0m,    \u001b[1;36m13\u001b[0m,   \u001b[1;36m198\u001b[0m, \u001b[1;36m11486\u001b[0m,    \u001b[1;36m11\u001b[0m,   \u001b[1;36m304\u001b[0m,   \u001b[1;36m260\u001b[0m, \u001b[1;36m14210\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_batch(\n",
    "    composer: Composer,\n",
    "    *,\n",
    "    split: Literal[\"train\", \"valid\"],\n",
    "    batch_size: int,\n",
    "    block_size: int,\n",
    "    generator: torch.Generator,\n",
    "    device: torch.device,\n",
    "    device_type: Literal[\"cpu\", \"cuda\"] = \"cpu\",\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
    "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
    "    if split == \"train\":\n",
    "        data = np.memmap(composer.train_path, dtype=np.uint16, mode=\"r\")\n",
    "    else:\n",
    "        data = np.memmap(composer.valid_path, dtype=np.uint16, mode=\"r\")\n",
    "\n",
    "    low, high = 0, len(data) - block_size\n",
    "    size = (batch_size,)\n",
    "\n",
    "    indices = torch.randint(low=low, high=high, size=size, generator=generator)\n",
    "\n",
    "    x = torch.stack([torch.from_numpy((data[index : index + block_size]).astype(np.int64)) for index in indices])\n",
    "    y = torch.stack(\n",
    "        [torch.from_numpy((data[index + 1 : index + 1 + block_size]).astype(np.int64)) for index in indices]\n",
    "    )\n",
    "    if device_type == \"cuda\":\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "generator = torch.Generator(device=composer.device)\n",
    "train_batch = get_batch(composer, split=\"train\", batch_size=composer.batch_size, block_size=composer.block_size, device=composer.device, generator=generator)\n",
    "x, y = train_batch\n",
    "pprint(x)\n",
    "pprint(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PyTorch's Dataset and Dataloader\n",
    "\n",
    "It is relatively simple to understand - and since there is not a need to\n",
    "[collate](https://pytorch.org/docs/stable/data.html#dataloader-collate-fn) the\n",
    "data, which makes things a bit easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Conditional and Joint probability\n",
    "2. Model definition\n",
    "3. Loss see page 5/6\n",
    "4. autoregressive self supervised def page 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Probability\n",
    "\n",
    "The GPT model, being autoregressive, directly models the **conditional\n",
    "probability** of each token given all previous tokens in a sequence. This is the\n",
    "core operational mechanism of the model, allowing it to predict the next token\n",
    "based on the preceding context. The conditional probability for each token $x_t$\n",
    "given its predecessors $x_{<t}$ is expressed as $P(x_t | x_{<t};\n",
    "\\Theta)$, where\n",
    "$\\Theta$ represents the model's parameters.\n",
    "\n",
    "### Joint Probability\n",
    "\n",
    "The **joint probability** of a sequence $\\mathbf{x} = (x_1, x_2, ..., x_T)$\n",
    "under the model is derived from the product of these conditional probabilities.\n",
    "By assuming that the probability of each token can be modeled based on its\n",
    "predecessors, the model implicitly defines a joint probability distribution over\n",
    "sequences. This joint probability is represented by the product:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{x}; \\Theta) = \\prod_{t=1}^{T} P(x_t | x_{<t}; \\Theta)\n",
    "$$\n",
    "\n",
    "This formulation shows that while the GPT model operates through conditional\n",
    "probabilities, the product of these conditional probabilities across a sequence\n",
    "results in the **joint probability** of the entire sequence. Thus, the GPT model\n",
    "effectively learns the joint probability distribution over sequences in the\n",
    "dataset by learning to predict the next token given its context, leveraging the\n",
    "chain rule of probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{cite}`math11112451`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://devblogs.nvidia.com/how-optimize-data-transfers-cuda-cc/\n",
    "- https://developer.nvidia.com/blog/how-overlap-data-transfers-cuda-cc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^1]: [OpenAI tiktoken](https://github.com/openai/tiktoken)\n",
    "[^2]: [How to Optimize Data Transfers in CUDA C/C++](https://devblogs.nvidia.com/how-optimize-data-transfers-cuda-cc/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omniverse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
