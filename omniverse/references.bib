-------------------- Machine Learning and Deep Learning --------------------
@book{zhang2023dive,
    title     = {Dive into Deep Learning},
    author    = {Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
    publisher = {Cambridge University Press},
    url       = {https://D2L.ai},
    year      = {2023}
}

@book{jung2022machine,
    title     = {Machine Learning: The Basics},
    author    = {Jung, A.},
    publisher = {Springer},
    address   = {Singapore},
    year      = {2022}
}

@book{deisenroth2020mathematics,
    title     = {Mathematics for Machine Learning},
    author    = {Deisenroth, Marc Peter and Faisal, A. Aldo and Ong, Cheng Soon},
    publisher = {Cambridge University Press},
    year      = {2020},
    url       = {https://mml-book.github.io/}
}

-------------------- Data Structures and Algorithms --------------------
@misc{pythonds3,
    author    = {{Runestone Interactive}},
    title     = {Problem Solving with Algorithms and Data Structures using Python},
    year      = {2023},
    url       = {https://runestone.academy/ns/books/published/pythonds3/index.html}
}

@book{cormen_2022_introduction_to_algorithms,
    title     ={Introduction to Algorithms},
    author    ={Cormen, Thomas H. and Leiserson, Charles E. and Rivest, Ronald L. and Stein, Clifford},
    publisher ={MIT Press},
    year      ={2022},
    edition   ={4}
}

-------------------- Linear Algebra --------------------

@book{cohen2021linear,
    title     = {Linear Algebra: Theory, Intuition, Code},
    author    = {Cohen, M.X.},
    isbn      = {9789083136608},
    url       = {https://books.google.com.sg/books?id=824xzgEACAAJ},
    year      = {2021},
    publisher = {sincXpress},
}

@book{axler1997linear,
    title     = {Linear Algebra Done Right},
    author    = {Axler, S.},
    isbn      = {9780387982595},
    lccn      = {95044889},
    series    = {Undergraduate Texts in Mathematics},
    url       = {https://books.google.com.sg/books?id=ovIYVIlithQC},
    year      = {1997},
    publisher = {Springer New York},
}

-------------------- Real, Complex and Functional Analysis --------------------

@book{muscat2014functional,
    title     = {Functional Analysis: An Introduction to Metric Spaces, Hilbert Spaces, and Banach Algebras},
    author    = {Muscat, Joseph},
    publisher = {Springer},
    year      = {2014}
}

-------------------- Probability and Statistics --------------------


-------------------- Papers --------------------

@misc{radford2019language,
    title   = {Language Models are Unsupervised Multitask Learners},
    author  = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
    year    = {2019}
}

@article{math11112451,
    author          = {Lee, Minhyeok},
    title           = {A Mathematical Interpretation of Autoregressive Generative Pre-Trained Transformer and Self-Supervised Learning},
    journal         = {Mathematics},
    volume          = {11},
    year            = {2023},
    number          = {11},
    article-number  = {2451},
    url             = {https://www.mdpi.com/2227-7390/11/11/2451},
    issn            = {2227-7390},
    abstract        = {In this paper, we present a rigorous mathematical examination of generative pre-trained transformer (GPT) models and their autoregressive self-supervised learning mechanisms. We begin by defining natural language space and knowledge space, which are two key concepts for understanding the dimensionality reduction process in GPT-based large language models (LLMs). By exploring projection functions and their inverses, we establish a framework for analyzing the language generation capabilities of these models. We then investigate the GPT representation space, examining its implications for the modelsâ€™ approximation properties. Finally, we discuss the limitations and challenges of GPT models and their learning mechanisms, considering trade-offs between complexity and generalization, as well as the implications of incomplete inverse projection functions. Our findings demonstrate that GPT models possess the capability to encode knowledge into low-dimensional vectors through their autoregressive self-supervised learning mechanism. This comprehensive analysis provides a solid mathematical foundation for future advancements in GPT-based LLMs, promising advancements in natural language processing tasks such as language translation, text summarization, and question answering due to improved understanding and optimization of model training and performance.},
    doi             = {10.3390/math11112451}
}

@misc{vaswani2023attention,
    title           = {Attention Is All You Need},
    author          = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
    year            = {2023},
    eprint          = {1706.03762},
    archivePrefix   = {arXiv},
    primaryClass    = {cs.CL}
}