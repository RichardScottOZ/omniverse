-------------------- Machine Learning and Deep Learning --------------------
@book{zhang2023dive,
    title     = {Dive into Deep Learning},
    author    = {Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
    publisher = {Cambridge University Press},
    url       = {https://D2L.ai},
    year      = {2023}
}

@book{jung2022machine,
    title     = {Machine Learning: The Basics},
    author    = {Jung, A.},
    publisher = {Springer},
    address   = {Singapore},
    year      = {2022}
}

@book{deisenroth2020mathematics,
    title     = {Mathematics for Machine Learning},
    author    = {Deisenroth, Marc Peter and Faisal, A. Aldo and Ong, Cheng Soon},
    publisher = {Cambridge University Press},
    year      = {2020},
    url       = {https://mml-book.github.io/}
}

-------------------- Data Structures and Algorithms --------------------
@misc{pythonds3,
    author    = {{Runestone Interactive}},
    title     = {Problem Solving with Algorithms and Data Structures using Python},
    year      = {2023},
    url       = {https://runestone.academy/ns/books/published/pythonds3/index.html}
}

@book{cormen_2022_introduction_to_algorithms,
    title     ={Introduction to Algorithms},
    author    ={Cormen, Thomas H. and Leiserson, Charles E. and Rivest, Ronald L. and Stein, Clifford},
    publisher ={MIT Press},
    year      ={2022},
    edition   ={4}
}

-------------------- Linear Algebra --------------------

@book{cohen2021linear,
    title     = {Linear Algebra: Theory, Intuition, Code},
    author    = {Cohen, M.X.},
    isbn      = {9789083136608},
    url       = {https://books.google.com.sg/books?id=824xzgEACAAJ},
    year      = {2021},
    publisher = {sincXpress},
}

@book{axler1997linear,
    title     = {Linear Algebra Done Right},
    author    = {Axler, S.},
    isbn      = {9780387982595},
    lccn      = {95044889},
    series    = {Undergraduate Texts in Mathematics},
    url       = {https://books.google.com.sg/books?id=ovIYVIlithQC},
    year      = {1997},
    publisher = {Springer New York},
}

-------------------- Real, Complex and Functional Analysis --------------------

@book{muscat2014functional,
    title     = {Functional Analysis: An Introduction to Metric Spaces, Hilbert Spaces, and Banach Algebras},
    author    = {Muscat, Joseph},
    publisher = {Springer},
    year      = {2014}
}

-------------------- Probability and Statistics --------------------


-------------------- Papers --------------------

@misc{radford2019language,
    title   = {Language Models are Unsupervised Multitask Learners},
    author  = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
    year    = {2019}
}

@article{math11112451,
    author          = {Lee, Minhyeok},
    title           = {A Mathematical Interpretation of Autoregressive Generative Pre-Trained Transformer and Self-Supervised Learning},
    journal         = {Mathematics},
    volume          = {11},
    year            = {2023},
    number          = {11},
    article-number  = {2451},
    url             = {https://www.mdpi.com/2227-7390/11/11/2451},
    issn            = {2227-7390},
    abstract        = {In this paper, we present a rigorous mathematical examination of generative pre-trained transformer (GPT) models and their autoregressive self-supervised learning mechanisms. We begin by defining natural language space and knowledge space, which are two key concepts for understanding the dimensionality reduction process in GPT-based large language models (LLMs). By exploring projection functions and their inverses, we establish a framework for analyzing the language generation capabilities of these models. We then investigate the GPT representation space, examining its implications for the modelsâ€™ approximation properties. Finally, we discuss the limitations and challenges of GPT models and their learning mechanisms, considering trade-offs between complexity and generalization, as well as the implications of incomplete inverse projection functions. Our findings demonstrate that GPT models possess the capability to encode knowledge into low-dimensional vectors through their autoregressive self-supervised learning mechanism. This comprehensive analysis provides a solid mathematical foundation for future advancements in GPT-based LLMs, promising advancements in natural language processing tasks such as language translation, text summarization, and question answering due to improved understanding and optimization of model training and performance.},
    doi             = {10.3390/math11112451}
}

@misc{vaswani2017attention,
    title           = {Attention Is All You Need},
    author          = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
    year            = {2017},
    eprint          = {1706.03762},
    archivePrefix   = {arXiv},
    primaryClass    = {cs.CL}
}

@misc{bahdanau2014neural,
      title={Neural Machine Translation by Jointly Learning to Align and Translate},
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2014},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{lippe2023uvadlc,
   title        = {{UvA Deep Learning Tutorials}},
   author       = {Phillip Lippe},
   year         = 2023,
   howpublished = {\url{https://uvadlc-notebooks.readthedocs.io/en/latest/}}
}

@misc{radford2018improving,
  added-at = {2020-07-14T16:49:49.000+0200},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  biburl = {https://www.bibsonomy.org/bibtex/273ced32c0d4588eb95b6986dc2c8147c/jonaskaiser},
  interhash = {5c343ed9a31ac52fd17a898f72af228f},
  intrahash = {73ced32c0d4588eb95b6986dc2c8147c},
  keywords = {},
  timestamp = {2020-07-14T16:49:49.000+0200},
  title = {Improving language understanding by generative pre-training},
  year = 2018
}

@misc{finn2017modelagnostic,
      title={Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
      author={Chelsea Finn and Pieter Abbeel and Sergey Levine},
      year={2017},
      eprint={1703.03400},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{kaiser2017model,
      title={One Model To Learn Them All},
      author={Lukasz Kaiser and Aidan N. Gomez and Noam Shazeer and Ashish Vaswani and Niki Parmar and Llion Jones and Jakob Uszkoreit},
      year={2017},
      eprint={1706.05137},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}