{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Table of contents**<a id='toc0_'></a>    \n",
        "- [Decoder (GPT)](#toc1_)    \n",
        "  - [Constants](#toc1_1_)    \n",
        "  - [Reproducibility](#toc1_2_)    \n",
        "  - [Utilities](#toc1_3_)    \n",
        "  - [Config](#toc1_4_)    \n",
        "  - [Dataset](#toc1_5_)    \n",
        "    - [Refactor](#toc1_5_1_)    \n",
        "  - [DataLoader](#toc1_6_)    \n",
        "    - [Example](#toc1_6_1_)    \n",
        "  - [Model](#toc1_7_)    \n",
        "    - [Masks](#toc1_7_1_)    \n",
        "      - [Padding Mask](#toc1_7_1_1_)    \n",
        "      - [Look-Ahead Mask (Future Mask)](#toc1_7_1_2_)    \n",
        "      - [Using Both Masks in the Decoder](#toc1_7_1_3_)    \n",
        "  - [2-Digits Addition](#toc1_8_)    \n",
        "  - [Adder Decoder Walkthrough](#toc1_9_)    \n",
        "    - [Target Padding Mask (`target_padding_mask`)](#toc1_9_1_)    \n",
        "    - [Future Mask (`future_mask`)](#toc1_9_2_)    \n",
        "    - [Example of Source Padding and Future Masks](#toc1_9_3_)    \n",
        "      - [First Sample First Token](#toc1_9_3_1_)    \n",
        "      - [First Sample Fourth Token](#toc1_9_3_2_)    \n",
        "    - [Further Add a Singleton Dimension in Masks](#toc1_9_4_)    \n",
        "    - [MultiHeadAttention](#toc1_9_5_)    \n",
        "      - [A Primer](#toc1_9_5_1_)    \n",
        "      - [An Example](#toc1_9_5_2_)    \n",
        "    - [AddNorm (Residual Connection + Layer Normalization)](#toc1_9_6_)    \n",
        "      - [Residual Block](#toc1_9_6_1_)    \n",
        "      - [Layer Normalization](#toc1_9_6_2_)    \n",
        "      - [Combining Both](#toc1_9_6_3_)    \n",
        "    - [The Head and the Softmax Layer](#toc1_9_7_)    \n",
        "  - [Potential to use Module Dict?](#toc1_10_)    \n",
        "  - [Training with GPT-like Model](#toc1_11_)    \n",
        "    - [Loss Computation](#toc1_11_1_)    \n",
        "    - [Example](#toc1_11_2_)    \n",
        "    - [Confusion: Training versus Inference](#toc1_11_3_)    \n",
        "  - [Questions](#toc1_12_)    \n",
        "    - [Why Masked == 0 in some?](#toc1_12_1_)    \n",
        "    - [what is the reason of setting the attention scores's mask indexes to negative infinity](#toc1_12_2_)    \n",
        "    - [Why do we need both ignore index in Loss and also negative infinity mask](#toc1_12_3_)    \n",
        "    - [Target and Preds/Logits Shape](#toc1_12_4_)    \n",
        "    - [Why do we flatten prediction and target (logits)?](#toc1_12_5_)    \n",
        "      - [Background](#toc1_12_5_1_)    \n",
        "      - [Traditional Loss Computation](#toc1_12_5_2_)    \n",
        "      - [Why Flatten?](#toc1_12_5_3_)    \n",
        "      - [Step-by-step Flattening](#toc1_12_5_4_)    \n",
        "    - [Why sometimes unsqueeze masks?](#toc1_12_6_)    \n",
        "    - [Why does sequence length differ for source and target, usually I thought it is just all L, same.](#toc1_12_7_)    \n",
        "    - [Am i right to assume that the core idea of autoregressive model like decoder only (GPT like) is that for a given sample, there will eventually be L rows where L is the seq length, and therefore I can intuitively view it as 1 sample having L samples, since for each row, we will compute the loss. Am I right in my understanding? Do not hesistate to correct me.](#toc1_12_8_)    \n",
        "    - [QKV Again](#toc1_12_9_)    \n",
        "      - [Background and Assumptions](#toc1_12_9_1_)    \n",
        "      - [Context Vector](#toc1_12_9_2_)    \n",
        "      - [Query (Q), Key (K), and Value (V)](#toc1_12_9_3_)    \n",
        "      - [Mathematical Description](#toc1_12_9_4_)    \n",
        "  - [TODO](#toc1_13_)    \n",
        "  - [References and Further Readings](#toc1_14_)    \n",
        "\n",
        "<!-- vscode-jupyter-toc-config\n",
        "\tnumbering=false\n",
        "\tanchor=true\n",
        "\tflat=false\n",
        "\tminLevel=1\n",
        "\tmaxLevel=6\n",
        "\t/vscode-jupyter-toc-config -->\n",
        "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a id='toc1_'></a>[Decoder (GPT)](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Cvk1SmuCrdzK"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "import numpy as np\n",
        "import rich\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from rich.pretty import pprint\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from enum import Enum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_1_'></a>[Constants](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Constants(Enum):\n",
        "    \"\"\"Generic constants class.\"\"\"\n",
        "    # fmt: off\n",
        "    SEED : int  = 42\n",
        "    DEBUG: bool = True\n",
        "    # fmt: on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DYDfkBu2rdzM"
      },
      "outputs": [],
      "source": [
        "SEED   = 42\n",
        "DEBUG  = True\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_2_'></a>[Reproducibility](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mWV1lAHkrdzN"
      },
      "outputs": [],
      "source": [
        "def seed_all(seed: Optional[int] = 1992, seed_torch: bool = True) -> int:\n",
        "    \"\"\"\n",
        "    Seed all random number generators.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    seed : int, optional\n",
        "        Seed number to be used, by default 1992.\n",
        "    seed_torch : bool, optional\n",
        "        Whether to seed PyTorch or not, by default True.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    seed: int\n",
        "        The seed number.\n",
        "    \"\"\"\n",
        "    # fmt: off\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)       # set PYTHONHASHSEED env var at fixed value\n",
        "    np.random.seed(seed)                           # numpy pseudo-random generator\n",
        "    random.seed(seed)                              # python's built-in pseudo-random generator\n",
        "\n",
        "    if seed_torch:\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)           # pytorch (both CPU and CUDA)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.backends.cudnn.enabled = False\n",
        "    # fmt: on\n",
        "    return seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0vtP-UEArdzN",
        "outputId": "b3d1d974-7d30-46ed-fa92-a88338150f80"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seed_all(SEED, seed_torch=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_3_'></a>[Utilities](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DQ7N8MfGrdzP"
      },
      "outputs": [],
      "source": [
        "def forward_hook(\n",
        "    module: nn.Module, input: Tuple[torch.Tensor], output: torch.Tensor\n",
        ") -> None:\n",
        "    \"\"\"Custom hook function to print layer information.\"\"\"\n",
        "    if not hasattr(module, \"has_printed\"):\n",
        "        module.has_printed = False\n",
        "\n",
        "    if not module.has_printed:\n",
        "        print(f\"Layer: {module.__class__.__name__}\")\n",
        "        print(f\"Input shape: {str(input[0].shape)}\")\n",
        "        print(f\"Output shape: {str(output.shape)}\")\n",
        "        module.has_printed = True\n",
        "\n",
        "\n",
        "def are_both_models_same(state_dict_1, state_dict_2):\n",
        "    # Check if both models have the same keys\n",
        "    if set(state_dict_1.keys()) != set(state_dict_2.keys()):\n",
        "        return False\n",
        "\n",
        "    # Check if all tensors have the same shape and values\n",
        "    for key in state_dict_1.keys():\n",
        "        if state_dict_1[key].shape != state_dict_2[key].shape:\n",
        "            return False\n",
        "        if not torch.allclose(state_dict_1[key], state_dict_2[key]):\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "    # return model_1.state_dict().__str__() == model_2.state_dict().__str__()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_4_'></a>[Config](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5qgkgYhirdzQ"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class MultiHeadedAttentionConfig:\n",
        "    attention: Attention\n",
        "    d_model: int\n",
        "    H: int\n",
        "    dropout: float = 0.1\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class PositionwiseFeedForwardConfig:\n",
        "    d_model: int\n",
        "    d_ff: int\n",
        "    activation: Any = field(default_factory=lambda: nn.ReLU())\n",
        "    dropout: float = 0.1\n",
        "    bias: bool = True\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class AddNormConfig:\n",
        "    feature_dim: int\n",
        "    dropout: float\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DecoderBlockConfig:\n",
        "    masked_self_attention_mha: MultiHeadedAttentionConfig\n",
        "    feed_forward: PositionwiseFeedForwardConfig\n",
        "    add_norm_1: AddNormConfig\n",
        "    add_norm_2: AddNormConfig\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    d_model: int\n",
        "    vocab_size: int\n",
        "    max_seq_len: int\n",
        "    num_layers: int\n",
        "    dropout: float\n",
        "    decoder: DecoderBlockConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_5_'></a>[Dataset](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZX9ORv5IrdzR"
      },
      "outputs": [],
      "source": [
        "# fmt: off\n",
        "PLUS_SIGN  = 10\n",
        "MUL_SIGN   = 11\n",
        "MINUS_SIGN = 12\n",
        "EQUAL_SIGN = 13\n",
        "EOS        = 14\n",
        "BOS        = 15\n",
        "PAD        = 16\n",
        "UNK        = 17\n",
        "# fmt: on\n",
        "\n",
        "# map tokens to their corresponding index\n",
        "token_to_index: Dict[str, int] = {\n",
        "    \"0\": 0,\n",
        "    \"1\": 1,\n",
        "    \"2\": 2,\n",
        "    \"3\": 3,\n",
        "    \"4\": 4,\n",
        "    \"5\": 5,\n",
        "    \"6\": 6,\n",
        "    \"7\": 7,\n",
        "    \"8\": 8,\n",
        "    \"9\": 9,\n",
        "    \"+\": PLUS_SIGN,\n",
        "    \"*\": MUL_SIGN,\n",
        "    \"-\": MINUS_SIGN,\n",
        "    \"=\": EQUAL_SIGN,\n",
        "    \"<EOS>\": EOS,\n",
        "    \"<BOS>\": BOS,\n",
        "    \"<pad>\": PAD,\n",
        "    \"??\": UNK,\n",
        "}\n",
        "index_to_token: Dict[int, str] = dict((v, k) for k, v in token_to_index.items())\n",
        "vocab_size: int = len(token_to_index)\n",
        "\n",
        "\n",
        "def pad_number(num: int, length: int) -> str:\n",
        "    \"\"\"\n",
        "    Pad numbers with zeros in front so that they have uniform length.\n",
        "\n",
        "    Note, if a + b = c and num digits allowed to add is 2, then for\n",
        "    a and b we always pad to length 2, but for c we always pad to length 3.\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    6 + 90 = 96 -> 06 + 90 = 096\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    num : int\n",
        "        Number to be padded.\n",
        "    num_digits : int\n",
        "        Length of the resulting padded number string.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        Padded number string.\n",
        "    \"\"\"\n",
        "    return str(num).zfill(length)\n",
        "\n",
        "\n",
        "def equation_to_string(a: int, b: int, c: int, num_digits: int) -> str:\n",
        "    \"\"\"\n",
        "    Formats the addition equation as a string.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    a : int\n",
        "        First addend.\n",
        "    b : int\n",
        "        Second addend.\n",
        "    c : int\n",
        "        Sum of a and b.\n",
        "    num_digits : int\n",
        "        Number of digits each number in the equation should have.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        Formatted equation string.\n",
        "    \"\"\"\n",
        "    padded_a = pad_number(a, num_digits)\n",
        "    padded_b = pad_number(b, num_digits)\n",
        "    padded_c = pad_number(c, num_digits + 1) # note the padding here!\n",
        "    return f\"{padded_a}+{padded_b}={padded_c}\"\n",
        "\n",
        "def decode_equation(equation: List[int]) -> str:\n",
        "    \"\"\"\n",
        "    Convert an equation in list format to string format.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    equation : List[int]\n",
        "        The equation in list format.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        The equation in string format.\n",
        "    \"\"\"\n",
        "    if isinstance(equation, torch.Tensor): equation = equation.tolist()\n",
        "    res = \"\".join([str(index_to_token.get(x, UNK)) for x in equation])\n",
        "    return res.replace(\"<BOS>\", \"\").replace(\"<EOS>\", \"\")\n",
        "\n",
        "def encode_equation(equation: str, num_digits: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Convert an equation (up to the equal sign in it) in string format to a list.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    equation : str\n",
        "        The equation in string format.\n",
        "    num_digits : int\n",
        "        Number of digits each number in the equation should have.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "        The equation in list format as a tensor.\n",
        "    \"\"\"\n",
        "    plus_idx = equation.index(\"+\")\n",
        "    equal_idx = equation.index(\"=\")\n",
        "\n",
        "    a = pad_number(int(equation[:plus_idx]), num_digits)\n",
        "    b = pad_number(int(equation[plus_idx + 1:equal_idx]), num_digits)\n",
        "\n",
        "    new_equation = f\"{a}+{b}=\"\n",
        "\n",
        "    return torch.tensor(\n",
        "        [BOS] + [token_to_index.get(n, UNK) for n in new_equation],\n",
        "        dtype=torch.int\n",
        "    ).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SovDUghLrdzR"
      },
      "outputs": [],
      "source": [
        "def create_add_dataset(\n",
        "    num_digits: int, dataset_size: int, rng_seed: int = 1337\n",
        ") -> Tuple[List[torch.Tensor], List[str]]:\n",
        "    rng = torch.Generator()\n",
        "    rng.manual_seed(rng_seed)\n",
        "\n",
        "    max_num = 10**num_digits - 1\n",
        "\n",
        "    dataset_str = []\n",
        "    for _ in range(dataset_size):\n",
        "        a = torch.randint(low=0, high=max_num + 1, size=(1,), generator=rng).item()\n",
        "        b = torch.randint(low=0, high=max_num + 1, size=(1,), generator=rng).item()\n",
        "        c = a + b\n",
        "\n",
        "        equation = equation_to_string(a, b, c, num_digits)\n",
        "\n",
        "        dataset_str.append(equation)\n",
        "\n",
        "    dataset_tensor = [\n",
        "        torch.tensor([BOS] + [token_to_index.get(n, UNK) for n in x] + [EOS])\n",
        "        for x in dataset_str\n",
        "    ]\n",
        "    return dataset_tensor, dataset_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xEfK777ErdzR",
        "outputId": "20b75567-38b1-4e96-9892-08f60d18195d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span><span style=\"font-weight: bold\">])</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span><span style=\"font-weight: bold\">])</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span><span style=\"font-weight: bold\">])</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span><span style=\"font-weight: bold\">])</span>\n",
              "<span style=\"font-weight: bold\">]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m[\u001b[0m\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m15\u001b[0m,  \u001b[1;36m1\u001b[0m,  \u001b[1;36m5\u001b[0m, \u001b[1;36m10\u001b[0m,  \u001b[1;36m5\u001b[0m,  \u001b[1;36m7\u001b[0m, \u001b[1;36m13\u001b[0m,  \u001b[1;36m0\u001b[0m,  \u001b[1;36m7\u001b[0m,  \u001b[1;36m2\u001b[0m, \u001b[1;36m14\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m15\u001b[0m,  \u001b[1;36m9\u001b[0m,  \u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m,  \u001b[1;36m0\u001b[0m,  \u001b[1;36m0\u001b[0m, \u001b[1;36m13\u001b[0m,  \u001b[1;36m0\u001b[0m,  \u001b[1;36m9\u001b[0m,  \u001b[1;36m2\u001b[0m, \u001b[1;36m14\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m15\u001b[0m,  \u001b[1;36m9\u001b[0m,  \u001b[1;36m5\u001b[0m, \u001b[1;36m10\u001b[0m,  \u001b[1;36m5\u001b[0m,  \u001b[1;36m3\u001b[0m, \u001b[1;36m13\u001b[0m,  \u001b[1;36m1\u001b[0m,  \u001b[1;36m4\u001b[0m,  \u001b[1;36m8\u001b[0m, \u001b[1;36m14\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m15\u001b[0m,  \u001b[1;36m1\u001b[0m,  \u001b[1;36m5\u001b[0m, \u001b[1;36m10\u001b[0m,  \u001b[1;36m1\u001b[0m,  \u001b[1;36m0\u001b[0m, \u001b[1;36m13\u001b[0m,  \u001b[1;36m0\u001b[0m,  \u001b[1;36m2\u001b[0m,  \u001b[1;36m5\u001b[0m, \u001b[1;36m14\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
              "\u001b[1m]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'15+57=072'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'92+00=092'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'95+53=148'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'15+10=025'</span><span style=\"font-weight: bold\">]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m[\u001b[0m\u001b[32m'15+\u001b[0m\u001b[32m57\u001b[0m\u001b[32m=\u001b[0m\u001b[32m072\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'92+\u001b[0m\u001b[32m00\u001b[0m\u001b[32m=\u001b[0m\u001b[32m092\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'95+\u001b[0m\u001b[32m53\u001b[0m\u001b[32m=\u001b[0m\u001b[32m148\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'15+\u001b[0m\u001b[32m10\u001b[0m\u001b[32m=\u001b[0m\u001b[32m025\u001b[0m\u001b[32m'\u001b[0m\u001b[1m]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "('15+57=072', '15+57=072')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_tensor, dataset_str = create_add_dataset(num_digits=2, dataset_size=4)\n",
        "pprint(dataset_tensor)\n",
        "pprint(dataset_str)\n",
        "\n",
        "decode_equation(dataset_tensor[0]), decode_equation([15, 1, 5, 10, 5, 7, 13,  0, 7,  2, 14])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHs3grM1rdzS"
      },
      "source": [
        "Some notes:\n",
        "\n",
        "1. We included other operations besides addition for future use. So it may seem redundant for now.\n",
        "2. Kapathy's version is more efficient since for an expression such as `15+87=102` would be encoded as `1587102` since for one, we restrict the `num_digits` to be fixed, this means that if `num_digits=2`, then it follows that only numbers that are less or equals to 2 digits can be added together. As a result, `1587102` will always be interpreted as first 2 digit = first num, next 2 digits = second num, last 3 digits as third or the answer (sum of first two). Let's look at two more examples to appreciate this:\n",
        "   1. `0639045 <> 6 + 39 = 45`\n",
        "   2. `5101052 <> 51 + 1 = 52`\n",
        "3. He also encoded the answer backwards because its easier for GPT model, but for a first review, I will not do so to avoid confusion.\n",
        "\n",
        "KAPATHY:\n",
        "\n",
        "As one more example, the problem 6 + 39 = 45 would be encoded as:\n",
        "\n",
        "\"0639054\"\n",
        "\n",
        "where you will notice that we are padding with zeros to make sure that we always\n",
        "produce strings of the exact same size: n + n + (n + 1). When n=2, this is 7.\n",
        "At test time, we will feed in an addition problem by giving the first 2n digits,\n",
        "and hoping that the GPT model completes the sequence with the next (n+1) digits\n",
        "correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5tTF5J-rdzS"
      },
      "source": [
        "TODO: to make it inherit Dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_5_1_'></a>[Refactor](#toc0_)\n",
        "\n",
        "Here, the refactor should be done such that there exists a class that\n",
        "inherits `Dataset` and the input and outputs are generated on the fly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_6_'></a>[DataLoader](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "z3hCsgtrrdzS"
      },
      "outputs": [],
      "source": [
        "def collate_fn(\n",
        "    batch: List[torch.Tensor],\n",
        "    batch_first: bool,\n",
        "    padding_value: int,\n",
        "):\n",
        "    input_padded = torch.nn.utils.rnn.pad_sequence(\n",
        "        batch, batch_first=batch_first, padding_value=padding_value\n",
        "    )\n",
        "    return input_padded\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataLoaders:\n",
        "    # fmt: off\n",
        "    rng_seed    : int\n",
        "    num_digits  : int\n",
        "    dataset_size: int\n",
        "    batch_size: int\n",
        "    train_set = None\n",
        "    val_set = None\n",
        "    test_set = None\n",
        "    train_loader = None\n",
        "    val_loader  = None\n",
        "    test_loader = None\n",
        "    train_size = None\n",
        "    val_size = None\n",
        "    test_size = None\n",
        "    # fmt: on\n",
        "\n",
        "    def __post_init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "    # dangerous list default\n",
        "    def split_data(self, split: List[float] = [0.7, 0.1, 0.2]) -> None:\n",
        "        # if sum(split) != 1.0:\n",
        "        #     raise ValueError(f\"Split ratios should sum to 1 but got {sum(split)}.\")\n",
        "\n",
        "        # fmt: off\n",
        "        self.train_size = round(self.dataset_size * split[0])\n",
        "        self.val_size   = round(self.dataset_size * split[1])\n",
        "        self.test_size  = self.dataset_size - self.train_size - self.val_size\n",
        "\n",
        "        dataset, _ = create_add_dataset(self.num_digits, self.dataset_size)\n",
        "        self.train_set, self.val_set, self.test_set = torch.utils.data.random_split(\n",
        "            dataset,\n",
        "            [self.train_size, self.val_size, self.test_size],\n",
        "            generator=torch.Generator().manual_seed(self.rng_seed),\n",
        "        )\n",
        "\n",
        "        self.train_loader = DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True, collate_fn=lambda batch: collate_fn(batch, batch_first=True, padding_value=PAD))\n",
        "        self.val_loader   = DataLoader(self.val_set, batch_size=self.batch_size, shuffle=False, collate_fn=lambda batch: collate_fn(batch, batch_first=True, padding_value=PAD))\n",
        "        self.test_loader  = DataLoader(self.test_set, batch_size=self.batch_size, shuffle=False, collate_fn=lambda batch: collate_fn(batch, batch_first=True, padding_value=PAD))\n",
        "        # fmt: on\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vFJLJfJ7rdzT"
      },
      "outputs": [],
      "source": [
        "dataloaders = DataLoaders(rng_seed = 1337, num_digits=2, dataset_size=8, batch_size=2)\n",
        "dataloaders.split_data(split=[1,0,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0uwiq6BrdzT"
      },
      "source": [
        "Note here the padding in collate is \"redundant\" since in our earlier code\n",
        "we ensured that all sample has same number of characters by way of padding\n",
        "zeros in front. For example, `23 + 3 =26` will become `23 + 03 = 026`. Consequently,\n",
        "all samples $\\in$ batch will have same length by definition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7JB46sb_rdzT",
        "outputId": "c25be86b-1cc0-424d-9db6-73231961ab6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([15,  9,  5, 10,  5,  3, 13,  1,  4,  8, 14])\n",
            "95+53=148\n",
            "11\n",
            "tensor([15,  1,  5, 10,  1,  0, 13,  0,  2,  5, 14])\n",
            "15+10=025\n",
            "11\n",
            "--------------------------------------------------------------------------------\n",
            "tensor([15,  3,  4, 10,  9,  0, 13,  1,  2,  4, 14])\n",
            "34+90=124\n",
            "11\n",
            "tensor([15,  9,  2, 10,  0,  0, 13,  0,  9,  2, 14])\n",
            "92+00=092\n",
            "11\n",
            "--------------------------------------------------------------------------------\n",
            "tensor([15,  1,  2, 10,  2,  0, 13,  0,  3,  2, 14])\n",
            "12+20=032\n",
            "11\n",
            "tensor([15,  9,  7, 10,  8,  6, 13,  1,  8,  3, 14])\n",
            "97+86=183\n",
            "11\n",
            "--------------------------------------------------------------------------------\n",
            "tensor([15,  9,  0, 10,  3,  8, 13,  1,  2,  8, 14])\n",
            "90+38=128\n",
            "11\n",
            "tensor([15,  1,  5, 10,  5,  7, 13,  0,  7,  2, 14])\n",
            "15+57=072\n",
            "11\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "seed_all(133338, seed_torch=True)\n",
        "\n",
        "for batch in dataloaders.train_loader:\n",
        "    for sample in batch:\n",
        "        print(sample)\n",
        "        print(decode_equation(sample))\n",
        "        print(len(sample))\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_6_1_'></a>[Example](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "uu7QQ4h8rdzT"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# from typing import List\n",
        "# from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# sequences = [\n",
        "#     torch.tensor([1, 2]),\n",
        "#     torch.tensor([3, 4, 5]),\n",
        "#     torch.tensor([6, 7, 8, 9]),\n",
        "#     torch.tensor([2, 3]),\n",
        "# ]\n",
        "# # Let's say PAD is represented by the integer 16\n",
        "# PAD = 16\n",
        "# sample_dataloader = DataLoader(\n",
        "#     sequences,\n",
        "#     batch_size=4,\n",
        "#     collate_fn=lambda b: collate_fn(b, batch_first=True, padding_value=PAD),\n",
        "# )\n",
        "# batch = next(iter(sample_dataloader))\n",
        "# pprint(batch)\n",
        "\n",
        "# # Create the pad mask\n",
        "# pad_mask = batch == PAD\n",
        "# print(pad_mask)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBFRNjFYrdzT"
      },
      "source": [
        "DISTINGUISH BETWEEN GPT DECODER ONLY VS ENCODER DECODER SEQ TO SEQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "G4jzuj9yrdzU",
        "outputId": "2e368854-1357-4091-dd2d-f201b302d32a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[[ True, False, False]]],\n",
              "\n",
              "\n",
              "        [[[ True,  True, False]]],\n",
              "\n",
              "\n",
              "        [[[ True,  True,  True]]]])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def construct_future_mask(seq_len: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Construct a binary mask that contains 1's for all valid connections and 0's for\n",
        "    all outgoing future connections. This mask will be applied to the attention\n",
        "    logits in decoder self-attention such that all logits with a 0 mask are set to\n",
        "    -inf.\n",
        "\n",
        "    :param seq_len: length of the input sequence\n",
        "    :return: (seq_len,seq_len) mask\n",
        "    \"\"\"\n",
        "    future_mask = torch.triu(torch.ones((seq_len, seq_len)), diagonal=1).to(torch.bool)\n",
        "    future_mask = future_mask.contiguous()\n",
        "    return future_mask == 0\n",
        "\n",
        "construct_future_mask(seq_len=3)[:, None, None, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FEPATXYSrdzU"
      },
      "outputs": [],
      "source": [
        "def construct_batches(x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    # drops last column of the input, meaning remove last element of each row\n",
        "    input = x[:, :-1].long()\n",
        "    equal_sign_loc = [(equation==EQUAL_SIGN).nonzero(as_tuple=True)[0].item() for equation in x]\n",
        "\n",
        "\n",
        "    # Mask out the tokens before the equal sign\n",
        "    target = [\n",
        "        torch.cat(\n",
        "            (torch.tensor([PAD] * (equal_sign_loc[i]), dtype=torch.long), x[i, equal_sign_loc[i] + 1:])\n",
        "        ) for i in range(len(x))\n",
        "    ]\n",
        "    target = torch.stack(target).long()\n",
        "\n",
        "    batch_size, seq_len = input.size()\n",
        "    future_mask = construct_future_mask(seq_len)\n",
        "    # future mask has shape (L, L) but we want it to be (B, L, L) then (B, 1, L, L)\n",
        "    future_mask = future_mask.view(1, seq_len, seq_len).expand(size=(batch_size, -1, -1)).unsqueeze(1)\n",
        "\n",
        "    # padding_mask before view has shape: (batch_size, seq_len)\n",
        "    # we want it to be (B, L, L) then (B, 1, L, L)\n",
        "    padding_mask = input != PAD\n",
        "    padding_mask = padding_mask.view(batch_size, 1, 1, seq_len).expand(size=(batch_size, 1, seq_len, seq_len))\n",
        "    return input, target, padding_mask, future_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IEFJbHyrdzU"
      },
      "source": [
        "In PyTorch, `input == PAD` will perform element-wise comparison between each element of the `input` tensor and the constant `PAD`. The `PAD` constant is usually an integer that represents the padding token in a sequence. For example, in NLP tasks, padding tokens are often used to make all sequences in a batch the same length.\n",
        "\n",
        "If `input` is a tensor of shape `(Batch Size, Sequence Length)`, then `input == PAD` will return a boolean tensor of the same shape, where each element at position `(i, j)` will be `True` if `input[i, j]` is equal to `PAD`, and `False` otherwise. This boolean tensor will serve as a mask that identifies where the padding tokens are located in the original `input` tensor.\n",
        "\n",
        "---\n",
        "\n",
        "The `src_mask` term in the context of transformers typically refers to the \"source mask,\" which is designed to prevent the self-attention mechanism from considering certain tokens in the source sequence. This mask is applied to the attention scores before the softmax operation during the calculation of self-attention. The primary purposes of using such a mask are:\n",
        "\n",
        "1. Padding Masking: When sequences are batched together, shorter sequences are often padded with special tokens (usually denoted by zeros or a specific padding token) to match the length of the longest sequence in the batch. The `src_mask` helps the model ignore these padding tokens by setting the corresponding attention scores to a large negative value (usually `-inf`), so that they become zero after the softmax operation.\n",
        "\n",
        "2. Future Information Masking: In some tasks like sequence-to-sequence prediction, it's important that a token does not attend to future tokens in the sequence. This is another use-case for the mask, although this is more commonly seen in the target mask (`tgt_mask`) rather than the source mask (`src_mask`).\n",
        "\n",
        "To determine whether `src_mask` in a specific implementation is the padding mask from the dataloader, you'll need to check the code where this variable is defined or used. Typically, if the mask is intended to filter out padding tokens, then yes, it could very well be the same as the padding mask generated during data loading.\n",
        "\n",
        "The actual implementation may vary, but the concept generally remains the same. You'll often see the mask being used in the self-attention calculation, specifically right before the softmax operation to zero out particular positions.\n",
        "\n",
        "---\n",
        "\n",
        "What are `pad_mask`?\n",
        "\n",
        "The primary reason for using a padding mask (`pad_mask`) is to ensure that the model does not consider padding tokens during training or inference. Padding tokens are usually added to sequences to make them have a uniform length, but they don't carry any meaningful information. Ignoring them is crucial for several reasons:\n",
        "\n",
        "1. **Attention Mechanisms**: If your model uses attention, the mask ensures that attention scores for padding tokens are set to a very low value (often negative infinity), so that these scores don't affect the weighted sum of the input sequence.\n",
        "\n",
        "2. **Loss Computation**: When computing loss, padding tokens should not contribute. Including them could mislead the model during training, as they don't represent genuine mistakes in prediction.\n",
        "\n",
        "3. **Output Interpretation**: When the model makes predictions, padding tokens should not be taken into account for tasks like sequence-to-sequence translation, summarization, etc.\n",
        "\n",
        "4. **Computational Efficiency**: In some models or algorithms, knowing which tokens are padding can speed up computation by allowing the model to skip unnecessary operations.\n",
        "\n",
        "In summary, the padding mask is a utility to ensure that the model focuses only on the meaningful parts of the input sequence, thereby improving both accuracy and computational efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "fu0C3l_ArdzU",
        "outputId": "d2928468-1002-4de0-fa44-f3fec74b15c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[15,  9,  0, 10,  3,  8, 13,  1,  2,  8, 14],\n",
              "        [15,  1,  5, 10,  5,  7, 13,  0,  7,  2, 14]])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch = next(iter(dataloaders.train_loader))\n",
        "batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqb7TK4JrdzV"
      },
      "source": [
        "WHY concat target instead of stack them normally?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "YV5dtxROrdzV",
        "outputId": "ac59be5a-86a0-4428-aec6-5dc0e2043e99"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([2, 10]), torch.Size([2, 10]), torch.Size([2, 1, 10, 10]))"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "construct_batches(batch)[0].shape, construct_batches(batch)[1].shape, construct_batches(batch)[2].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_7_'></a>[Model](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "jl_IgzQSrdzV"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import math\n",
        "import unittest\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import rich\n",
        "import torch\n",
        "# from d2l import torch as d2l\n",
        "from rich.pretty import pprint\n",
        "from torch import nn\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Attention(ABC, nn.Module):\n",
        "    \"\"\"\n",
        "    Base class for attention mechanisms.\n",
        "\n",
        "    This abstract class provides a scaffold for attention mechanisms, with a\n",
        "    dropout layer for regularization included. Subclasses are expected to\n",
        "    implement the `forward` method.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    dropout : The dropout layer applied to the attention scores.\n",
        "        type: nn.Dropout\n",
        "\n",
        "    Note\n",
        "    ----\n",
        "    1.  ABC method might be redundant since inheritance from nn.Module ensures\n",
        "        forward method to be implemented.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dropout: float = 0.0) -> None:\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout, inplace=False)\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(\n",
        "        self,\n",
        "        query: torch.Tensor,\n",
        "        key: torch.Tensor,\n",
        "        value: torch.Tensor,\n",
        "        mask: Optional[torch.BoolTensor] = None,\n",
        "    ) -> torch.Tensor:\n",
        "        raise NotImplementedError(\n",
        "            \"The forward method must be implemented by the subclass.\"\n",
        "        )\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(Attention):\n",
        "    \"\"\"\n",
        "    Implements scaled dot-product attention mechanism.\n",
        "\n",
        "    This class is a derived instance of the Attention class that computes the\n",
        "    scaled dot-product attention, defined by the following operation:\n",
        "\n",
        "    .. math::\n",
        "        \\\\text{Attention}(Q, K, V) = \\\\text{softmax} \\\\left( \\\\frac{QK^T}{\\\\sqrt{d_k}} \\\\right) V\n",
        "\n",
        "    where:\n",
        "    - Q is the query matrix\n",
        "    - K is the key matrix\n",
        "    - V is the value matrix\n",
        "    - d_k is the dimension of the keys\n",
        "\n",
        "    The attention mechanism can be applied in two different contexts: self-attention\n",
        "    and cross-attention. Self-attention allows the model to integrate information\n",
        "    from the entire sequence, while cross-attention allows the model to focus on\n",
        "    information from a different sequence (e.g., encoder outputs).\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    forward(query, key, value, mask)\n",
        "        Computes the forward pass for the scaled dot-product attention.\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        query: torch.Tensor,\n",
        "        key: torch.Tensor,\n",
        "        value: torch.Tensor,\n",
        "        mask: Optional[torch.BoolTensor] = None,\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Perform the forward pass for scaled dot-product attention.\n",
        "\n",
        "        This function applies the attention mechanism on the input tensors `query`,\n",
        "        `key`, and `value`. It's worth noting that for cross-attention, the sequence\n",
        "        lengths of `query` and `key`/`value` may differ. This is because `query` is\n",
        "        usually projected from the decoder's states, while `key` and `value` are from\n",
        "        the encoder's states.\n",
        "\n",
        "        Notations\n",
        "        ---------\n",
        "        - `B`  : Batch size\n",
        "        - `D`  : Embedding dimension\n",
        "        - `H`  : Number of heads\n",
        "        - `d_k`: Dimension of the keys    = D // H\n",
        "        - `d_q`: Dimension of the queries = D // H\n",
        "        - `d_v`: Dimension of the values  = D // H\n",
        "        - `N`  : Batch size\n",
        "        - `T`  : Sequence length for `query`\n",
        "        - `S`  : Sequence length for `key` and `value`\n",
        "        - `L`  : Sequence length for `query`, `key` and `value` generic.\n",
        "\n",
        "        NOTE: We use `L` in our notes instead of `T` and `S` since we assume all query,\n",
        "        key and value are of same length.\n",
        "\n",
        "        TODO: which shape is for cross-self?\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        query:  A tensor of query vectors representing the set of elements each sequence\n",
        "                is seeking to attend to. It contains a batch of sequences, each with a set of\n",
        "                vectors across multiple attention heads.\n",
        "                    type :  torch.Tensor\n",
        "                    shape: `(N, H, S or T, d_q)` where `d_q = D // H`\n",
        "\n",
        "        key  :  A tensor of key vectors that are paired with values to form a mapping. The\n",
        "                dot product of a query with these keys determines the attention weight for the\n",
        "                corresponding values.\n",
        "                    type :  torch.Tensor\n",
        "                    shape: `(N, H, S or T, d_k)` where `d_k = D // H`\n",
        "        value: A tensor of value vectors that are aggregated based on the attention\n",
        "               weights to form the output of the attention mechanism.\n",
        "                    type :  torch.Tensor\n",
        "                    shape: `(N, H, S or T, d_v)` where `d_v = D // H`\n",
        "        mask : An optional boolean mask tensor that can be used to mask out certain positions\n",
        "               from the attention mechanism. For self-attention, the mask shape is typically\n",
        "               `(B, T, T)`. For cross-attention, the mask typically has a shape of `(B, T, S)`\n",
        "               allowing different target positions to attend to different source positions.\n",
        "               Here, `T` is the sequence length of the queries (note `T` is the same for\n",
        "               self-attention), `T_k` and `T_v` are the sequence lengths of the keys and values\n",
        "               which could be equal to `T` in self-attention or vary in cross-attention, and\n",
        "               `S` is the sequence length of the source (encoder) when using cross-attention.\n",
        "\n",
        "               However, to cater to the head dimension `H`, right after the `B` dimension, we\n",
        "               will need to add (unsqueeze) an dimension to have `(B, 1, T, T)` for\n",
        "               self-attention and `(B, 1, T, S)` for cross-attention.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Tuple[torch.Tensor, torch.Tensor]\n",
        "            The context vectors and the attention weights. The context vectors are the weighted sum\n",
        "            of the `value` vectors, representing the information to be attended to.\n",
        "            The attention weights represent the attention probabilities.\n",
        "\n",
        "            - Context Vectors shape:   `(N, T, d_k)`\n",
        "            - Attention Weights shape: `(N, T, S)`\n",
        "        \"\"\"\n",
        "        # fmt: off\n",
        "        d_q               = query.size(dim=-1)\n",
        "\n",
        "        attention_scores  = torch.matmul(query, key.transpose(dim0=-2, dim1=-1)) / torch.sqrt(torch.tensor(d_q).float())\n",
        "        attention_scores  = attention_scores.masked_fill(mask == 0, float(\"-inf\")) if mask is not None else attention_scores\n",
        "\n",
        "        attention_weights = attention_scores.softmax(dim=-1)\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        context_vector    = torch.matmul(attention_weights, value)\n",
        "        # fmt: on\n",
        "        return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "h-er6x7rrdzW"
      },
      "outputs": [],
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    __slots__ = [\n",
        "        \"d_model\",\n",
        "        \"d_k\",\n",
        "        \"d_q\",\n",
        "        \"d_v\",\n",
        "        \"H\",\n",
        "        \"W_Q\",\n",
        "        \"W_K\",\n",
        "        \"W_V\",\n",
        "        \"W_O\",\n",
        "        \"attention\",\n",
        "        \"dropout\",\n",
        "    ]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        attention: Attention,\n",
        "        H: int,\n",
        "        d_model: int,\n",
        "        dropout: float = 0.1,\n",
        "        bias: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        assert d_model % H == 0\n",
        "\n",
        "        # fmt: off\n",
        "        self.d_model   = d_model       # D\n",
        "        self.d_k       = d_model // H  # stay true to notations\n",
        "        self.d_q       = d_model // H\n",
        "        self.d_v       = d_model // H\n",
        "\n",
        "        self.H         = H             # number of heads\n",
        "\n",
        "        # shadow my notations, actually they are of shape D x D.\n",
        "        self.W_Q       = nn.Linear(self.d_model, self.d_q * self.H, bias=bias)  # D x D\n",
        "        self.W_K       = nn.Linear(self.d_model, self.d_k * self.H, bias=bias)\n",
        "        self.W_V       = nn.Linear(self.d_model, self.d_v * self.H, bias=bias)\n",
        "        self.W_O       = nn.Linear(self.d_model, self.d_model, bias=bias)\n",
        "\n",
        "        self.attention = attention\n",
        "        self.dropout   = nn.Dropout(p=dropout, inplace=False)\n",
        "        # fmt: on\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        query: torch.Tensor,\n",
        "        key: torch.Tensor,\n",
        "        value: torch.Tensor,\n",
        "        mask: Optional[torch.BoolTensor] = None,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Notations\n",
        "        ---------\n",
        "        B:      Batch size\n",
        "        S or L: Source sequence length\n",
        "        T or L: Target sequence length\n",
        "        D:      Embedding dimension\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        query:  Although named as query, it is the embeddings Z from the token_embedding + positional_embedding layer.\n",
        "                type:  torch.Tensor\n",
        "                shape: (B, S or T, D)\n",
        "        key:    Although named as key, it is the embeddings Z from the token_embedding + positional_embedding layer.\n",
        "                type:  torch.Tensor\n",
        "                shape: (B, S or T, D)\n",
        "        value:  Although named as value, it is the embeddings Z from the token_embedding + positional_embedding layer.\n",
        "                type:  torch.Tensor\n",
        "                shape: (B, S or T, D)\n",
        "        mask:   Mask to be applied to the attention scores.\n",
        "                type:  torch.BoolTensor\n",
        "                shape: (B, 1, S or T, S or T)\n",
        "\n",
        "        Variables\n",
        "        ---------\n",
        "        W_Q.weight (D, D)\n",
        "\n",
        "        \"\"\"\n",
        "        # fmt: off\n",
        "        if mask is not None:\n",
        "            assert mask.ndim     == 4, f\"Mask should have 4 dimensions but got {mask.ndim}.\"\n",
        "            assert mask.shape[0] == query.shape[0], (\"Batch size of mask and query must match.\")\n",
        "            assert mask.shape[1] == 1, (\"Mask should have shape (batch_size, 1, seq_len, seq_len).\")\n",
        "            assert mask.shape[2] == mask.shape[3] == query.shape[1], (\"Mask should have shape (batch_size, 1, seq_len, seq_len).\")\n",
        "\n",
        "\n",
        "        Q = self.W_Q(query).contiguous() # Z @ W_Q -> LxD @ DxD = LxD\n",
        "        K = self.W_K(key).contiguous()   # Z @ W_K\n",
        "        V = self.W_V(value).contiguous() # Z @ W_V\n",
        "\n",
        "\n",
        "        Q = self.transpose_qkv(Q)        # [B, H, L, D]\n",
        "        K = self.transpose_qkv(K)\n",
        "        V = self.transpose_qkv(V)\n",
        "\n",
        "        # Attention\n",
        "        # same as the other code: x = torch.matmul(p_atten, value)\n",
        "        context_vector, attention_weights = self.attention(Q, K, V, mask)\n",
        "        context_vector_concat             = self.reverse_transpose_qkv(context_vector)\n",
        "        # fmt: on\n",
        "        return self.W_O(context_vector_concat)\n",
        "\n",
        "    def _reset_parameters(self) -> None:\n",
        "        \"\"\"See PyTorch's code for inspiration!\"\"\"\n",
        "        ...\n",
        "\n",
        "    def transpose_qkv(self, q_or_k_or_v: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Transposition for parallel computation of multiple attention heads.\n",
        "        TODO: Why does transpose allow parallel computation?\n",
        "        \"\"\"\n",
        "        # fmt: off\n",
        "        # 1. q_or_k_or_v is shape (B, L, D)\n",
        "        # 2. aim to make it of shape (B, L, H, D / H = d_qkv)\n",
        "        batch_size, seq_len, _ = q_or_k_or_v.shape\n",
        "        q_or_k_or_v            = q_or_k_or_v.view(batch_size, seq_len, self.H, self.d_model // self.H)\n",
        "\n",
        "        # 3. switch H from 3rd to 2nd dimension, or in python swap 2nd to 1st\n",
        "        q_or_k_or_v            = q_or_k_or_v.permute(0, 2, 1, 3)\n",
        "        # fmt: on\n",
        "        return q_or_k_or_v\n",
        "\n",
        "    def reverse_transpose_qkv(self, q_or_k_or_v: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Reverse the transposition operation for concatenating multiple attention heads.\"\"\"\n",
        "        # fmt: off\n",
        "        # 1. q_or_k_or_v is shape (B, H, L, D / H = d_qkv)\n",
        "        # 2. aim to make it of shape (B, L, H, D / H = d_qkv)\n",
        "        q_or_k_or_v = q_or_k_or_v.permute(0, 2, 1, 3)\n",
        "\n",
        "        # 3. Merge H and d_qkv into D\n",
        "        batch_size, seq_len, _, _ = q_or_k_or_v.shape\n",
        "        q_or_k_or_v = q_or_k_or_v.contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        # fmt: on\n",
        "        return q_or_k_or_v\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        sublayer: Callable[[torch.Tensor], torch.Tensor],\n",
        "    ) -> torch.Tensor:\n",
        "        return x + sublayer(x)\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, feature_dim: int, eps: float = 1e-6) -> None:\n",
        "        super().__init__()\n",
        "        # fmt: off\n",
        "        self.gamma = nn.Parameter(torch.ones(feature_dim))\n",
        "        self.beta  = nn.Parameter(torch.zeros(feature_dim))\n",
        "        self.eps   = eps\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        # fmt: on\n",
        "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
        "\n",
        "class AddNorm(nn.Module):\n",
        "    \"\"\"AddNorm is apt since the diagram uses Add + Norm.\n",
        "    Some call it SubLayer connection (Harvard) some call it\n",
        "    residual connection: x + dropout(sublayer(layernorm(x)))\n",
        "\n",
        "    If stay true, then we apply residual then layer norm. So\n",
        "    we adopt the d2l method.\"\"\"\n",
        "    def __init__(self, feature_dim: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        # fmt: off\n",
        "        self.dropout    = nn.Dropout(p=dropout, inplace=False)\n",
        "        self.layer_norm = nn.LayerNorm(normalized_shape=feature_dim)\n",
        "        # fmt: on\n",
        "\n",
        "    def forward(\n",
        "        self, x: torch.Tensor, sublayer: Callable[[torch.Tensor], torch.Tensor]\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"G(F(x) + x) where G = layer norm and F = sublayer\"\"\"\n",
        "        return self.layer_norm(x + sublayer(self.dropout(x)))\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        d_ff: int,\n",
        "        activation: nn.Module = nn.ReLU(),\n",
        "        dropout: float = 0.1,\n",
        "        bias: bool = True,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        # fmt: off\n",
        "        self.dense_1    = nn.Linear(d_model, d_ff, bias=bias)\n",
        "        self.dense_2    = nn.Linear(d_ff, d_model, bias=bias)\n",
        "        self.activation = activation\n",
        "        self.dropout    = nn.Dropout(p=dropout, inplace=False)\n",
        "        self.ffn        = nn.Sequential(self.dense_1, self.activation, self.dropout, self.dense_2)\n",
        "        # fmt: on\n",
        "\n",
        "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
        "        return self.ffn(z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "qMKX_p2srdzW"
      },
      "outputs": [],
      "source": [
        "class GPTDecoderBlock(nn.Module):\n",
        "    \"\"\"GPTDecoderBlock focuses on masked self-attention and feed-forward layers.\n",
        "\n",
        "    The architecture follows the GPT-style decoder, which only has masked\n",
        "    self-attention and position-wise feed-forward layers, omitting the\n",
        "    encoder-decoder cross-attention.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: ModelConfig) -> None:\n",
        "        super().__init__()\n",
        "        # fmt: off\n",
        "        self.masked_self_attention_mha = MultiHeadedAttention(**config.decoder.masked_self_attention_mha.__dict__)\n",
        "        # self.encoder_decoder_cross_attention_mha = MultiHeadedAttention(**config.decoder.encoder_decoder_cross_attention_mha)\n",
        "\n",
        "        self.feed_forward              = PositionwiseFeedForward(**config.decoder.feed_forward.__dict__)\n",
        "\n",
        "        self.add_norm_1                = AddNorm(**config.decoder.add_norm_1.__dict__)\n",
        "        self.add_norm_2                = AddNorm(**config.decoder.add_norm_2.__dict__)\n",
        "\n",
        "        # self.feed_forward.register_forward_hook(forward_hook)\n",
        "        # fmt: on\n",
        "\n",
        "    def forward(self, z: torch.Tensor, target_mask: torch.BoolTensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        z:           Input sequence.\n",
        "                     type:  torch.Tensor\n",
        "                     shape: (B, S or T, D)\n",
        "        target_mask: Target mask.\n",
        "                     type:  torch.BoolTensor\n",
        "                     shape: (B, 1, S or T, S or T)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        z:           Output tensor after masked self-attention and feed-forward layers.\n",
        "                     type:  torch.Tensor\n",
        "                     shape: (B, S or T, D)\n",
        "        \"\"\"\n",
        "        z = self.add_norm_1(\n",
        "            z,\n",
        "            lambda z: self.masked_self_attention_mha(\n",
        "                query=z, key=z, value=z, mask=target_mask\n",
        "            ),\n",
        "        )\n",
        "        z = self.add_norm_2(z, self.feed_forward)\n",
        "        return z\n",
        "\n",
        "\n",
        "class GPTDecoder(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        # fmt: off\n",
        "        self.d_model       : int                   = config.d_model\n",
        "        self.tok_embed     : nn.Embedding          = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.pos_embed     : nn.Parameter          = nn.Parameter(torch.zeros(1, config.max_seq_len, config.d_model))\n",
        "        self.decoder_blocks: List[GPTDecoderBlock] = nn.ModuleList([GPTDecoderBlock(config) for _ in range(config.num_layers)])\n",
        "\n",
        "        self.dropout       : nn.Dropout            = nn.Dropout(config.dropout)\n",
        "        self.layer_norm    : nn.LayerNorm          = nn.LayerNorm(config.d_model)\n",
        "        self.head          : nn.Linear             = nn.Linear(config.d_model, config.vocab_size)  # last layer\n",
        "        # fmt: on\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self) -> None:\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_tokens: torch.LongTensor,\n",
        "        target_padding_mask: torch.BoolTensor,\n",
        "        future_mask: torch.BoolTensor,\n",
        "    ) -> torch.FloatTensor:\n",
        "        \"\"\"\n",
        "        Notations\n",
        "        ---------\n",
        "        B:      Batch size\n",
        "        S or L: Source sequence length\n",
        "        T or L: Target sequence length\n",
        "        D:      Embedding dimension\n",
        "        V:      Vocabulary size (Class size)\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_tokens:        Input sequence.\n",
        "                             type:  torch.Tensor\n",
        "                             shape: (B, S or T)\n",
        "        target_padding_mask: Target padding mask.\n",
        "                             type:  torch.BoolTensor\n",
        "                             shape: (B, 1, S or T, S or T)\n",
        "        future_mask:         Future mask.\n",
        "                             type:  torch.BoolTensor\n",
        "                             shape: (B, 1, S or T, S or T)\n",
        "\n",
        "        Variables\n",
        "        ---------\n",
        "        z:                   Input sequence after token and position embedding.\n",
        "                             type:  torch.Tensor\n",
        "                             shape: (B, S or T, D)\n",
        "        target_mask:         Target mask.\n",
        "                             type:  torch.BoolTensor\n",
        "                             shape: (B, 1, S or T, S or T)\n",
        "        logits:              Output logits.\n",
        "                             type:  torch.FloatTensor\n",
        "                             shape: (B, S or T, V)\n",
        "        \"\"\"\n",
        "        seq_len = input_tokens.size(1)\n",
        "        target_mask: torch.BoolTensor = torch.logical_and(\n",
        "            target_padding_mask, future_mask\n",
        "        )\n",
        "\n",
        "        # fmt: off\n",
        "        z = self.tok_embed(input_tokens) # * math.sqrt(self.d_model) for better optimization landscape\n",
        "        z = z + self.pos_embed[:, :seq_len, :]\n",
        "        z = self.dropout(z)\n",
        "\n",
        "        for decoder_block in self.decoder_blocks:\n",
        "            z  = decoder_block(z, target_mask)\n",
        "\n",
        "        z      = self.layer_norm(z)\n",
        "        logits = self.head(z)\n",
        "        # fmt: on\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_7_1_'></a>[Masks](#toc0_)\n",
        "\n",
        "In Transformer models, especially in the decoder, two types of masks are commonly used: padding masks and look-ahead masks (or future masks). Here's why each is important and why you might need both:\n",
        "\n",
        "#### <a id='toc1_7_1_1_'></a>[Padding Mask](#toc0_)\n",
        "\n",
        "1. **Why it's needed**: When you're dealing with sequences of different lengths, you pad the shorter sequences with zeros to make them the same length as the longest one in the batch. These zero-paddings should not contribute to the output of the attention mechanism.\n",
        "  \n",
        "2. **Where it's used**: Both the encoder and the decoder use padding masks.\n",
        "\n",
        "3. **How it works**: The padding mask marks the padded positions so that they can be excluded from contributing to the attention mechanism. In practice, you'll typically set the corresponding attention scores to negative infinity before applying the softmax operation.\n",
        "\n",
        "#### <a id='toc1_7_1_2_'></a>[Look-Ahead Mask (Future Mask)](#toc0_)\n",
        "\n",
        "1. **Why it's needed**: In the decoder, each position can only attend to positions that come before it in the sequence to maintain the auto-regressive property. This is different from the encoder, where all positions can attend to all other positions.\n",
        "\n",
        "2. **Where it's used**: This mask is specifically for the decoder.\n",
        "\n",
        "3. **How it works**: The look-ahead mask is used to mask out future positions (i.e., positions that come after the current position) so that they don't contribute to the current attention scores. Before the softmax operation, you'll mark these positions so that their contributions are effectively zero.\n",
        "\n",
        "#### <a id='toc1_7_1_3_'></a>[Using Both Masks in the Decoder](#toc0_)\n",
        "\n",
        "It's possible to use both types of masks in the decoder to address different requirements:\n",
        "\n",
        "- Padding mask is for ignoring padded positions.\n",
        "- Look-ahead mask is for ensuring that each position only attends to positions before it in the sequence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_8_'></a>[2-Digits Addition](#toc0_)\n",
        "\n",
        "- `max_len` example: `<BOS>90+38=128<EOS>`\n",
        "  - another reminder our max len in this example is same but this is on purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "dEnXbaKfrdzc"
      },
      "outputs": [],
      "source": [
        "rng_seed = 1992\n",
        "num_digits = 2\n",
        "dataset_size = 10000\n",
        "batch_size = 256\n",
        "\n",
        "# max_len is determined by 1+ num_digits + 1 + num_digits + 1 + num_digits + 1 + 1\n",
        "# where the 1s represent BOS, Plus sign, Equal sign, the extra digit in the sum, EOS, respectively.\n",
        "max_len = 1 + 1 + 1 + 1 + 2 * num_digits + (num_digits + 1)\n",
        "\n",
        "dataloaders = DataLoaders(rng_seed, num_digits, dataset_size, batch_size=batch_size)\n",
        "dataloaders.split_data(split=[0.7, 0.2, 0.1])\n",
        "\n",
        "train_size, val_size, test_size = dataloaders.train_size, dataloaders.val_size, dataloaders.test_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "NTROZtD4rdzc",
        "outputId": "253c0d26-f2d1-49ad-bd5c-b623d6c79339"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_size: 270226, train_set_size: 7000\n"
          ]
        }
      ],
      "source": [
        "# Create individual component configurations\n",
        "masked_self_attention_mha_config = MultiHeadedAttentionConfig(\n",
        "     attention=ScaledDotProductAttention(),\n",
        "    d_model=128, H=4, dropout=0.1\n",
        ")\n",
        "\n",
        "feed_forward_config = PositionwiseFeedForwardConfig(\n",
        "    d_model=128, d_ff=256, activation=nn.ReLU(), dropout=0.1, bias=True\n",
        ")\n",
        "\n",
        "add_norm_config_1 = AddNormConfig(feature_dim=128, dropout=0.1)\n",
        "add_norm_config_2 = AddNormConfig(feature_dim=128, dropout=0.1)\n",
        "\n",
        "# Create DecoderBlockConfig\n",
        "decoder_block_config = DecoderBlockConfig(\n",
        "    masked_self_attention_mha=masked_self_attention_mha_config,\n",
        "    feed_forward=feed_forward_config,\n",
        "    add_norm_1=add_norm_config_1,\n",
        "    add_norm_2=add_norm_config_2,\n",
        ")\n",
        "\n",
        "# Create the overall ModelConfig\n",
        "model_config = ModelConfig(\n",
        "    d_model=128,\n",
        "    vocab_size=vocab_size,  # You'll need to specify this based on your dataset\n",
        "    max_seq_len=max_len,  # Assuming max_len is defined elsewhere in your code\n",
        "    num_layers=2,\n",
        "    dropout=0.1,\n",
        "    decoder=decoder_block_config,\n",
        ")\n",
        "\n",
        "model = GPTDecoder(model_config).to(DEVICE)\n",
        "model_size = sum([p.numel() for p in model.parameters()])\n",
        "print(f'model_size: {model_size}, train_set_size: {train_size}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "TpbLlCLRrdzc"
      },
      "outputs": [],
      "source": [
        "warmup_steps = 3*len(dataloaders.train_loader)\n",
        "# lr first increases in the warmup steps, and then descreases\n",
        "lr_fn        = lambda step: model_config.d_model**(-0.5) * min([(step+1)**(-0.5), (step+1)*warmup_steps**(-1.5)])\n",
        "optimizer    = torch.optim.Adam(model.parameters(), lr=0.2, betas=(0.9, 0.98), eps=1e-9)\n",
        "scheduler    = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_fn)\n",
        "criterion    = nn.CrossEntropyLoss(ignore_index=PAD, reduction=\"mean\")\n",
        "\n",
        "def train_epoch(\n",
        "    model: torch.nn.Module,\n",
        "    dataloader: DataLoader,\n",
        "    criterion: torch.nn.Module,\n",
        "    optimizer: Optimizer,\n",
        "    scheduler: Optional[_LRScheduler] = None,\n",
        "    grad_norm_clip: float = 1.0,\n",
        "    device: str = \"cuda\",\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    input: This is the input sequence (the EOS token is removed).\n",
        "        shape: [B, S or T]\n",
        "    target: This is the input shifted by one time step to the right (the BOS token is removed).\n",
        "        shape: [B, S or T]\n",
        "    target_padding_mask:\n",
        "        shape: [B, 1, S or T, S or T]\n",
        "    future_mask:\n",
        "        shape: [B, 1, S or T, S or T]\n",
        "    logits:\n",
        "        shape: [B, S or T, V]\n",
        "        shape: [B, V, S or T] when passed in to loss as pytorch enforces the 2nd dim to be class/vocab.\n",
        "    \"\"\"\n",
        "    model.to(\n",
        "        device=device,\n",
        "        dtype=next(model.parameters()).dtype,\n",
        "        non_blocking=True,\n",
        "    )\n",
        "    model.train()\n",
        "\n",
        "    # fmt: off\n",
        "    losses:       List[float] = []\n",
        "    num_batches:  int         = len(dataloader)\n",
        "    progress_bar: tqdm        =  tqdm(enumerate(dataloader, start=1), total=num_batches)\n",
        "    # fmt: on\n",
        "\n",
        "    for _batch_index, x in progress_bar:\n",
        "        input, target, target_padding_mask, future_mask = construct_batches(x)\n",
        "        # flatten the target tensor, why?\n",
        "        #target = target.view(-1).to(device)\n",
        "        input, target_padding_mask, future_mask = (\n",
        "            input.to(device),\n",
        "            target_padding_mask.to(device),\n",
        "            future_mask.to(device),\n",
        "        )\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(\n",
        "            input,\n",
        "            target_padding_mask=target_padding_mask,\n",
        "            future_mask=future_mask,\n",
        "        )\n",
        "        #logits = logits.view(size=(-1, logits.size(-1)))\n",
        "\n",
        "        loss = criterion(logits.permute(0, 2, 1).contiguous(), target.contiguous())\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_norm_clip)\n",
        "        optimizer.step()\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        if _batch_index > 0 and _batch_index % 50 == 0:\n",
        "            progress_bar.set_description(\n",
        "                f\"ep: {scheduler.last_epoch // num_batches}, train loss={loss.item():.3f}, lr={scheduler.get_last_lr()[0]:.5f}\"\n",
        "            )\n",
        "\n",
        "    return np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "5zFpwjjvrdzc"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def train(model, dataloaders, epochs):\n",
        "    global early_stop_count\n",
        "    train_size = len(dataloaders.train_loader) * batch_size\n",
        "    for ep in range(epochs):\n",
        "        train_loss = train_epoch(model, dataloaders.train_loader, criterion, optimizer, scheduler, grad_norm_clip=1.0, device=DEVICE)\n",
        "        val_loss = validate(model, dataloaders.val_loader)\n",
        "        print(f\"ep {ep}: train_loss: {train_loss:.5f}, val_loss: {val_loss:.5f}\")\n",
        "\n",
        "    return train_loss, val_loss\n",
        "\n",
        "\n",
        "def validate(model, dataloder):\n",
        "    \"function for computing the loss on the validation set\"\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for i, x in enumerate(dataloder):\n",
        "            input, target, target_padding_mask, future_mask = construct_batches(x)\n",
        "            target = target.view(-1)\n",
        "            pred = model(input, target_padding_mask=target_padding_mask, future_mask=future_mask).to(DEVICE)\n",
        "\n",
        "            pred = pred.view(-1, pred.size(-1))\n",
        "            losses.append(criterion(pred, target).item())\n",
        "    return np.mean(losses)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_sum(model, x):\n",
        "    \"Function for computing the sum of two numbers.\"\n",
        "    for i in range(num_digits + 2):\n",
        "        pad_mask = (x != PAD).view(1, 1, 1, x.size(-1)).to(DEVICE)\n",
        "        future_mask = construct_future_mask(seq_len=x.size(1))\n",
        "        # input, target, target_padding_mask, future_mask = construct_batches(x)\n",
        "        #logits = model(x, target_padding_mask=target_padding_mask, future_mask=future_mask).to(DEVICE)\n",
        "\n",
        "        logits = model(x, pad_mask, future_mask)\n",
        "        last_output = logits.argmax(-1)[:, -1].view(1, 1)\n",
        "        x = torch.cat((x, last_output), 1).to(DEVICE)\n",
        "        if last_output.item() == EOS:\n",
        "            break\n",
        "    return x[0]\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, num_batch=None):\n",
        "    \"\"\"Function for evaluation the model.\n",
        "    This function take equations, and truncate them up to the equal-sign, and feed them to the\n",
        "    model to get the predictions, compare them with the correct answers, and output the accuracy.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    acc, count = 0, 0\n",
        "    num_wrong_to_display = 5\n",
        "    for idx, x in enumerate(dataloader):\n",
        "        for equation in x:\n",
        "            loc_equal_sign = equation.tolist().index(EQUAL_SIGN)\n",
        "            loc_EOS = equation.tolist().index(EOS)\n",
        "            input = equation[0 : loc_equal_sign + 1].view(1, -1).to(DEVICE)\n",
        "            ans = equation[: loc_EOS + 1].tolist()\n",
        "            ans_pred = compute_sum(model, input)\n",
        "            count += 1\n",
        "\n",
        "            if ans == ans_pred.tolist():\n",
        "                acc += 1\n",
        "            else:\n",
        "                if num_wrong_to_display > 0:\n",
        "                    print(\n",
        "                        f'correct equation: {decode_equation(equation).replace(\"<pad>\",\"\")}'\n",
        "                    )\n",
        "                    print(f\"predicted:        {decode_equation(ans_pred)}\")\n",
        "                    num_wrong_to_display -= 1\n",
        "        if num_batch and idx > num_batch:\n",
        "            break\n",
        "    return acc / count\n",
        "\n",
        "\n",
        "def what_is(question: str) -> str:\n",
        "    \"function for computing the sum of two numbers with input in literal string format\"\n",
        "    pred = compute_sum(model, encode_equation(question, num_digits).view(1, -1))\n",
        "    pred = decode_equation(pred)\n",
        "    pred = pred[pred.index(\"=\") + 1 :]\n",
        "    return question + pred\n",
        "\n",
        "@dataclass\n",
        "class OptimizerConfig:\n",
        "    pass\n",
        "\n",
        "def create_optimizer(model: torch.nn.Module, opt_config: OptimizerConfig):\n",
        "    \"\"\"\n",
        "    This long function is unfortunately doing something very simple and is being very defensive:\n",
        "    We are separating out all parameters of the model into two buckets: those that will experience\n",
        "    weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
        "    We are then returning the PyTorch optimizer object.\n",
        "    \"\"\"\n",
        "\n",
        "    # separate out all parameters to those that will and won't experience regularizing weight decay\n",
        "    decay = set()\n",
        "    no_decay = set()\n",
        "    whitelist_weight_modules = (torch.nn.Linear,)\n",
        "    blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
        "    for mn, m in model.named_modules():\n",
        "        for pn, p in m.named_parameters():\n",
        "            fpn = \"%s.%s\" % (mn, pn) if mn else pn  # full param name\n",
        "            # random note: because named_modules and named_parameters are recursive\n",
        "            # we will see the same tensors p many many times. but doing it this way\n",
        "            # allows us to know which parent module any tensor p belongs to...\n",
        "            if pn.endswith(\"bias\"):\n",
        "                # all biases will not be decayed\n",
        "                no_decay.add(fpn)\n",
        "            elif pn.endswith(\"weight\") and isinstance(m, whitelist_weight_modules):\n",
        "                # weights of whitelist modules will be weight decayed\n",
        "                decay.add(fpn)\n",
        "            elif pn.endswith(\"in_proj_weight\"):\n",
        "                # MHA projection layer\n",
        "                decay.add(fpn)\n",
        "            elif pn.endswith(\"weight\") and isinstance(m, blacklist_weight_modules):\n",
        "                # weights of blacklist modules will NOT be weight decayed\n",
        "                no_decay.add(fpn)\n",
        "            elif pn.endswith(\"pos_emb\"):\n",
        "                # positional embedding shouldn't be decayed\n",
        "                no_decay.add(fpn)\n",
        "\n",
        "    # validate that we considered every parameter\n",
        "    param_dict = {pn: p for pn, p in model.named_parameters()}\n",
        "    inter_params = decay & no_decay\n",
        "    union_params = decay | no_decay\n",
        "    assert (\n",
        "        len(inter_params) == 0\n",
        "    ), \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params),)\n",
        "    assert (\n",
        "        len(param_dict.keys() - union_params) == 0\n",
        "    ), \"parameters %s were not separated into either decay/no_decay set!\" % (\n",
        "        str(param_dict.keys() - union_params),\n",
        "    )\n",
        "\n",
        "    # create the pytorch optimizer object\n",
        "    optim_groups = [\n",
        "        {\n",
        "            \"params\": [param_dict[pn] for pn in sorted(list(decay))],\n",
        "            \"weight_decay\": opt_config.weight_decay,\n",
        "        },\n",
        "        {\n",
        "            \"params\": [param_dict[pn] for pn in sorted(list(no_decay))],\n",
        "            \"weight_decay\": 0.0,\n",
        "        },\n",
        "    ]\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        optim_groups, lr=opt_config.learning_rate, betas=(0.9, 0.95)\n",
        "    )\n",
        "    return optimizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVvd-RL-rdzd"
      },
      "source": [
        "1. `input` is indeed `[bs, 10]` because max len is 11, so removed last token.\n",
        "2. `target` should be `[bs, 10]` but left shifted of the real original input but somehow i got 11.\n",
        "3. Think of vocab size to be num classes in my classification problem. But the"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "M5TwyxE5rdzd",
        "outputId": "3310e8c6-328e-41e5-acc8-7a4b63ed5845"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [00:01<00:00, 17.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ep 0: train_loss: 2.10042, val_loss: 1.27964\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [00:01<00:00, 19.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ep 1: train_loss: 1.24153, val_loss: 1.11355\n"
          ]
        }
      ],
      "source": [
        "if DEBUG:\n",
        "    train_loss, val_loss = train(model, dataloaders, epochs=2)\n",
        "    # torch.save(model.state_dict(), 'model_debug.pt')\n",
        "    # model_debug = torch.load('./model_debug.pt')\n",
        "    # if are_both_models_same(model.state_dict(), model_debug):\n",
        "    #     print(\"Pass\")\n",
        "    # else:\n",
        "    #     print(\"Fail\")\n",
        "else:\n",
        "    train_loss, val_loss = train(model, dataloaders, epochs=30)\n",
        "\n",
        "    #torch.save(model.state_dict(), 'model_non_debug.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT_Nf9O4rdzd"
      },
      "source": [
        "```\n",
        "100%|██████████| 28/28 [00:03<00:00,  8.07it/s]\n",
        "ep 0: train_loss: 2.10012, val_loss: 1.28574\n",
        "100%|██████████| 28/28 [00:03<00:00,  7.26it/s]\n",
        "ep 1: train_loss: 1.23693, val_loss: 1.10578\n",
        "```\n",
        "\n",
        "```\n",
        "ep 0: train_loss: 2.10042, val_loss: 1.27964\n",
        "100%|██████████| 28/28 [00:01<00:00, 19.49it/s]\n",
        "ep 1: train_loss: 1.24153, val_loss: 1.11355\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "hDQqBukYrdze",
        "outputId": "89e6a5ed-c5f1-4b53-fac1-6528f8157c67"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "'break' outside loop (668683560.py, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[29], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
          ]
        }
      ],
      "source": [
        "break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ujaa9mSNrdze"
      },
      "outputs": [],
      "source": [
        "test_loss = validate(model, dataloaders.test_loader)\n",
        "print('training set examples the model gives an incorrect result:')\n",
        "train_acc = evaluate(model, dataloaders.train_loader, 20)\n",
        "print('validataion set examples the model gives an incorrect result:')\n",
        "val_acc = evaluate(model, dataloaders.test_loader)\n",
        "print('test set examples the model gives an incorrect result:')\n",
        "test_acc = evaluate(model, dataloaders.test_loader)\n",
        "result = f'''train_size: {train_size}, train_loss: {train_loss},\n",
        "                val_loss: {val_loss}, test_loss: {test_loss},\n",
        "                test_acc: {test_acc}, val_acc: {val_acc}, train_acc: {train_acc}\n",
        "                '''\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbLTWPIRrdze"
      },
      "source": [
        "non debug\n",
        "\n",
        "```\n",
        "correct equation: 24+86=110\n",
        "predicted:        24+86=100\n",
        "correct equation: 84+26=110\n",
        "predicted:        84+26=100\n",
        "validataion set examples the model gives an incorrect result:\n",
        "test set examples the model gives an incorrect result:\n",
        "train_size: 7000, train_loss: 0.013642309483007662,\n",
        "                val_loss: 0.0008140208410623018, test_loss: 0.00040599027124699205,\n",
        "                test_acc: 1.0, val_acc: 1.0, train_acc: 0.9996448863636364\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_9_'></a>[Adder Decoder Walkthrough](#toc0_)\n",
        "\n",
        "In a decoder-only model like GPT, the input sequence is essentially the target.\n",
        "The model aims to generate tokens that come after the given input, treating it\n",
        "as the \"history\" or \"context\" for the task of text generation. Unlike\n",
        "encoder-decoder models like the original Transformer, where the encoder\n",
        "processes a source sequence and the decoder generates a target sequence, a\n",
        "decoder-only model works solely with what would traditionally be considered the\n",
        "target sequence. Therefore, the padding mask applied to this input sequence is\n",
        "more aptly named \"target_padding_mask\" to maintain terminological consistency\n",
        "with the original Transformer architecture.\n",
        "\n",
        "Consequently, the input (source to beginners like me) padding masks is called\n",
        "the target padding masks for the following reasons:\n",
        "\n",
        "In a decoder-only architecture like GPT, the input sequence serves as the target\n",
        "sequence for which you want to generate subsequent tokens. Despite its role as\n",
        "an input to the model, it's termed as \"target\" because in the original\n",
        "Transformer architecture, the decoder's job is to generate the target sequence.\n",
        "Therefore, the mask that works on this input sequence in a decoder-only model\n",
        "should more aptly be named \"target_padding_mask.\" This naming maintains\n",
        "consistency with the Transformer architecture and clarifies that you're working\n",
        "on what is essentially the target of the model's generation task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_9_1_'></a>[Target Padding Mask (`target_padding_mask`)](#toc0_)\n",
        "\n",
        "- Definition: An attention mask to ignore pad-tokens in the source input. But in decoder only model, the source is the target.\n",
        "- Shape     : `(B, S)` or `(B, L)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yzuvux-rdzf",
        "outputId": "c5fea1d9-d062-401e-dbe1-69a552c62c66"
      },
      "outputs": [],
      "source": [
        "pad_token_id = 16\n",
        "target_batch = torch.tensor(\n",
        "    [\n",
        "        [5, 7, 9, 16, 16],\n",
        "        [8, 6, 16, 16, 16],\n",
        "        [3, 12, 4, 11, 16],\n",
        "        [2, 1, 4, 16, 16],\n",
        "    ]\n",
        ")\n",
        "\n",
        "batch_size, seq_len = target_batch.size()\n",
        "\n",
        "target_padding_mask = target_batch != pad_token_id\n",
        "\n",
        "pprint(target_padding_mask)\n",
        "pprint(target_padding_mask.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_9_2_'></a>[Future Mask (`future_mask`)](#toc0_)\n",
        "\n",
        "```\n",
        ":param future_mask:\n",
        ":shape            : (L, L)\n",
        ":note             : Independent of batch size?\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzGyd-9Hrdzf",
        "outputId": "bf72ecf6-b19b-4142-d096-2e4c522c93de"
      },
      "outputs": [],
      "source": [
        "seq_len = 5\n",
        "future_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
        "future_mask = future_mask == 0\n",
        "\n",
        "pprint(future_mask)\n",
        "pprint(future_mask.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGaeKY3Drdzf"
      },
      "source": [
        "One thing we need to know is that we need to do a matmul of `attention_weights` (emphasize the weights word here because it is indeed derived\n",
        "from weights although not explicit) and the value (the input seq). This attention weights has a preceding\n",
        "`attention_scores` prior to softmax, and we need to fill the tensors in this `attention_scores` with `-inf` because\n",
        "the softmax operation on `-inf` is zero, effectively zero out masked logits.\n",
        "\n",
        "Let's consider what zeroing out these masked logits actually does. The attention\n",
        "mechanism can be thought of as a weighted average of all the tokens in the input\n",
        "sequence. Each token is assigned a weight, with higher weights indicating more\n",
        "relevance to the token under consideration. If a certain token should not be\n",
        "considered at all (e.g., it's a future token that should not be visible to the\n",
        "current decoder step, or it's a padding token), its weight should be zero.\n",
        "\n",
        "In the case of a masked self-attention mechanism, as is often used in the\n",
        "decoder of a transformer, there are two main scenarios where masking comes into\n",
        "play:\n",
        "\n",
        "1. **Padding Tokens**: You don't want the attention mechanism to consider\n",
        "   padding tokens as they carry no useful information. If it did, it could skew\n",
        "   the resulting weighted average.\n",
        "\n",
        "2. **Future Tokens in Decoding**: In autoregressive decoding, the model\n",
        "   shouldn't have access to future tokens in the sequence when making\n",
        "   predictions. Otherwise, the model would cheat by peeking ahead.\n",
        "\n",
        "By setting the corresponding positions in the attention scores tensor to `-inf`\n",
        "and then applying a softmax, you effectively get a zero at those positions in\n",
        "the attention weights tensor. This results in completely ignoring those tokens\n",
        "when taking the weighted sum of the value vectors, thus implementing the desired\n",
        "masking behavior.\n",
        "\n",
        "To summarize, zeroing out masked logits ensures that the tokens corresponding to\n",
        "those logits do not contribute to the computed context, whether because they are\n",
        "padding or because they are future tokens that should not be visible to the\n",
        "model at a given time step.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG7eD_Ucrdzg"
      },
      "source": [
        "The purpose of applying `logical_and` between `target_padding_mask` and `future_mask` is to combine the constraints from both masks when calculating self-attention scores in the transformer's decoder. The `target_padding_mask` is designed to mask out the padding tokens in the input sequence, while the `future_mask` ensures that a given position cannot attend to future positions in the sequence. By combining these masks, you can perform the necessary masking for both padding and future tokens in a single step.\n",
        "\n",
        "Here's how it works:\n",
        "\n",
        "1. `target_padding_mask`: Masks out the padding tokens so that they don't contribute to the attention calculations. True values mean \"attend to this token,\" and False values mean \"ignore this token.\"\n",
        "  \n",
        "2. `future_mask`: The future mask is created as a lower triangular matrix, where the lower triangle, including the diagonal, is filled with ones, and the upper triangle is filled with zeros. Masks out future tokens in a sequence so that a token at a given position can only attend to positions that come before it (and itself). True values mean \"attend to this token,\" and False values mean \"ignore this token.\"\n",
        "\n",
        "3. `logical_and(target_padding_mask, future_mask)`: Combines the two masks. A True in the resulting mask means that the condition for both padding and future attention is satisfied.\n",
        "\n",
        "By combining these two masks, the decoder obeys the autoregressive property, ensuring it doesn't see future tokens, while also ignoring padding tokens in the input sequence. We may term it the `target_mask`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n7fIAcdrdzg"
      },
      "source": [
        "Same mask applied to all h heads.\n",
        "Same mask applied to all h heads.\n",
        "Same mask applied to all h heads.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_9_3_'></a>[Example of Source Padding and Future Masks](#toc0_)\n",
        "\n",
        "#### <a id='toc1_9_3_1_'></a>[First Sample First Token](#toc0_)\n",
        "\n",
        "- `target_padding_mask` has size of `[4, 5]`.\n",
        "  - We zoom in to the first row (sample) which is of length 5.\n",
        "  - This length 5 is the sequence length, which is `T, T, T, F, F` indicating the last 2 tokens being padded.\n",
        "- `future_mask` has size of `[5, 5]`.\n",
        "  - We note that this is indepedent of batch size. Each sample should have the same future mask shape of `[L, L]`.\n",
        "  - This `L=5` should necessary be same for the sequence length in `target_padding_mask`.\n",
        "- First, let's consider one batch of 4 samples. What we do first is to broadcast `future_mask` to `[4, 5, 5]` because we want each sample/row in the batch to have the same future mask. As shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iV9EB14Srdzg",
        "outputId": "d6b1f799-ac60-4546-e82e-2d0898a05c40"
      },
      "outputs": [],
      "source": [
        "pprint(future_mask)\n",
        "future_mask = future_mask.view(1, seq_len, seq_len).expand(size=(batch_size, -1, -1))\n",
        "pprint(future_mask)\n",
        "pprint(future_mask.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0Z8Cy-urdzh"
      },
      "source": [
        "- Now, we can zoom in to one particular sample since both `target_padding_mask` and `future_mask` have the same first dimension of batch size.\n",
        "- What is incomplete is that we need to broadcast `target_padding_mask`'s last dimension to have the same dimensions as `future_mask`. This means we broadcast `[4, 5]` to `[4, 5, 5]`. But why?\n",
        "- For simplicity, we slice the first same of both below.\n",
        "- The first row of the `future_mask` of the first sample is `T, F, F, F, F`. This corresponds to what? This is the future mask of the first token in the sequence. Well, that is confusing, because it apparently have 5 elements, and has \"information\" of the other 4 tokens in the sequence. Let's explain in details below:\n",
        "  - Regarding the first row of the `future_mask` in the first sample, which is `[T, F, F, F, F]`, it might initially seem confusing why there are 5 elements. Each of these elements, in fact, corresponds to whether the first token can attend to other tokens at each respective position in the sequence. Here's how to interpret it:\n",
        "    - The first element (`True`) indicates that the first token can attend to itself.\n",
        "    - The next four elements (`False`) specify that the first token should not attend to any of the future tokens in the sequence.\n",
        "- Consequently, what is the first token in the sequence of the `target_padding_mask`? Recall earlier we mentioned that the first sample's `target_padding_mask` is `T, T, T, F, F` and therefore the first token in the sequence is `T`.\n",
        "- What do we want to achieve here? We want to make sure that the model does not **attend** to tokens in the sequence that are masked with `False`.\n",
        "- In other words, the first token in the sequence of the first sample has `target_padding_mask` of `T` and `future_masks` of `T, F, F, F, F`.\n",
        "- We need to broadcast this `T` to `T, T, T, T, T` to align with `T, F, F, F, F` because? Because we need ensure that this first token in the sequence is also able to considered in relation to every other token in the sequence.\n",
        "- So the first token is not a padded token, which is `T`, similarly, the first token needs to attend to itself at the first position, hence `T` and `T` give `T`. But for the second `T` in the now broadcasted `target_padding_mask`, it is still representing the first token or?\n",
        "- Broadcasting the first token's `target_padding_mask` value of `T` to `[T, T, T, T, T]` ensures that when this first token is being considered for attention computations, it is free to attend to any position, barring any restrictions set by `future_mask`.\n",
        "- Tricky: after broadcasting, each `T` in `[T, T, T, T, T]` is still representing the first token. They indicate that when the first token is compared with *any* token in the sequence (including itself), it is not a padding token. The element-wise `AND` with the `future_mask` then further refines this by restricting it from attending to future tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqRW6JMSrdzh",
        "outputId": "bb448720-8cad-45dd-ad35-e4631737542e"
      },
      "outputs": [],
      "source": [
        "pprint(target_padding_mask)\n",
        "pprint(target_padding_mask[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDoSC5wOrdzh",
        "outputId": "5d2d209b-2edd-4af0-d3d4-4a50f7fb105d"
      },
      "outputs": [],
      "source": [
        "pprint(target_padding_mask)\n",
        "target_padding_mask = target_padding_mask.view(batch_size, 1, seq_len).expand(size=(batch_size, seq_len, seq_len))\n",
        "pprint(target_padding_mask)\n",
        "pprint(target_padding_mask.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcXfdJ7Frdzh",
        "outputId": "ba0594ea-1472-48dc-9735-ab7ba0d4f641"
      },
      "outputs": [],
      "source": [
        "pprint(target_padding_mask[0])\n",
        "pprint(future_mask[0])\n",
        "pprint(target_padding_mask[0] & future_mask[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <a id='toc1_9_3_2_'></a>[First Sample Fourth Token](#toc0_)\n",
        "\n",
        "Now let's look at another example—the 4th token in the sequence, where `target_padding_mask = [T, T, T, F, F]` and `future_mask` is a lower triangular matrix with `True`s.\n",
        "\n",
        "1. **4th Token's target_padding_mask**: The 4th token has a value of `F` in `target_padding_mask`, indicating it's a padding token.\n",
        "   \n",
        "2. **4th Row of future_mask**: The 4th row in `future_mask` is `[True, True, True, True, False]`. This means that if this token were not a padding token, it would be allowed to attend to all the previous tokens in the sequence and itself, but not to any future token.\n",
        "\n",
        "3. **Broadcast target_padding_mask**: To align `target_padding_mask` with `future_mask`, we'd broadcast `F` from the `target_padding_mask` to `[F, F, F, F, F]`. This way, when we consider the 4th token in relation to any other token in the sequence, it's still marked as a padding token.\n",
        "\n",
        "4. **Element-wise AND with future_mask**: After broadcasting, you'd perform an element-wise AND between `[F, F, F, F, F]` and `[True, True, True, True, False]`, resulting in `[F, F, F, F, F]`.\n",
        "\n",
        "5. **Interpretation**: This effectively means that the 4th token won't attend to any other token in the sequence, and no token will attend to it either, as it is a padding token.\n",
        "\n",
        "So, the masks are doing their jobs correctly: the `target_padding_mask` indicates whether each token is a padding token or not, and `future_mask` dictates the \"rules\" of attention regarding what each token can attend to. Combining them ensures that both conditions are met."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_9_4_'></a>[Further Add a Singleton Dimension in Masks](#toc0_)\n",
        "\n",
        "Now both masks are of shape: `(B, L, L)` but we need to add a singleton dimension to the last dimension to make it `(B, 1, L, L)`.\n",
        "\n",
        "In deep learning frameworks like PyTorch, the dimensions of the tensors involved\n",
        "in operations like matrix multiplication or attention mechanisms often have\n",
        "specific semantic meanings. In the context of attention mechanisms, especially\n",
        "in the transformer architecture, the attention mask usually has a shape that is\n",
        "compatible with the attention logits for element-wise multiplication.\n",
        "\n",
        "In the transformer model, the attention logits are often computed as a dot\n",
        "product between query and key vectors, resulting in a tensor of shape\n",
        "`(Batch size, Num heads, Sequence length, Sequence length)` or `(B, H, L, L)`.\n",
        "Here, `B` is the batch size, `H` is the number of attention heads, and `L` is\n",
        "the sequence length.\n",
        "\n",
        "To make the mask tensor compatible for element-wise operations with this 4D\n",
        "tensor, it needs to have a shape that can be broadcasted to `(B, H, L, L)`. A\n",
        "mask of shape `(B, 1, L, L)` fulfills this requirement.\n",
        "\n",
        "The singleton dimension is added so that the mask can be easily broadcast to the\n",
        "shape of the attention logits tensor during the computation. When a tensor with\n",
        "shape `(B, 1, L, L)` is element-wise multiplied with a tensor of shape\n",
        "`(B, H, L, L)`, the singleton dimension (the `1`) allows the mask to be used for\n",
        "each attention head without explicitly replicating the mask `H` times. This is\n",
        "more memory-efficient and often faster.\n",
        "\n",
        "Thus, adding a singleton dimension in masks is a preparatory step that allows\n",
        "for efficient element-wise operations later in the model's forward pass.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_8uVfTRrdzi",
        "outputId": "45261936-f619-4d6b-c4f8-5f5a0566870a"
      },
      "outputs": [],
      "source": [
        "target_padding_mask = target_padding_mask.unsqueeze(1)\n",
        "pprint(target_padding_mask.shape)\n",
        "\n",
        "future_mask = future_mask.unsqueeze(1)\n",
        "pprint(future_mask.shape)\n",
        "\n",
        "target_mask = target_padding_mask & future_mask\n",
        "pprint(target_mask.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_9_5_'></a>[MultiHeadAttention](#toc0_)\n",
        "\n",
        "We start off by understanding the rationale of the following block:\n",
        "\n",
        "```python\n",
        "Q = self.W_Q(query).contiguous() # Z @ W_Q -> BxLxD @ DxD = BxLxD\n",
        "K = self.W_K(key).contiguous()   # Z @ W_K\n",
        "V = self.W_V(value).contiguous() # Z @ W_V\n",
        "```\n",
        "\n",
        "#### <a id='toc1_9_5_1_'></a>[A Primer](#toc0_)\n",
        "\n",
        "In the context of the Transformer architecture and self-attention mechanism, the\n",
        "matrices $\\mathbf{W}^{Q}, \\mathbf{W}^{K},$ and $\\mathbf{W}^{V}$ are learnable\n",
        "parameters designed to project the input embeddings $\\mathbf{Z}$ into distinct\n",
        "subspaces tailored for attention calculations. Let's explore their purpose and\n",
        "their resulting transformations:\n",
        "\n",
        "1. **The Role of Weights**:\n",
        "\n",
        "   - $\\mathbf{W}^{Q}$: Projects input embeddings into a query subspace,\n",
        "     determining the type of information each token seeks from others.\n",
        "   - $\\mathbf{W}^{K}$: Positions the embeddings in a key subspace, highlighting\n",
        "     the token features that others would search for.\n",
        "   - $\\mathbf{W}^{V}$: Transforms embeddings into a value subspace, showcasing\n",
        "     the actual token content to be aggregated by the attention scores.\n",
        "\n",
        "2. **Intuitive & Mathematical Interpretations**:\n",
        "\n",
        "   - **Query Transformation** ($\\mathbf{Z} \\mathbf{W}^{Q}$): Intuitively, it\n",
        "     tailors the raw embeddings to optimally question the rest of the sequence.\n",
        "     Mathematically, it's a linear transformation of the embedding space into\n",
        "     the query space, akin to a high-dimensional rotation and scaling,\n",
        "     emphasizing aspects relevant to querying.\n",
        "\n",
        "   - **Key Transformation** ($\\mathbf{Z} \\mathbf{W}^{K}$): Intuitively, it\n",
        "     accentuates token features that other tokens might seek. Mathematically,\n",
        "     it's another linear transformation emphasizing aspects that make tokens\n",
        "     searchable.\n",
        "\n",
        "   - **Value Transformation** ($\\mathbf{Z} \\mathbf{W}^{V}$): Intuitively, it\n",
        "     prepares tokens to share their intrinsic content when beckoned by the\n",
        "     attention mechanism. Mathematically, it's a linear transformation\n",
        "     accentuating token content aspects.\n",
        "\n",
        "3. **Creating Q, K, V**:\n",
        "\n",
        "   - $\\mathbf{Q} = \\mathbf{Z} \\mathbf{W}^{Q}$\n",
        "   - $\\mathbf{K} = \\mathbf{Z} \\mathbf{W}^{K}$\n",
        "   - $\\mathbf{V} = \\mathbf{Z} \\mathbf{W}^{V}$\n",
        "\n",
        "   These operations recast the embedded tokens into roles for the attention\n",
        "   mechanism:\n",
        "\n",
        "   - $\\mathbf{Q}$: Information seekers. The queries are seeking information, and\n",
        "     the computation $Q @ K^T$ finds how much each part of the input (holder)\n",
        "     should be attended to.\n",
        "   - $\\mathbf{K}$: Information gatekeepers. The keys hold the information being\n",
        "     sought, and their arrangement in space defines the subspace that the\n",
        "     queries are projected onto to find these relevance scores.\n",
        "   - $\\mathbf{V}$: Information providers. The values contain the content that\n",
        "     needs to be retrieved, and once we have the attention weights, we know how\n",
        "     much of each value to retrieve and combine to form the output.\n",
        "\n",
        "   Mathematically, the resulting matrices ($\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}$)\n",
        "   have rows that represent different aspects (querying, key, value) of the\n",
        "   original tokens.\n",
        "\n",
        "4. **Relevance to Self-Attention**:\n",
        "\n",
        "   The transformations set the stage for attention score calculations. In this\n",
        "   step, each query vector in $\\mathbf{Q}$ computes its similarity (via dot\n",
        "   product) against all key vectors in $\\mathbf{K}$. This score matrix reveals\n",
        "   the attention weightage for each token regarding every other token in the\n",
        "   sequence.\n",
        "\n",
        "   Specifically, $\\mathbf{Q} @ \\mathbf{K}^T$ calculates how each token (query)\n",
        "   aligns with every other token (key). It's akin to measuring the relevance of\n",
        "   each word to every other word in the sequence.\n",
        "\n",
        "   After normalizing these scores (typically with softmax), we get the attention\n",
        "   weights. These weights guide how the value vectors in $\\mathbf{V}$ are\n",
        "   aggregated. The outcome is a new matrix where each row aggregates\n",
        "   contextually relevant information from the entire sequence. This enriched\n",
        "   output feeds into subsequent transformer layers for further processing.\n",
        "\n",
        "Overall, by using the $\\mathbf{W}^{Q}, \\mathbf{W}^{K},$ and $\\mathbf{W}^{V}$\n",
        "matrices, the transformer fine-tunes its focus on inter-token relationships,\n",
        "enabling the model to capture intricate contextual nuances within a given\n",
        "sequence.\n",
        "\n",
        "#### <a id='toc1_9_5_2_'></a>[An Example](#toc0_)\n",
        "\n",
        "Let's use the sentence \"The cat walks by the bank\" to walk through the\n",
        "self-attention mechanism with analogies and to clarify how it works step by\n",
        "step.\n",
        "\n",
        "**Setting the Scene (Embedding the Sentence):** Imagine each word in the\n",
        "sentence is a person at a party (our tokens). They start by telling a basic fact\n",
        "about themselves (their initial embedding).\n",
        "\n",
        "**The Roles:**\n",
        "\n",
        "- **Q (Seekers)**: Each person (word) is curious about the stories (contexts) of\n",
        "  others at the party. They have their own perspective or question (Q vector).\n",
        "- **K (Holders)**: At the same time, each person has a name tag with keywords\n",
        "  that describe their story (K vector).\n",
        "- **V (Retrievers)**: They also hold a bag of their experiences (V vector),\n",
        "  ready to share.\n",
        "\n",
        "**Transformations (Applying W Matrices):** We give each person a set of glasses\n",
        "(the matrices $W_Q, W_K, W_V$) that changes how they see the world (the space\n",
        "they project to).\n",
        "\n",
        "- With $W_Q$ glasses, they focus on what they want to know from others.\n",
        "- With $W_K$ glasses, they highlight their name tag details, making some\n",
        "  features stand out more.\n",
        "- With $W_V$ glasses, they prepare to share the contents of their bag\n",
        "  effectively.\n",
        "\n",
        "**Attention (Calculating Q @ K.T):** Now, each person looks around the room\n",
        "(sequence) with their $W_Q$ glasses and sees the highlighted name tags (after\n",
        "$W_K$ transformation) of everyone else. They measure how similar their question\n",
        "is to the others' name tags—this is the dot product $Q @ K^T$.\n",
        "\n",
        "For \"cat,\" let’s say it’s curious about the notion of \"walking\" and \"bank.\" It\n",
        "will measure the similarity (attention scores) between its curiosity and the\n",
        "name tags of \"walks,\" \"by,\" \"the,\" \"bank.\"\n",
        "\n",
        "**Normalization (Softmax):** After measuring, \"cat\" decides how much to focus on\n",
        "each story—this is softmax. Some stories are very relevant (\"walks\"), some\n",
        "moderately (\"by,\" \"the\"), and some might be highly relevant depending on context\n",
        "(\"bank\" — is it a river bank or a financial institution?).\n",
        "\n",
        "**Retrieval (Applying Attention to V):** Now \"cat\" decides to listen to the\n",
        "stories in proportion to its focus. It takes pieces (weighted by attention\n",
        "scores) from each person's experience bag (V vectors) and combines them into a\n",
        "richer, contextual understanding of itself in the sentence. This combination\n",
        "gives us the new representation of \"cat,\" informed by the entire context of the\n",
        "sentence.\n",
        "\n",
        "In essence:\n",
        "\n",
        "- **Q (Query):** What does \"cat\" want to know?\n",
        "- **K (Key):** Who has relevant information to \"cat\"’s curiosity?\n",
        "- **V (Value):** What stories does \"cat\" gather from others, and how much does\n",
        "  it take from each to understand its role in the sentence?\n",
        "\n",
        "The output of self-attention for \"cat\" now encapsulates not just \"cat\" but its\n",
        "relationship and relevance to \"walks,\" \"by,\" \"the,\" \"bank\" in a way that no\n",
        "single word could convey alone. This output then becomes the input to the next\n",
        "layer, where the process can repeat, enabling the model to develop an even more\n",
        "nuanced understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"transformer.png\" width=\"600\">\n",
        "\n",
        "\n",
        "### <a id='toc1_9_6_'></a>[AddNorm (Residual Connection + Layer Normalization)](#toc0_)\n",
        "\n",
        "- https://www.d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html#residual-connection-and-layer-normalization\n",
        "- https://nlp.seas.harvard.edu/annotated-transformer\n",
        "\n",
        "#### <a id='toc1_9_6_1_'></a>[Residual Block](#toc0_)\n",
        "\n",
        "A residual block takes an input $X$ and a sub-layer (or function) $f$, and computes $X + f(X)$.\n",
        "\n",
        "```python\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        sublayer: Callable[[torch.Tensor], torch.Tensor],\n",
        "    ) -> torch.Tensor:\n",
        "        return x + sublayer(x)\n",
        "```\n",
        "\n",
        "The intuition behind a residual block is to facilitate the training of deeper networks by providing a \"shortcut\" or \"skip connection\" that allows the gradient to be directly backpropagated to earlier layers. Essentially, in a standard deep learning model, each layer transforms its input. As the network depth increases, these transformations can degrade the network's performance, mainly due to the vanishing or exploding gradient problems. This makes it challenging to train very deep networks.\n",
        "\n",
        "The residual block aims to address this problem. It adds the original input back to the output of the network layer, forming $F(x) + x$ instead of just $F(x)$. Mathematically, if $x$ is the input and $F(x)$ is the transformed version, then the residual block computes $F(x) + x$.\n",
        "\n",
        "This architecture has a few advantages:\n",
        "\n",
        "1. **Easier Learning**: During training, if the best transformation is an identity map (i.e., the output should be the same as the input), the residual block can easily learn this. The layers in $F(x)$ only need to learn to approximate zero in this case, which is generally easier than learning an identity map in a traditional stack of layers.\n",
        "\n",
        "2. **Mitigating Vanishing/Exploding Gradients**: The skip connections provide an unobstructed path for the gradients to flow, which can help mitigate the vanishing or exploding gradient problems in very deep networks.\n",
        "\n",
        "3. **Enabling Deeper Networks**: Because of the above advantages, residual blocks make it possible to train very deep networks effectively. Deep networks can represent very complex functions, which can be advantageous for many tasks.\n",
        "\n",
        "4. **Parameter Efficiency**: Residual blocks often require fewer parameters to achieve similar performance compared to traditional deep networks, making them more parameter-efficient.\n",
        "\n",
        "In summary, the residual block is a simple yet effective idea that has enabled the training of much deeper networks, thereby pushing the boundaries of what is achievable in various machine learning tasks.\n",
        "\n",
        "#### <a id='toc1_9_6_2_'></a>[Layer Normalization](#toc0_)\n",
        "\n",
        "Layer normalization normalizes the features across the feature dimension. Given the feature $X$ with shape $[B, L, D]$ (where $B$ is the batch size, $L$ is the sequence length, and $D$ is the feature dimension), layer normalization computes:\n",
        "\n",
        "$$\n",
        "\\text{Norm}(X) = \\frac{X - \\text{mean}(X)}{\\sqrt{\\text{var}(X) + \\epsilon}} \\times \\gamma + \\beta\n",
        "$$\n",
        "\n",
        "Where $\\gamma$ and $\\beta$ are learnable parameters and $\\epsilon$ is a small constant for numerical stability.\n",
        "\n",
        "```python\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, feature_dim: int, eps: float = 1e-6) -> None:\n",
        "        super().__init__()\n",
        "        # fmt: off\n",
        "        self.gamma = nn.Parameter(torch.ones(feature_dim))\n",
        "        self.beta  = nn.Parameter(torch.zeros(feature_dim))\n",
        "        self.eps   = eps\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std  = x.std(dim=-1, keepdim=True)\n",
        "        # fmt: on\n",
        "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
        "```\n",
        "\n",
        "#### <a id='toc1_9_6_3_'></a>[Combining Both](#toc0_)\n",
        "\n",
        "Finally, you can combine these into a single block, much like the `ResidualConnection` or `AddNorm` classes you mentioned earlier.\n",
        "\n",
        "```python\n",
        "class AddNorm(nn.Module):\n",
        "    def __init__(self, feature_dim, dropout_rate):\n",
        "        super(AddNorm, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.layer_norm = LayerNorm(feature_dim)\n",
        "\n",
        "    def forward(self, x, sublayer_output):\n",
        "        return self.layer_norm(x + self.dropout(sublayer_output))\n",
        "```\n",
        "\n",
        "This `AddNorm` class applies dropout to the output of the sub-layer, adds it to the original input, and then applies layer normalization. Note that this version doesn't include an embedded layer normalization operation in the residual block; instead, it utilizes a separate layer normalization class, which is then used in the `AddNorm` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How Loss is Computed?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming we have B = batch size, L = sequence length, V = vocab size\n",
        "B, L, V = 2, 3, 4  # Example dimensions\n",
        "\n",
        "# Instantiate the CrossEntropyLoss\n",
        "# By default, it reduces by averaging the losses over each observation in the input\n",
        "criterion = nn.CrossEntropyLoss(reduction=\"mean\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.9269</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.4873</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9007</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.1055</span><span style=\"font-weight: bold\">]</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6784</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.2345</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0431</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.6047</span><span style=\"font-weight: bold\">]</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3559</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.6866</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.4934</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2415</span><span style=\"font-weight: bold\">]]</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.1109</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0915</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.3169</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2168</span><span style=\"font-weight: bold\">]</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.3097</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.3957</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8034</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.6216</span><span style=\"font-weight: bold\">]</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.5920</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0631</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.8286</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3309</span><span style=\"font-weight: bold\">]]])</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1.9269\u001b[0m,  \u001b[1;36m1.4873\u001b[0m,  \u001b[1;36m0.9007\u001b[0m, \u001b[1;36m-2.1055\u001b[0m\u001b[1m]\u001b[0m,\n",
              "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.6784\u001b[0m, \u001b[1;36m-1.2345\u001b[0m, \u001b[1;36m-0.0431\u001b[0m, \u001b[1;36m-1.6047\u001b[0m\u001b[1m]\u001b[0m,\n",
              "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.3559\u001b[0m, \u001b[1;36m-0.6866\u001b[0m, \u001b[1;36m-0.4934\u001b[0m,  \u001b[1;36m0.2415\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m,\n",
              "\u001b[2;32m│   │   \u001b[0m\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-1.1109\u001b[0m,  \u001b[1;36m0.0915\u001b[0m, \u001b[1;36m-2.3169\u001b[0m, \u001b[1;36m-0.2168\u001b[0m\u001b[1m]\u001b[0m,\n",
              "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.3097\u001b[0m, \u001b[1;36m-0.3957\u001b[0m,  \u001b[1;36m0.8034\u001b[0m, \u001b[1;36m-0.6216\u001b[0m\u001b[1m]\u001b[0m,\n",
              "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.5920\u001b[0m, \u001b[1;36m-0.0631\u001b[0m, \u001b[1;36m-0.8286\u001b[0m,  \u001b[1;36m0.3309\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">]])</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.9269</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.4873</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9007</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.1055</span><span style=\"font-weight: bold\">]</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6784</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.2345</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0431</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.6047</span><span style=\"font-weight: bold\">]</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3559</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.6866</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.4934</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2415</span><span style=\"font-weight: bold\">]])</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1.9269\u001b[0m,  \u001b[1;36m1.4873\u001b[0m,  \u001b[1;36m0.9007\u001b[0m, \u001b[1;36m-2.1055\u001b[0m\u001b[1m]\u001b[0m,\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.6784\u001b[0m, \u001b[1;36m-1.2345\u001b[0m, \u001b[1;36m-0.0431\u001b[0m, \u001b[1;36m-1.6047\u001b[0m\u001b[1m]\u001b[0m,\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.3559\u001b[0m, \u001b[1;36m-0.6866\u001b[0m, \u001b[1;36m-0.4934\u001b[0m,  \u001b[1;36m0.2415\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">])</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Randomly generated logits and target for illustration\n",
        "# Logits are typically obtained from the last linear layer of your model\n",
        "logits = torch.randn(B, L, V, generator=torch.Generator().manual_seed(SEED))\n",
        "targets = torch.randint(low=0, high=V, size=(B, L), generator=torch.Generator().manual_seed(SEED))  # Randomly generated target indices\n",
        "\n",
        "pprint(logits)\n",
        "pprint(targets)\n",
        "pprint(logits[0]) # logits for the first sequence [L=10, V=18]\n",
        "pprint(targets[0]) # target for the first sequence [L=10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Permute logits to shape [B, V, S]\n",
        "logits_permuted = logits.permute(0, 2, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.7008</span><span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1.7008\u001b[0m\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "loss = criterion(logits_permuted, targets)\n",
        "pprint(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.9269</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.4873</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9007</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.1055</span><span style=\"font-weight: bold\">])</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1.9269\u001b[0m,  \u001b[1;36m1.4873\u001b[0m,  \u001b[1;36m0.9007\u001b[0m, \u001b[1;36m-2.1055\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "first_sequence_logits = logits[0]\n",
        "first_sequence_targets = targets[0]\n",
        "\n",
        "first_sequence_first_token_logits = first_sequence_logits[0]\n",
        "first_sequence_first_token_target = first_sequence_targets[0]\n",
        "\n",
        "pprint(first_sequence_first_token_logits)\n",
        "pprint(first_sequence_first_token_target)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.9269</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.4873</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9007</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.1055</span><span style=\"font-weight: bold\">]])</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1.9269\u001b[0m,  \u001b[1;36m1.4873\u001b[0m,  \u001b[1;36m0.9007\u001b[0m, \u001b[1;36m-2.1055\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">])</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6784</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.2345</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0431</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.6047</span><span style=\"font-weight: bold\">]])</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.6784\u001b[0m, \u001b[1;36m-1.2345\u001b[0m, \u001b[1;36m-0.0431\u001b[0m, \u001b[1;36m-1.6047\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">])</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3559</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.6866</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.4934</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2415</span><span style=\"font-weight: bold\">]])</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.3559\u001b[0m, \u001b[1;36m-0.6866\u001b[0m, \u001b[1;36m-0.4934\u001b[0m,  \u001b[1;36m0.2415\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">])</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.1109</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0915</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.3169</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2168</span><span style=\"font-weight: bold\">]])</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-1.1109\u001b[0m,  \u001b[1;36m0.0915\u001b[0m, \u001b[1;36m-2.3169\u001b[0m, \u001b[1;36m-0.2168\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">])</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.3097</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.3957</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8034</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.6216</span><span style=\"font-weight: bold\">]])</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.3097\u001b[0m, \u001b[1;36m-0.3957\u001b[0m,  \u001b[1;36m0.8034\u001b[0m, \u001b[1;36m-0.6216\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">])</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.5920</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0631</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.8286</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3309</span><span style=\"font-weight: bold\">]])</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.5920\u001b[0m, \u001b[1;36m-0.0631\u001b[0m, \u001b[1;36m-0.8286\u001b[0m,  \u001b[1;36m0.3309\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">])</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "total_loss = 0\n",
        "\n",
        "for i in range(B):\n",
        "    for j in range(L):\n",
        "        pprint(logits[i, j].unsqueeze(0))\n",
        "        pprint(targets[i, j].unsqueeze(0))\n",
        "        total_loss += criterion(logits[i, j].unsqueeze(0), targets[i, j].unsqueeze(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_loss /= (B * L)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.7008)"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_10_'></a>[Potential to use Module Dict?](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taBKOEM1rdzm"
      },
      "outputs": [],
      "source": [
        "class ModelModuleDict(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleDict({\n",
        "            'fc1': nn.Linear(2, 5),\n",
        "            'relu': nn.ReLU(),\n",
        "            'fc2': nn.Linear(5, 1)\n",
        "        })\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "# Initialize a random tensor as input\n",
        "input_tensor = torch.randn(1, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpkfNuQMrdzm"
      },
      "outputs": [],
      "source": [
        "seed_all(1, seed_torch=True)\n",
        "model_sequential = nn.Sequential(\n",
        "    nn.Linear(2, 5),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(5, 1)\n",
        ")\n",
        "# Forward pass using nn.Sequential model\n",
        "model_sequential(input_tensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_HGZqyDrdzm"
      },
      "outputs": [],
      "source": [
        "seed_all(1, seed_torch=True)\n",
        "model_moduledict   = ModelModuleDict()\n",
        "model_moduledict(input_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_11_'></a>[Training with GPT-like Model](#toc0_)\n",
        "\n",
        "If you're working with a GPT-like model, which is a decoder-only architecture, the training mechanics differ slightly compared to the encoder-decoder models like seq2seq. In a GPT-style model, the entire sequence (input and output) is provided to the model at once, and each token is predicted based on the tokens that came before it. The model is still autoregressive, but there's no separate encoder to produce an intermediate representation; the \"encoding\" is effectively built into the ongoing autoregressive decoding process.\n",
        "\n",
        "In your case, if the equations are like `90+38=128`, during training you'd provide `90+38=` as the input and then use the remaining part `128` as the expected output, potentially along with special tokens to demarcate sequence boundaries or to flag the equation/result parts. However, unlike an encoder-decoder model where the decoder gets to \"peek\" at the correct output during training (also known as \"teacher forcing\"), here every token in the output is predicted one by one, based solely on the preceding tokens.\n",
        "\n",
        "In such a setup, you can definitely feed the entire equation to the model and try to predict each subsequent token based on the preceding tokens. For example, given `90+38=`, the model should predict `1`, `2`, `8` in succession.\n",
        "\n",
        "### <a id='toc1_11_1_'></a>[Loss Computation](#toc0_)\n",
        "\n",
        "For training a GPT-like model, you'd usually use a standard loss function like cross-entropy loss for each token's prediction. You'd compare the token predicted by the model to the actual token in the target sequence to compute the loss. This is calculated for each token and then averaged over the sequence or batch, depending on your implementation.\n",
        "\n",
        "### <a id='toc1_11_2_'></a>[Example](#toc0_)\n",
        "\n",
        "In a GPT-like model, each token in the sequence is used to predict the next token. The model takes a sequence of tokens and produces a new sequence of the same length where each new token is predicted based on all the preceding tokens in the input sequence. The loss is then computed between the predicted sequence and the target sequence.\n",
        "\n",
        "Let's take a closer look at an example:\n",
        "\n",
        "- The original tensor: `[15, 9, 0, 10, 3, 8, 13, 1, 2, 8, 14]` which corresponds to `<SOS>90+38=128<EOS>`\n",
        "- Input tensor:  `[15, 9,  0,  10, 3,  8,  13, 1,  2, 8]`, which corresponds to `<SOS>90+38=128` without `EOS`\n",
        "- Target tensor:     `[9,  0,  10, 3,  8,  13, 1,  2,  8, 14]`\n",
        "                     `[16, 16, 16, 16, 16, 16, 1,  2,  8, 14]`\n",
        "\n",
        "During training:\n",
        "\n",
        "1. **First Timestep**: The model takes `[15]` (or `[<BOS>]` if 15 is your BOS token) and tries to predict the next token. Ideally, it should predict `9`. But here, your target sequence starts with masked tokens (`16`, if 16 is your masking token). So the loss is computed between the predicted token and the masked token `16`. But since `CrossEntropyLoss` has an `ignore_index` (now you know what they are right!), you can set it to say `16` or (default `-1` but you would need to change padding number) and tell the model that whenever the ground truth is `16`, the loss\n",
        "is zeroed out so it is not counted? This allows the model to focus on learning from the relevant parts of the sequence while ignoring the masked portions.\n",
        "\n",
        "2. **Second Timestep**: The model takes `[15, 9]` and predicts the next token, which should be `0`. Again, the target is a masked token `16`.\n",
        "\n",
        "3. **...**\n",
        "\n",
        "4. **Eighth Timestep**: The model takes `[15, 9,  0,  10, 3,  8,  13]` (which is `90+38=`) and predicts the next token. Now the target is `1`, so the loss is computed between the predicted token and `1`. There is no mask anymore here, so the loss will be computed.\n",
        "5. **Ninth Timestep**: The model takes `[15, 9,  0,  10, 3,  8,  13, 1]` (which is `90+38=1`) and predicts the next token. Now the target is `2`, so the loss is computed between the predicted token and `2`.\n",
        "   1. Here's an important thing for beginners (me), In a typical GPT-like architecture used for sequence-to-sequence tasks like this one, the model doesn't use its own predictions as input during training. Instead, it uses the original, ground-truth input sequence. This is known as \"teacher forcing.\" In teacher forcing, even if the model predicts a wrong token at some timestep, it doesn't affect the input sequence for subsequent timesteps. The model continues to get the original input sequence for the entire training epoch.\n",
        "   2. So if model predicts a `3` during the eighth timestep, where the ground trut is `1`, the model would simply incur a higher loss for that prediction. However, the input for the ninth timestep would still be the ground truth sequence up to that point, regardless of what the model predicted at the eighth timestep.\n",
        "   3. But it is noted that this behaviour is still autoregressive.\n",
        "6. **Tenth**: The model takes `[15, 9,  0,  10, 3,  8,  13, 1, 2]` and predicts the next token which is `8`.\n",
        "7. **Last**: The model takes `[15, 9,  0,  10, 3,  8,  13, 1, 2, 8]` and predicts the next token which is `14` the `EOS`.\n",
        "   1. The reason you need to predict `EOS` is simple intuitively, consider the case where there's no need for `EOS`, then the model will not know when to stop.\n",
        "\n",
        "This goes on until the entire sequence is processed. Note that the model never actually \"sees\" the target tokens during the prediction. It is solely relying on the tokens that came before the current token in the input sequence. After the model makes its prediction, then the predicted tokens are compared to the target tokens to compute the loss, which is then backpropagated to update the model weights.\n",
        "\n",
        "### <a id='toc1_11_3_'></a>[Confusion: Training versus Inference](#toc0_)\n",
        "\n",
        "The statement \"it generates one token at a time and uses its own previously generated tokens as context for generating subsequent tokens\" is generally true for GPT-like models during the inference stage, not during training. During inference (or generation), the model does indeed use its own previously generated tokens to produce the next token, since there is no ground truth sequence to rely on. In that case, if the model makes an incorrect prediction at a certain timestep, that incorrect token is used as part of the context for the following timestep.\n",
        "\n",
        "During training, however, the model typically uses the ground truth tokens for the preceding sequence as context for predicting each next token, as described in your example. This resembles teacher forcing, in that the ground truth, rather than the model's own predictions, is used to guide training.\n",
        "\n",
        "So there's no contradiction, but the behavior is context-dependent:\n",
        "\n",
        "- During training, the ground truth sequence is used for context.\n",
        "- During inference, the model's own previously generated tokens are used for context.\n",
        "\n",
        "Both approaches are consistent with the autoregressive nature of the model: in both cases, the token at each position is generated based on the tokens at all previous positions. The difference lies in whether those preceding tokens come from the ground truth (during training) or from the model's own previous outputs (during inference).\n",
        "\n",
        "### Training vs Inference\n",
        "\n",
        "In an autoregressive model like a Transformer decoder, the concept of \"learning\n",
        "the representation of the sequence as it goes\" does not refer to the model\n",
        "processing one token at a time during actual forward passes. Instead, it refers\n",
        "to the model's ability to generate or predict one token at a time during\n",
        "inference, while training on a full sequence in a batched manner.\n",
        "\n",
        "During training:\n",
        "\n",
        "- All tokens are processed in parallel for efficiency. This is possible because\n",
        "  the entire sequence is known beforehand (it's the training data).\n",
        "- The \"autoregressive\" property is enforced by using masks in the self-attention\n",
        "  mechanism. This masking ensures that the prediction for each token can only\n",
        "  depend on previously generated tokens, not on future tokens which the model\n",
        "  has no access to during inference. This is how the model learns the\n",
        "  conditional probability distribution of each token given the previous tokens,\n",
        "  despite the parallel processing of tokens.\n",
        "\n",
        "During inference:\n",
        "\n",
        "- The model starts with an initial token (such as a start-of-sequence token) and\n",
        "  generates the next token based on this single input.\n",
        "- Then, the model uses both the initial token and the newly generated token to\n",
        "  predict the third token, and so on.\n",
        "- This process is sequential and each new token is predicted based on the\n",
        "  previously generated tokens, creating a sequence one token at a time.\n",
        "\n",
        "So, when we say that the model learns the representation of the sequence as it\n",
        "goes, we mean that the model is trained to handle sequences in such a way that\n",
        "it can generate them one piece at a time, respecting the causal order inherent\n",
        "to the task (e.g., language modeling). The parallel processing during training\n",
        "does not contradict the autoregressive nature of the model; it is simply a\n",
        "computational efficiency that is enabled by knowing the full sequence in\n",
        "advance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_12_'></a>[Questions](#toc0_)\n",
        "\n",
        "### <a id='toc1_12_1_'></a>[Why Masked == 0 in some?](#toc0_)\n",
        "\n",
        "The use of `mask == 0` in the `masked_fill` operation is a result of how the mask is constructed. Essentially, different implementations may represent masks differently:\n",
        "\n",
        "1. **Boolean Masking with True/False**: In some implementations, the mask might be a Boolean tensor where `True` denotes the positions to mask (set to negative infinity) and `False` for the positions to keep. In such cases, you can directly use the mask in `masked_fill` as in your provided code:\n",
        "\n",
        "    ```python\n",
        "    attention_scores = attention_scores.masked_fill(mask, float(\"-inf\"))\n",
        "    ```\n",
        "\n",
        "    Here, if `mask[i][j]` is `True`, `attention_scores[i][j]` would be set to `-inf`.\n",
        "\n",
        "2. **Integer Masking with 1/0**: In other implementations, the mask might be an integer tensor where `1` denotes the positions to keep and `0` denotes the positions to mask. In such cases, you'll often find the mask is inverted (`mask == 0`) before using `masked_fill`:\n",
        "\n",
        "    ```python\n",
        "    attention_scores = attention_scores.masked_fill(mask == 0, float(\"-inf\"))\n",
        "    ```\n",
        "\n",
        "    Here, if `mask[i][j]` is `0`, `attention_scores[i][j]` would be set to `-inf`.\n",
        "\n",
        "The core functionality—masking certain positions in the attention scores—is the same in both cases. The difference lies in how the mask tensor is constructed and interpreted. So, if you find an implementation using `mask == 0`, it's likely using an integer mask where `0` signifies positions to mask, whereas if it's directly using `mask`, it's probably a Boolean mask where `True` signifies positions to mask.\n",
        "\n",
        "### <a id='toc1_12_2_'></a>[what is the reason of setting the attention scores's mask indexes to negative infinity](#toc0_)\n",
        "\n",
        "\n",
        "In the attention mechanism, particularly in the Scaled Dot-Product Attention, attention scores are computed for each query-key pair and then passed through a softmax function to obtain attention weights. These weights are used to take a weighted sum of the value vectors, resulting in the final output or the context vectors. The purpose of the mask is to prevent certain tokens (like padding tokens) from being attended to.\n",
        "\n",
        "The reason for setting masked attention scores to negative infinity (`-inf`) lies in the properties of the softmax function:\n",
        "\n",
        "1. **Softmax Behavior**: The softmax function transforms its input (the attention scores in this case) into a probability distribution. Mathematically, the softmax function for a given vector $x$ is defined as:\n",
        "\n",
        "$$\n",
        "\\text{Softmax}(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{N} e^{x_j}}\n",
        "$$\n",
        "\n",
        "2. **Impact of Negative Infinity**: When you pass negative infinity through the softmax function, $e^{-\\infty}$ approaches zero. As a result, the masked positions get a near-zero weight in the attention mechanism.\n",
        "\n",
        "$$\n",
        "\\text{Softmax}(-\\infty) = \\frac{e^{-\\infty}}{\\sum_{j=1}^{N} e^{x_j}} \\approx 0\n",
        "$$\n",
        "\n",
        "3. **Avoiding Unwanted Attention**: The point of setting these specific positions to `-inf` is to ensure that when softmax is applied, these positions get zero attention weights. This is a way of making sure that the model does not attend to the positions we've masked (like padding tokens or future tokens in the sequence, depending on the mask).\n",
        "\n",
        "In summary, setting the masked attention scores to `-inf` and then passing them through a softmax effectively nullifies the contribution of the masked positions in the resulting attention-weighted sum of the value vectors. This is a commonly used trick to impose a certain structure (like masking out future information in the decoder) or to handle variable-length sequences with padding.\n",
        "\n",
        "### <a id='toc1_12_3_'></a>[Why do we need both ignore index in Loss and also negative infinity mask](#toc0_)\n",
        "\n",
        "Using an \"ignore index\" in the `CrossEntropyLoss` function in PyTorch can ignore the effect of certain tokens (like padding tokens) during the loss computation. However, the purpose of the mask in the attention mechanism and the \"ignore index\" in the loss function serve different roles in the model, and they operate at different stages of the computational graph.\n",
        "\n",
        "1. **Ignore Index in Loss Function**: The \"ignore index\" in the loss function ensures that the model's output at certain positions (typically corresponding to padding tokens) does not contribute to the loss. This happens at the very end of the forward pass, just before backpropagation begins.\n",
        "\n",
        "2. **Mask in Attention Mechanism**: The mask in the attention mechanism, on the other hand, operates during the forward pass at the time when attention scores are computed. This is a more \"internal\" operation and ensures that certain positions do not contribute to the output at all, not just during the loss computation but actually in the intermediate representations (i.e., context vectors) that the model computes.\n",
        "\n",
        "To put it another way, even if you're ignoring certain tokens in your loss calculation, those tokens can still influence the model's output unless they're masked out in the attention mechanism itself.\n",
        "\n",
        "For example, consider a decoder in a sequence-to-sequence model:\n",
        "- If you don't use a mask in the attention mechanism, future tokens could influence the output at the current timestep, which is not desirable.\n",
        "- Even if you use an \"ignore index\" in your loss function, it doesn't prevent the model from \"cheating\" by peeking at the future tokens if they are not masked in the attention mechanism.\n",
        "\n",
        "So in summary, using an \"ignore index\" in `CrossEntropyLoss` is not a replacement for using attention masks. Both have specific roles in the model, and they are often used together to ensure both that the model attends to the right tokens and that it is trained properly.\n",
        "\n",
        "### <a id='toc1_12_4_'></a>[Target and Preds/Logits Shape](#toc0_)\n",
        "\n",
        "The target tensor for the cross-entropy loss function should typically have a shape of `[batch_size, sequence_length]` where each entry in the tensor is an integer representing the index of the true class (i.e., the actual word/token from the vocabulary) for that position in the sequence. Here `batch_size` refers to the number of sequences in each batch, and `sequence_length` is the length of each sequence.\n",
        "\n",
        "Let's break it down step-by-step:\n",
        "\n",
        "1. **Last Linear Layer of Decoder**: When you say that the last linear layer of your decoder has shape `[bs, vocab_size]`, it means that for each example in the batch, you're outputting a distribution over the vocabulary. The values can be logit scores that represent the likelihood of each word in your vocabulary being the next word in the sequence.\n",
        "\n",
        "2. **Target Shape**: In comparison, your target tensor should contain the actual words (as integers) that appear at each position in your sequence for each example in the batch. The target tensor does not need to have a `vocab_size` dimension because it is not a distribution; it contains the indices of the actual next words. Thus, it should have a shape `[bs, sequence_length]`.\n",
        "\n",
        "3. **Cross-Entropy Loss**: When using the cross-entropy loss, the logits (i.e., the output from your linear layer) should have a shape `[bs, sequence_length, vocab_size]`, while the target should have a shape `[bs, sequence_length]`. The cross-entropy loss function will internally apply a softmax to the logits, and then compute the log-likelihood between the predicted distribution and the target class.\n",
        "\n",
        "To sum up, if your decoder's last linear layer has shape `[bs, vocab_size]` for each time step, make sure that your target tensor has the shape `[bs, sequence_length]`, and your logits should be `[bs, sequence_length, vocab_size]` when you feed them into the cross-entropy loss function.\n",
        "\n",
        "### <a id='toc1_12_5_'></a>[Why do we flatten prediction and target (logits)?](#toc0_)\n",
        "\n",
        "Flattening both the predicted logits and the target labels serves a specific purpose when using the cross-entropy loss function for sequence data. Let's dig into each component to understand why this is done:\n",
        "\n",
        "#### <a id='toc1_12_5_1_'></a>[Background](#toc0_)\n",
        "\n",
        "1. **Logits Tensor**: In a sequence-to-sequence model, you usually generate a sequence of logits for each item in your batch. The logits for each position in the sequence form a vector of size `vocab_size`, which gives you a probability distribution across all possible tokens.\n",
        "  \n",
        "   Shape: `[batch_size, sequence_length, vocab_size]`\n",
        "\n",
        "2. **Targets Tensor**: Your ground truth data, the `targets`, are integers representing the correct class labels (or tokens) at each sequence position.\n",
        "\n",
        "   Shape: `[batch_size, sequence_length]`\n",
        "\n",
        "#### <a id='toc1_12_5_2_'></a>[Traditional Loss Computation](#toc0_)\n",
        "\n",
        "Typically, the cross-entropy loss between predicted probabilities and target labels for one data point is computed, and then you average over all data points. In sequence-to-sequence models, you can think of each position in the sequence as a separate data point.\n",
        "\n",
        "#### <a id='toc1_12_5_3_'></a>[Why Flatten?](#toc0_)\n",
        "1. **Batch and Sequence Unification**: The idea of flattening both logits and targets is to treat each `(batch, sequence_position)` pair as an independent data point. Instead of having a batch of sequences, you have a \"flattened\" batch of tokens. This simplifies the application of the loss function by converting the 3D logits tensor and 2D targets tensor into 2D and 1D tensors, respectively.\n",
        "\n",
        "2. **Efficiency**: Loss computations often benefit from vectorization for computational efficiency. By flattening the tensors, you enable a more efficient matrix operation, which is generally faster than using nested loops over each sequence and batch.\n",
        "\n",
        "3. **Alignment**: The key is to ensure that each row in the flattened logits corresponds to the same position in the flattened targets. This alignment is crucial for the correct computation of the loss.\n",
        "\n",
        "#### <a id='toc1_12_5_4_'></a>[Step-by-step Flattening](#toc0_)\n",
        "\n",
        "1. **Logits Flattening**: `logits.view(-1, logits.size(-1))` will take the 3D tensor `[batch_size, seq_length, vocab_size]` and reshape it into a 2D tensor of shape `[batch_size * seq_length, vocab_size]`.\n",
        "\n",
        "2. **Targets Flattening**: `targets.view(-1)` will take the 2D tensor `[batch_size, seq_length]` and convert it into a 1D tensor of shape `[batch_size * seq_length]`.\n",
        "\n",
        "3. **Loss Calculation**: Both flattened tensors are then used in the cross-entropy loss function. The loss between each row in the flattened logits and the corresponding element in the flattened targets is computed.\n",
        "\n",
        "By flattening the tensors this way, you maintain the correspondence between each logit and its corresponding target, enabling you to correctly compute the loss for each token across all sequences and batches.\n",
        "\n",
        "### <a id='toc1_12_6_'></a>[Why sometimes unsqueeze masks?](#toc0_)\n",
        "\n",
        "The `unsqueeze` operation is used to add an additional dimension to the tensor. In attention mechanisms, particularly the scaled dot-product attention used in models like the Transformer, the masks usually need to have the same number of dimensions as the attention logits for proper broadcasting.\n",
        "\n",
        "For instance, let's say your source tensor (`src`) has a shape of $B \\times L$ where $B$ is the batch size and $L$ is the sequence length. The attention logit tensor resulting from the query-key dot product would then have shape $B \\times N \\times L \\times L$, where $N$ is the number of attention heads.\n",
        "\n",
        "The mask needs to align with the $L \\times L$ dimensions of this 4D tensor. In order to accomplish that, you add singleton dimensions to make it compatible with the attention logit tensor. By unsqueezing the mask tensor from $B \\times L$ to $B \\times 1 \\times 1 \\times L$, you enable broadcasting such that the mask effectively gets expanded to $B \\times N \\times L \\times L$ during the attention calculation, perfectly aligning with the attention logits.\n",
        "\n",
        "That's why the line:\n",
        "```python\n",
        "self.src_mask = (src != pad).unsqueeze(-2)\n",
        "```\n",
        "adds a singleton dimension, converting the shape from $B \\times L$ to $B \\times 1 \\times 1 \\times L$ for proper broadcasting during the attention computations.\n",
        "\n",
        "### <a id='toc1_12_7_'></a>[Why does sequence length differ for source and target, usually I thought it is just all L, same.](#toc0_)\n",
        "\n",
        "In many sequence-to-sequence tasks, the source and target sequences can have different lengths. Here are a few scenarios where this happens:\n",
        "\n",
        "1. **Machine Translation**: A sentence in one language may require more or fewer words when translated into another language.\n",
        "  \n",
        "2. **Text Summarization**: The source text is usually much longer than the summarized target text.\n",
        "\n",
        "3. **Question Answering**: The source document could be quite lengthy, while the target answer might be a short sentence or even a single word.\n",
        "\n",
        "4. **Code Generation**: Given a natural language query, the corresponding code snippet could be of varying length that doesn't directly correlate with the length of the query.\n",
        "\n",
        "5. **Dialogue Systems**: The system's response may not be of the same length as the user's query.\n",
        "\n",
        "So, in general, source and target sequence lengths (`S` and `T` in the function signature) could be different, and the attention mechanism accommodates that by allowing for `key` and `query` tensors with different sequence lengths. Although\n",
        "lilian weng use `L` for seq length.\n",
        "\n",
        "### <a id='toc1_12_8_'></a>[Am i right to assume that the core idea of autoregressive model like decoder only (GPT like) is that for a given sample, there will eventually be L rows where L is the seq length, and therefore I can intuitively view it as 1 sample having L samples, since for each row, we will compute the loss. Am I right in my understanding? Do not hesistate to correct me.](#toc0_)\n",
        "\n",
        "\n",
        "Your description captures an important aspect of autoregressive models like decoder-only Transformers (e.g., GPT). Specifically, you're talking about how the model treats a sequence during training. Let's break down your understanding step by step.\n",
        "\n",
        "1. **Sequence Length**: When you mention \"L rows,\" where \"L\" is the sequence length, you're essentially pointing out that each sequence is divided into \"L\" time steps (or tokens). Each time step becomes an input-output pair for training the model.\n",
        "\n",
        "2. **One Sequence as Multiple Samples**: You're correct to intuit that a single sequence of length \"L\" can be treated like \"L\" samples, at least in the context of loss calculation. This is because, during training, the model computes the loss at each time step by comparing the predicted token with the actual next token in the sequence.\n",
        "\n",
        "3. **Loss Computation**: The loss is often computed at each position and then averaged over the sequence length or summed up, depending on the specific loss function or training regime.\n",
        "\n",
        "However, it's crucial to clarify that although a single sequence may contribute \"L\" terms to the loss function, this is not equivalent to having \"L\" independent samples. The key difference lies in the autoregressive property: the prediction at each time step is conditioned on the preceding tokens. This introduces a temporal dependency across the \"L\" positions, making them not entirely independent samples.\n",
        "\n",
        "In other words, while it's accurate to say that a single sequence contributes multiple terms to the loss function, these terms are correlated because they come from the same sequence and are generated in an autoregressive manner.\n",
        "\n",
        "To summarize, you're mostly correct in your understanding that a single sequence is broken down into multiple steps for the purpose of loss computation, but it's important to remember that these steps are not independent samples due to the autoregressive nature of the model.\n",
        "\n",
        "### <a id='toc1_12_9_'></a>[QKV Again](#toc0_)\n",
        "\n",
        "#### <a id='toc1_12_9_1_'></a>[Background and Assumptions](#toc0_)\n",
        "\n",
        "The Transformer architecture, introduced by Vaswani et al. in 2017, has become a cornerstone in NLP and many other machine learning tasks. It is built around the concept of self-attention, which allows the model to weigh different parts of the input when making predictions or transformations. The context vector, as well as Q, K, and V vectors, play a crucial role in this architecture.\n",
        "\n",
        "#### <a id='toc1_12_9_2_'></a>[Context Vector](#toc0_)\n",
        "\n",
        "The term \"context vector\" is commonly used to refer to the weighted sum of value vectors (`V`), after the attention scores have been computed. The purpose of this vector is to encode information from different parts of the input sequence in a way that is most useful for the task at hand. In the attention mechanism, each word (or token) is represented as a context vector that aggregates information from all the other words in the sentence, weighted by their relevance or \"attention score\".\n",
        "\n",
        "#### <a id='toc1_12_9_3_'></a>[Query (Q), Key (K), and Value (V)](#toc0_)\n",
        "\n",
        "1. **Query (Q):** This is a representation of the element for which we are calculating the context. The query is used to find relevant keys, which in turn helps in identifying relevant values. Mathematically, we take the dot product of the Query with each Key to get an attention score.\n",
        "\n",
        "   $$\n",
        "   \\text{Attention Score} = Q \\cdot K^T\n",
        "   $$\n",
        "\n",
        "2. **Key (K):** Keys serve as a set of indicators, helping the model identify which values should be attended to when forming the context vector for each query.\n",
        "\n",
        "3. **Value (V):** Values hold the actual content that will be used to form the context vector. Once the attention scores have been computed using Q and K, these scores are used to weigh the Value vectors before summing them up to get the final context vector.\n",
        "\n",
        "#### <a id='toc1_12_9_4_'></a>[Mathematical Description](#toc0_)\n",
        "\n",
        "To obtain the context vector, we first calculate the attention scores for each Query-Key pair:\n",
        "\n",
        "$$\n",
        "\\text{Attention Score} = \\frac{Q \\cdot K^T}{\\sqrt{d_k}}\n",
        "$$\n",
        "\n",
        "We then take the softmax of these scores:\n",
        "\n",
        "$$\n",
        "\\text{Softmax Score} = \\text{Softmax}(\\text{Attention Score})\n",
        "$$\n",
        "\n",
        "Finally, we use these softmax scores to compute a weighted sum of the Value vectors:\n",
        "\n",
        "$$\n",
        "\\text{Context Vector} = \\text{Softmax Score} \\cdot V\n",
        "$$\n",
        "\n",
        "In summary, Q, K, and V vectors are instrumental in the computation of the context vector. The Query helps to identify relevant Keys, which in turn are used to weigh the Values, culminating in a context vector that holds the contextual representation useful for a given task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_13_'></a>[TODO](#toc0_)\n",
        "\n",
        "1. Add Positional Encoding\n",
        "2. Add LR Scheduler\n",
        "3. Check why need to use `torch.nn.utils.clip_grad_norm_` to clip gradients\n",
        "4. Why unsqueeze mask?\n",
        "5. Can you init weights inside Encoder instead of outside?\n",
        "6. Add Epoch and Batch State see my old code.\n",
        "7. Important use `Vocab` class like in https://github.com/jsbaan/transformer-from-scratch/blob/main/vocabulary.py.\n",
        "\n",
        "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_14_'></a>[References and Further Readings](#toc0_)\n",
        "\n",
        "- https://slds-lmu.github.io/seminar_nlp_ss20/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
