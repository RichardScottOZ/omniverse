

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Concept &#8212; Omniverse</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'transformer/concept';</script>
    <link rel="canonical" href="https://www.gaohongnan.com/transformer/concept.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Styling, Formatting, and Linting" href="../software_engineering/devops/continuous-integration/styling.html" />
    <link rel="prev" title="Notations" href="notations.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Omniverse - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Omniverse - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Transformers - Attention is All You Need</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="intro.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="notations.html">Notations</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Concept</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Software Engineering</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../software_engineering/devops/continuous-integration/styling.html">Styling, Formatting, and Linting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software_engineering/design_patterns/dependency-inversion-principle.html">Dependency Inversion Principle</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data Structures and Algorithms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../dsa/complexity_analysis/intro.html">Complexity Analysis</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../dsa/complexity_analysis/master_theorem.html">Master Theorem </a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../dsa/stack/intro.html">Stack</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../dsa/stack/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../dsa/searching_algorithms/linear_search/intro.html">Linear Search</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../dsa/searching_algorithms/linear_search/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../dsa/searching_algorithms/binary_search/intro.html">Binary Search</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../dsa/searching_algorithms/binary_search/concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../dsa/searching_algorithms/binary_search/problems/875-koko-eating-bananas.html">Koko Eating Bananas</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Linear Algebra</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../linear_algebra/01_preliminaries/intro.html">Preliminaries</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/01_preliminaries/01-fields.html">Fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/01_preliminaries/02-systems-of-linear-equations.html">Systems of Linear Equations</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../linear_algebra/02_vectors/intro.html">Vectors</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/02_vectors/01-vector-definition.html">Vector and Its Definition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/02_vectors/02-vector-operation.html">Vector and Its Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/02_vectors/03-vector-norm.html">Vector Norm and Distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/02_vectors/04-vector-products.html">A First Look at Vector Products</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Computer Science</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../computer_science/type_theory/intro.html">Type Theory, A Very Rudimentary Introduction</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/01-subtypes.html">Subtypes</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References, Resources and Roadmap</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../bibliography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/transformer/concept.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Concept</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention">Self-Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition">Intuition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analogy-1-a-generic-example">Analogy 1. A Generic Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analogy-2-a-more-concrete-example">Analogy 2. A More Concrete Example</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#casual-attention-masked-self-attention">Casual Attention/Masked Self-Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Intuition</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#perplexity">Perplexity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#all-the-whys">All the Whys?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#can-you-explain-the-subspaces-projection-in-attention-mechanism">Can you explain the Subspaces projection in Attention Mechanism?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-softmax-in-attention-mechanism">Why Softmax in Attention Mechanism?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-scale-in-attention-mechanism">Why Scale in Attention Mechanism?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-need-positional-encoding-what-happens-if-we-don-t-use-it">Why do need Positional Encoding? What happens if we don’t use it?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references-and-further-readings">References and Further Readings</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="concept">
<h1>Concept<a class="headerlink" href="#concept" title="Permalink to this heading">#</a></h1>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#self-attention" id="id2">Self-Attention</a></p>
<ul>
<li><p><a class="reference internal" href="#intuition" id="id3">Intuition</a></p></li>
<li><p><a class="reference internal" href="#analogy-1-a-generic-example" id="id4">Analogy 1. A Generic Example</a></p></li>
<li><p><a class="reference internal" href="#analogy-2-a-more-concrete-example" id="id5">Analogy 2. A More Concrete Example</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#casual-attention-masked-self-attention" id="id6">Casual Attention/Masked Self-Attention</a></p>
<ul>
<li><p><a class="reference internal" href="#id1" id="id7">Intuition</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#perplexity" id="id8">Perplexity</a></p></li>
<li><p><a class="reference internal" href="#all-the-whys" id="id9">All the Whys?</a></p>
<ul>
<li><p><a class="reference internal" href="#can-you-explain-the-subspaces-projection-in-attention-mechanism" id="id10">Can you explain the Subspaces projection in Attention Mechanism?</a></p></li>
<li><p><a class="reference internal" href="#why-softmax-in-attention-mechanism" id="id11">Why Softmax in Attention Mechanism?</a></p></li>
<li><p><a class="reference internal" href="#why-scale-in-attention-mechanism" id="id12">Why Scale in Attention Mechanism?</a></p></li>
<li><p><a class="reference internal" href="#why-do-need-positional-encoding-what-happens-if-we-don-t-use-it" id="id13">Why do need Positional Encoding? What happens if we don’t use it?</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#references-and-further-readings" id="id14">References and Further Readings</a></p></li>
</ul>
</nav>
<section id="self-attention">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Self-Attention</a><a class="headerlink" href="#self-attention" title="Permalink to this heading">#</a></h2>
<section id="intuition">
<h3><a class="toc-backref" href="#id3" role="doc-backlink">Intuition</a><a class="headerlink" href="#intuition" title="Permalink to this heading">#</a></h3>
<p>Let’s use the sentence “Cat walks by the bank” to walk through the
self-attention mechanism with analogies and to clarify how it works step by
step. The sentence is tokenized into <code class="docutils literal notranslate"><span class="pre">[&quot;cat&quot;,</span> <span class="pre">&quot;walks&quot;,</span> <span class="pre">&quot;by&quot;,</span> <span class="pre">&quot;the&quot;,</span> <span class="pre">&quot;bank&quot;]</span></code> and
can be represented as an input sequence <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> of <span class="math notranslate nohighlight">\(L=5\)</span> tokens, where we
consider each word as a token.</p>
<p>For each token, we have a vector representation of it, which is called the
embedding of the token. We can represent the embedding of the <span class="math notranslate nohighlight">\(i\)</span>-th token as
<span class="math notranslate nohighlight">\(\mathbf{z}_i\)</span>. For example, the embedding of the token “cat” can be represented
as <span class="math notranslate nohighlight">\(\mathbf{z}_1 \in \mathbb{R}^{1 \times D}\)</span>, where <span class="math notranslate nohighlight">\(D\)</span> is the dimension of the
embedding.</p>
<p>The self-attention mechanism aims to project each of our embedding
<span class="math notranslate nohighlight">\(\mathbf{z}_i\)</span> in the input sequence into a new embedding vector, which we call
the <strong>context vector</strong> <span class="math notranslate nohighlight">\(\mathbf{c}_i\)</span>, which still lies in the same dimension
<span class="math notranslate nohighlight">\(D\)</span>, albeit going through some projections to different subspaces during the
process, which we will explain later.</p>
<p>This context vector <span class="math notranslate nohighlight">\(\mathbf{c}_i\)</span> is a representation of the <span class="math notranslate nohighlight">\(i\)</span>-th token in
the input sequence, which is a weighted sum of all the embeddings of the tokens
in the input sequence. The weights are calculated by the attention mechanism.</p>
<p>More concretely, the initial embedding <span class="math notranslate nohighlight">\(\mathbf{z}_1\)</span> of the token “cat” is only
holding semantic information about the token “cat” itself. However, after going
through the self-attention mechanism, the context vector <span class="math notranslate nohighlight">\(\mathbf{c}_1\)</span> of the
token “cat” is a weighted sum of all the embeddings of the tokens in the input
sequence, which means that the context vector <span class="math notranslate nohighlight">\(\mathbf{c}_1\)</span> of the token “cat”
is holding information about the token “cat” itself as well as the information
about the other tokens in the input sequence. It allows the token “cat” to have
a better understanding of itself in the context of the whole sentence (i.e.
should I, the token “cat”, pay more attention to the token “bank” as a financial
institution or a river bank and pay less attention to the token “by”?).</p>
</section>
<section id="analogy-1-a-generic-example">
<h3><a class="toc-backref" href="#id4" role="doc-backlink">Analogy 1. A Generic Example</a><a class="headerlink" href="#analogy-1-a-generic-example" title="Permalink to this heading">#</a></h3>
<p><strong>Setting the Scene (Embedding the Sentence):</strong> Imagine each word in the
sentence is a person at a party (our tokens). They start by telling a basic fact
about themselves (their initial embedding with semantic meaning).</p>
<p><strong>The Roles:</strong></p>
<ul class="simple">
<li><p><strong><span class="math notranslate nohighlight">\(Q\)</span> (Seekers)</strong>: Each person (word) is curious about the stories
(contexts) of others at the party. They have their own perspective or
question (Q vector).</p></li>
<li><p><strong><span class="math notranslate nohighlight">\(K\)</span> (Holders)</strong>: At the same time, each person has a name tag with
keywords that describe their story (K vector).</p></li>
<li><p><strong><span class="math notranslate nohighlight">\(V\)</span> (Retrievers)</strong>: They also hold a bag of their experiences (V vector),
ready to share.</p></li>
</ul>
<p><strong>Transformations (Applying W Matrices):</strong> We give each person a set of glasses
(the matrices <span class="math notranslate nohighlight">\(W_Q, W_K, W_V\)</span>) that changes how they see the world (the space
they project to).</p>
<ul class="simple">
<li><p>With <span class="math notranslate nohighlight">\(W_Q\)</span> glasses, they focus on what they want to know from others.</p></li>
<li><p>With <span class="math notranslate nohighlight">\(W_K\)</span> glasses, they highlight their name tag details, making some
features stand out more.</p></li>
<li><p>With <span class="math notranslate nohighlight">\(W_V\)</span> glasses, they prepare to share the contents of their bag
effectively.</p></li>
</ul>
<p><strong>Attention (Calculating <span class="math notranslate nohighlight">\(Q &#64; K^T\)</span>):</strong> Now, each person looks around the room
(sequence) with their <span class="math notranslate nohighlight">\(W_Q\)</span> glasses and sees the highlighted name tags (after
<span class="math notranslate nohighlight">\(W_K\)</span> transformation) of everyone else. They measure how similar their question
is to the others’ name tags—this is the dot product <span class="math notranslate nohighlight">\(Q &#64; K^T\)</span>.</p>
<p>For “cat,” let’s say it’s curious about the notion of “walks” and “bank.” It
will measure the similarity (attention scores) between its curiosity and the
name tags of “walks,” “by,” “the,” “bank.”</p>
<p><strong>Normalization (Softmax):</strong> After measuring, “cat” decides how much to focus on
each story—this is softmax. Some stories are very relevant (“walks”), some
moderately (“by,” “the”), and some might be highly relevant depending on context
(“bank” — is it a river bank or a financial institution?).</p>
<p><strong>Retrieval (Applying Attention to V):</strong> Now “cat” decides to listen to the
stories in proportion to its focus. It takes pieces (weighted by attention
scores) from each person’s experience bag (V vectors) and combines them into a
richer, contextual understanding of itself in the sentence. This combination
gives us the new representation of “cat,” informed by the entire context of the
sentence.</p>
<p>In essence:</p>
<ul class="simple">
<li><p><strong>Q (Query):</strong> What does “cat” want to know?</p></li>
<li><p><strong>K (Key):</strong> Who has relevant information to “cat”’s curiosity?</p></li>
<li><p><strong>V (Value):</strong> What stories does “cat” gather from others, and how much does
it take from each to understand its role in the sentence?</p></li>
</ul>
<p>The output of self-attention for “cat” now encapsulates not just “cat” but its
relationship and relevance to “walks,” “by,” “the,” “bank” in a way that no
single word could convey alone. This output then becomes the input to the next
layer, where the process can repeat, enabling the model to develop an even more
nuanced understanding.</p>
</section>
<section id="analogy-2-a-more-concrete-example">
<h3><a class="toc-backref" href="#id5" role="doc-backlink">Analogy 2. A More Concrete Example</a><a class="headerlink" href="#analogy-2-a-more-concrete-example" title="Permalink to this heading">#</a></h3>
<ol class="arabic">
<li><p><strong>Attention Scores</strong>: Once you have your <span class="math notranslate nohighlight">\( Q, K, V \)</span> matrices (which are all
<span class="math notranslate nohighlight">\( L \times D \)</span> in this simplified example), you calculate the dot product
between queries <span class="math notranslate nohighlight">\( Q \)</span> and keys <span class="math notranslate nohighlight">\( K
\)</span>. This is essentially
measuring how each word in the sentence relates to every other word.
Mathematically, you’ll get a matrix of shape <span class="math notranslate nohighlight">\( L \times L \)</span>, where each
element <span class="math notranslate nohighlight">\( (i, j) \)</span> represents the “affinity” between the <span class="math notranslate nohighlight">\( i^{th} \)</span> and
<span class="math notranslate nohighlight">\(
j^{th} \)</span> words.</p>
<p><strong>Intuition</strong>: Imagine you’re trying to understand the role of the word
“cat” in the sentence. You calculate its dot product with every other word
to get a set of scores. These scores tell you how much each word in the
sentence should be “attended to” when you’re focusing on “cat.”</p>
</li>
<li><p><strong>Scaling and Softmax</strong>: The attention scores are scaled down by <span class="math notranslate nohighlight">\( \sqrt{D} \)</span>
and then a softmax is applied. This turns the scores into probabilities
(attention weights) and ensures that they sum to 1 for each word you’re
focusing on.</p>
<p><strong>Intuition</strong>: After scaling and softmax, you get a set of weights that tell
you how to create a weighted sum of all the words in the sentence when
you’re focusing on a particular word like “cat.”</p>
</li>
<li><p><strong>Context Vector</strong>: Finally, these attention weights are used to create a
weighted sum of the value vectors <span class="math notranslate nohighlight">\( V \)</span>. This weighted sum is your context
vector.</p>
<p><strong>Intuition</strong>: When focusing on the word “cat,” you look at the attention
weights to decide how much of each other word you should include in your
understanding of “cat.” You then sum up these weighted words to get a new
vector, or “context,” for the word “cat.”</p>
</li>
<li><p><strong>Output</strong>: The output will be another <span class="math notranslate nohighlight">\( L \times D \)</span> matrix, where each row
is the new “contextualized” representation of each word in your sentence.</p></li>
</ol>
<p>In your mind, you can picture it as a series of transformations: starting from
the initial <span class="math notranslate nohighlight">\(L \times D\)</span> matrix, through an <span class="math notranslate nohighlight">\( L \times L \)</span> attention score
matrix and attention weights, and back to a new <span class="math notranslate nohighlight">\( L \times D \)</span> context matrix.
Each step refines the information content of your sentence, focusing on
different relationships between the words.</p>
</section>
</section>
<section id="casual-attention-masked-self-attention">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">Casual Attention/Masked Self-Attention</a><a class="headerlink" href="#casual-attention-masked-self-attention" title="Permalink to this heading">#</a></h2>
<section id="id1">
<h3><a class="toc-backref" href="#id7" role="doc-backlink">Intuition</a><a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<p>One sentence to summarize the understanding.</p>
<p><strong>Casual attention (masked self attention) in decoder reduces to self attention
for the last token in the input sequence.</strong></p>
<p>Causal attention in a decoder architecture, such as the one used in Transformer
models, effectively reduces to self-attention for the last token in the input
sequence.</p>
<ol class="arabic simple">
<li><p><strong>Causal Attention Mechanism</strong>: In a causal attention mechanism, each token
is allowed to attend to itself and all preceding tokens in the sequence. This
is enforced by masking future tokens to prevent information flow from future
tokens into the current or past tokens. This mechanism is crucial in
generative models where the prediction for the current token should not be
influenced by future tokens, as they are not known during inference.</p></li>
<li><p><strong>Self-Attention Mechanism</strong>: In self-attention, each token computes
attention scores with every other token in the sequence, including itself.
These attention scores are used to create a weighted sum of the values (token
representations), which becomes the new representation of the token.</p></li>
<li><p><strong>Last Token in the Sequence</strong>: When considering the last token in the
sequence, the causal attention mechanism’s nature implies that this token has
access to all previous tokens in the sequence, including itself. There are no
future tokens to mask. Therefore, the attention mechanism for this token
becomes identical to the standard self-attention mechanism where it is
attending to all tokens up to itself.</p></li>
</ol>
</section>
</section>
<section id="perplexity">
<h2><a class="toc-backref" href="#id8" role="doc-backlink">Perplexity</a><a class="headerlink" href="#perplexity" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://keras.io/api/keras_nlp/metrics/perplexity/">https://keras.io/api/keras_nlp/metrics/perplexity/</a></p></li>
<li><p><a class="reference external" href="https://lightning.ai/docs/torchmetrics/stable/text/perplexity.html">https://lightning.ai/docs/torchmetrics/stable/text/perplexity.html</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/docs/transformers/perplexity">https://huggingface.co/docs/transformers/perplexity</a></p></li>
</ul>
</section>
<section id="all-the-whys">
<h2><a class="toc-backref" href="#id9" role="doc-backlink">All the Whys?</a><a class="headerlink" href="#all-the-whys" title="Permalink to this heading">#</a></h2>
<section id="can-you-explain-the-subspaces-projection-in-attention-mechanism">
<h3><a class="toc-backref" href="#id10" role="doc-backlink">Can you explain the Subspaces projection in Attention Mechanism?</a><a class="headerlink" href="#can-you-explain-the-subspaces-projection-in-attention-mechanism" title="Permalink to this heading">#</a></h3>
</section>
<section id="why-softmax-in-attention-mechanism">
<h3><a class="toc-backref" href="#id11" role="doc-backlink">Why Softmax in Attention Mechanism?</a><a class="headerlink" href="#why-softmax-in-attention-mechanism" title="Permalink to this heading">#</a></h3>
<p>See <a class="reference external" href="https://www.youtube.com/watch?v=UPtG_38Oq8o">https://www.youtube.com/watch?v=UPtG_38Oq8o</a> around 16 min.</p>
</section>
<section id="why-scale-in-attention-mechanism">
<h3><a class="toc-backref" href="#id12" role="doc-backlink">Why Scale in Attention Mechanism?</a><a class="headerlink" href="#why-scale-in-attention-mechanism" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html">https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html</a></p></li>
</ul>
<p>Let’s look at the notes:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html">https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html</a></p></li>
</ul>
<ol class="arabic simple">
<li><p><strong>Scaling Factor Introduction</strong>: The author introduces the concept of a
scaling factor <span class="math notranslate nohighlight">\(\frac{1}{\sqrt{d_k}}\)</span>, which is applied during the attention
mechanism in transformers. This scaling factor is used to maintain an
appropriate variance of the attention scores after initialization.</p></li>
<li><p><strong>Initialization Goal</strong>: The goal of initialization is to have each layer of
the neural network maintain equal variance throughout. This is to ensure that
the gradients are neither vanishing nor exploding as they propagate through
the layers, which is crucial for effective learning.</p></li>
<li><p><strong>Variance in Dot Products</strong>: The author then explains that when taking a dot
product of two vectors, both sampled from normal distributions with variance
<span class="math notranslate nohighlight">\(\sigma^2\)</span>, the resulting scalar will have a variance that is <span class="math notranslate nohighlight">\(d_k\)</span> times
higher, specifically <span class="math notranslate nohighlight">\(\sigma^4 \cdot d_k\)</span>. Here, <span class="math notranslate nohighlight">\(d_k\)</span> represents the
dimension of the key/query vectors in the attention mechanism, and <span class="math notranslate nohighlight">\(q_i\)</span> and
<span class="math notranslate nohighlight">\(k_i\)</span> are the components of the query and key vectors respectively.</p></li>
<li><p><strong>Scaling Down the Variance</strong>: Without scaling down the variance of the dot
product (which is <span class="math notranslate nohighlight">\(\sigma^4 \cdot d_k\)</span>), the softmax function, which is
applied to the attention scores to obtain the probabilities, would become
saturated. This means that one logit (the vector of raw (non-normalized)
predictions that a classification model generates, which is then passed to a
normalization function) would have a very high score close to 1, while the
rest would have scores close to 0. This saturation makes it difficult for the
network to learn because the gradients would be close to zero for all
elements except the one with the highest score.</p></li>
<li><p><strong>Maintaining Variance Close to 1</strong>: The author notes that despite the
multiplication by <span class="math notranslate nohighlight">\(\sigma^4\)</span>, the practice of keeping the original variance
<span class="math notranslate nohighlight">\(\sigma^2\)</span> close to 1 means that the scaling factor does not introduce a
significant issue. By multiplying the dot product by <span class="math notranslate nohighlight">\(\frac{1}{\sqrt{d_k}}\)</span>,
the variance of the product is effectively scaled back to the original level
of <span class="math notranslate nohighlight">\(\sigma^2\)</span>, preventing the softmax function from saturating and allowing
the model to learn effectively.</p></li>
</ol>
<p>The gist is:</p>
<p>It is important to maintain equal variance across all layers in a neural
network, particularly in the context of the transformer model’s attention
mechanism. By doing so, the model helps to ensure that the gradients are stable
during backpropagation, avoiding the vanishing or exploding gradients problem
and enabling effective learning.</p>
<p>In the specific context of the attention mechanism, the variance of the dot
products used to calculate attention scores is scaled down by the factor
<span class="math notranslate nohighlight">\(\frac{1}{\sqrt{d_k}}\)</span> to prevent softmax saturation. This allows each element
to have a chance to influence the model’s learning, rather than having a single
element dominate because of the variance scaling with <span class="math notranslate nohighlight">\(d_k\)</span>. This practice is
crucial for the learning process because it ensures the gradients are meaningful
and not diminished to the point where the model cannot learn from the data.</p>
</section>
<section id="why-do-need-positional-encoding-what-happens-if-we-don-t-use-it">
<h3><a class="toc-backref" href="#id13" role="doc-backlink">Why do need Positional Encoding? What happens if we don’t use it?</a><a class="headerlink" href="#why-do-need-positional-encoding-what-happens-if-we-don-t-use-it" title="Permalink to this heading">#</a></h3>
</section>
</section>
<section id="references-and-further-readings">
<h2><a class="toc-backref" href="#id14" role="doc-backlink">References and Further Readings</a><a class="headerlink" href="#references-and-further-readings" title="Permalink to this heading">#</a></h2>
<p>…</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./transformer"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="notations.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Notations</p>
      </div>
    </a>
    <a class="right-next"
       href="../software_engineering/devops/continuous-integration/styling.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Styling, Formatting, and Linting</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention">Self-Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition">Intuition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analogy-1-a-generic-example">Analogy 1. A Generic Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analogy-2-a-more-concrete-example">Analogy 2. A More Concrete Example</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#casual-attention-masked-self-attention">Casual Attention/Masked Self-Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Intuition</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#perplexity">Perplexity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#all-the-whys">All the Whys?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#can-you-explain-the-subspaces-projection-in-attention-mechanism">Can you explain the Subspaces projection in Attention Mechanism?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-softmax-in-attention-mechanism">Why Softmax in Attention Mechanism?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-scale-in-attention-mechanism">Why Scale in Attention Mechanism?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-need-positional-encoding-what-happens-if-we-don-t-use-it">Why do need Positional Encoding? What happens if we don’t use it?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references-and-further-readings">References and Further Readings</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gao Hongnan
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>