

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>FAQ &#8212; Omniverse</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'transformer/dump';</script>
    <link rel="canonical" href="https://www.gaohongnan.com/transformer/dump.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Shakespeare" href="decoder/shakespeare.html" />
    <link rel="prev" title="Concept" href="concept.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Omniverse - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Omniverse - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Transformers - Attention is All You Need</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="intro.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="notations.html">Notations</a></li>
<li class="toctree-l1"><a class="reference internal" href="concept.html">Concept</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="decoder/shakespeare.html">Shakespeare</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../deep_learning/training_chronicles/intro.html">Training Chronicles</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../deep_learning/training_chronicles/loss.html">The Loss Landscape</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Software Engineering</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../software_engineering/devops/continuous-integration/styling.html">Styling, Formatting, and Linting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software_engineering/design_patterns/dependency-inversion-principle.html">Dependency Inversion Principle</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../software_engineering/concurrency_parallelism_asynchronous/intro.html">Concurrency, Parallelism and Asynchronous Programming</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../software_engineering/concurrency_parallelism_asynchronous/generator_yield.html">A Rudimentary Introduction to Generator and Yield in Python</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../software_engineering/serving/restful_api/intro.html">RESTful API</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../software_engineering/serving/restful_api/application_banking.html">Application: Designing a RESTful Banking API with FastAPI and SQLAlchemy</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Computer Science</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../computer_science/type_theory/intro.html">Type Theory, A Very Rudimentary Introduction</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/01-subtypes.html">Subtypes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/02-type-safety.html">Type Safety</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/03-subsumption.html">Subsumption</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/04-generics.html">Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/05-typevar-bound-constraints.html">Bound and Constraint in Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/06-invariance-covariance-contravariance.html">Invariance, Covariance and Contravariance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/07-pep-3124-overloading.html">Function Overloading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/08-pep-661-sentinel-values.html">Sentinel Types</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data Structures and Algorithms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../dsa/complexity_analysis/intro.html">Complexity Analysis</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../dsa/complexity_analysis/master_theorem.html">Master Theorem </a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../dsa/stack/intro.html">Stack</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../dsa/stack/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../dsa/searching_algorithms/linear_search/intro.html">Linear Search</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../dsa/searching_algorithms/linear_search/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../dsa/searching_algorithms/binary_search/intro.html">Binary Search</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../dsa/searching_algorithms/binary_search/concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../dsa/searching_algorithms/binary_search/problems/875-koko-eating-bananas.html">Koko Eating Bananas</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Linear Algebra</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../linear_algebra/01_preliminaries/intro.html">Preliminaries</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/01_preliminaries/01-fields.html">Fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/01_preliminaries/02-systems-of-linear-equations.html">Systems of Linear Equations</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../linear_algebra/02_vectors/intro.html">Vectors</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/02_vectors/01-vector-definition.html">Vector and Its Definition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/02_vectors/02-vector-operation.html">Vector and Its Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/02_vectors/03-vector-norm.html">Vector Norm and Distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/02_vectors/04-vector-products.html">A First Look at Vector Products</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References, Resources and Roadmap</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../bibliography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/transformer/dump.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>FAQ</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analogy-for-self-attention-03-march-2024">Analogy for self attention (03-march-2024)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bmm-versus-matmul"><code class="docutils literal notranslate"><span class="pre">bmm</span></code> versus <code class="docutils literal notranslate"><span class="pre">matmul</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-is-similar-to-kernels-in-cnn">Multi-Head is similar to kernels in CNN</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-role-of-feed-forward-network">The Role of Feed-Forward Network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-w-q-h-is-implemented-in-practice">How <span class="math notranslate nohighlight">\(W^{q}_h\)</span> is implemented in practice?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#do-not-split-the-embeddings-in-h-heads-instead-you-split-the-linear-transformed-embeddings">Do not split the embeddings in H heads, instead you split the linear transformed embeddings.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-two-approaches">The Two Approaches</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#approach-1-single-large-weight-matrix-implementation-paper-s-code">Approach 1: Single Large Weight Matrix Implementation (Paper’s Code)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#approach-2-separate-weight-matrices-notation-paper-notation">Approach 2: Separate Weight Matrices Notation (Paper Notation)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer-s-weight-sharing">Transformer’s Weight Sharing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#qn-1-if-there-are-more-than-1-sample-does-it-mean-for-each-sample-there-is-a-different-weight-matrix-for-q-k-and-v">Qn 1 If there are more than 1 sample, does it mean for each sample, there is a different weight matrix for Q, K and V?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#qn-2">Qn 2</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#so-if-i-have-4-images-in-a-batch-the-weight-is-the-same-weight-and-not-4-different-weights">So if I have 4 images in a batch the weight is the same weight and not 4 different weights</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#so-if-my-image-has-40-images-and-batch-size-of-4-will-there-be-8-different-weights">So if my image has 40 images and batch size of 4. Will there be 8 different weights</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#after-training-transformer-is-the-q-weight-matrix-wq-fixed">After training transformer is the Q weight matrix WQ fixed</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#then-how-does-it-learn-context-when-i-pass-in-new-sentence-if-weight-is-fixed">Then how does it learn context when I pass in new sentence if weight is fixed</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#but-weight-matrix-is-lets-say-dxd-right-it-does-not-take-into-account-the-vocabulary-words-how-it-knows-context">But weight matrix is let’s say DxD right it does not take into account the vocabulary words how it knows context</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#so-in-a-sense-once-u-train-a-model-all-weights-are-fixed-and-weights-are-shared-across-all-data-samples">So in a sense once u train a model all weights are fixed. And weights are shared across ALL data samples</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#so-if-i-have-40-samples-and-batch-size-8-they-accumulate-the-loss-and-back-prop-to-update-the-same-weight-matrix-for-all-40-samples-in">So if I have 40 samples and batch size 8 they accumulate the loss and back prop to update the same weight matrix for all 40 samples in</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#so-the-idea-is-to-learn-fixed-weights-that-represent-whole-data-set">So the idea is to learn fixed weights that represent whole data set</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#you-imagine-your-dataset-has-2-samples-cat-eat-mouse-and-mouse-eat-cat-the-eat-word-embedding-is-captured-such-that-the-model-knows-both-context-so-the-weight-matrix-is-also-adjusted">You imagine your dataset has 2 samples cat eat mouse and mouse eat cat, the eat word embedding is captured such that the model knows both context, so the weight matrix is also adjusted</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#kernels-and-multihead-can-you-explain-the-connection-of-image-filters-and-kernels-and-transformers-multi-head-are-they-related">Kernels and Multihead Can you explain the connection of image filters and kernels and transformers multi head, are they related</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#local-and-global-relationships">1. <strong>Local and Global Relationships</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#weight-sharing">2. <strong>Weight Sharing</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#multiple-representations">3. <strong>Multiple Representations</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#compositionality">4. <strong>Compositionality</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensionality-reduction-and-expansion">5. <strong>Dimensionality Reduction and Expansion</strong></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="faq">
<h1>FAQ<a class="headerlink" href="#faq" title="Permalink to this heading">#</a></h1>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#analogy-for-self-attention-03-march-2024" id="id1">Analogy for self attention (03-march-2024)</a></p></li>
<li><p><a class="reference internal" href="#bmm-versus-matmul" id="id2"><code class="docutils literal notranslate"><span class="pre">bmm</span></code> versus <code class="docutils literal notranslate"><span class="pre">matmul</span></code></a></p></li>
<li><p><a class="reference internal" href="#multi-head-is-similar-to-kernels-in-cnn" id="id3">Multi-Head is similar to kernels in CNN</a></p></li>
<li><p><a class="reference internal" href="#the-role-of-feed-forward-network" id="id4">The Role of Feed-Forward Network</a></p></li>
<li><p><a class="reference internal" href="#how-w-q-h-is-implemented-in-practice" id="id5">How <span class="math notranslate nohighlight">\(W^{q}_h\)</span> is implemented in practice?</a></p>
<ul>
<li><p><a class="reference internal" href="#do-not-split-the-embeddings-in-h-heads-instead-you-split-the-linear-transformed-embeddings" id="id6">Do not split the embeddings in H heads, instead you split the linear transformed embeddings.</a></p></li>
<li><p><a class="reference internal" href="#the-two-approaches" id="id7">The Two Approaches</a></p>
<ul>
<li><p><a class="reference internal" href="#approach-1-single-large-weight-matrix-implementation-paper-s-code" id="id8">Approach 1: Single Large Weight Matrix Implementation (Paper’s Code)</a></p></li>
<li><p><a class="reference internal" href="#approach-2-separate-weight-matrices-notation-paper-notation" id="id9">Approach 2: Separate Weight Matrices Notation (Paper Notation)</a></p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference internal" href="#transformer-s-weight-sharing" id="id10">Transformer’s Weight Sharing</a></p>
<ul>
<li><p><a class="reference internal" href="#qn-1-if-there-are-more-than-1-sample-does-it-mean-for-each-sample-there-is-a-different-weight-matrix-for-q-k-and-v" id="id11">Qn 1 If there are more than 1 sample, does it mean for each sample, there is a different weight matrix for Q, K and V?</a></p></li>
<li><p><a class="reference internal" href="#qn-2" id="id12">Qn 2</a></p>
<ul>
<li><p><a class="reference internal" href="#so-if-i-have-4-images-in-a-batch-the-weight-is-the-same-weight-and-not-4-different-weights" id="id13">So if I have 4 images in a batch the weight is the same weight and not 4 different weights</a></p></li>
<li><p><a class="reference internal" href="#so-if-my-image-has-40-images-and-batch-size-of-4-will-there-be-8-different-weights" id="id14">So if my image has 40 images and batch size of 4. Will there be 8 different weights</a></p></li>
<li><p><a class="reference internal" href="#after-training-transformer-is-the-q-weight-matrix-wq-fixed" id="id15">After training transformer is the Q weight matrix WQ fixed</a></p></li>
<li><p><a class="reference internal" href="#then-how-does-it-learn-context-when-i-pass-in-new-sentence-if-weight-is-fixed" id="id16">Then how does it learn context when I pass in new sentence if weight is fixed</a></p></li>
<li><p><a class="reference internal" href="#but-weight-matrix-is-lets-say-dxd-right-it-does-not-take-into-account-the-vocabulary-words-how-it-knows-context" id="id17">But weight matrix is let’s say DxD right it does not take into account the vocabulary words how it knows context</a></p></li>
<li><p><a class="reference internal" href="#so-in-a-sense-once-u-train-a-model-all-weights-are-fixed-and-weights-are-shared-across-all-data-samples" id="id18">So in a sense once u train a model all weights are fixed. And weights are shared across ALL data samples</a></p></li>
<li><p><a class="reference internal" href="#so-if-i-have-40-samples-and-batch-size-8-they-accumulate-the-loss-and-back-prop-to-update-the-same-weight-matrix-for-all-40-samples-in" id="id19">So if I have 40 samples and batch size 8 they accumulate the loss and back prop to update the same weight matrix for all 40 samples in</a></p></li>
<li><p><a class="reference internal" href="#so-the-idea-is-to-learn-fixed-weights-that-represent-whole-data-set" id="id20">So the idea is to learn fixed weights that represent whole data set</a></p></li>
<li><p><a class="reference internal" href="#you-imagine-your-dataset-has-2-samples-cat-eat-mouse-and-mouse-eat-cat-the-eat-word-embedding-is-captured-such-that-the-model-knows-both-context-so-the-weight-matrix-is-also-adjusted" id="id21">You imagine your dataset has 2 samples cat eat mouse and mouse eat cat, the eat word embedding is captured such that the model knows both context, so the weight matrix is also adjusted</a></p></li>
<li><p><a class="reference internal" href="#kernels-and-multihead-can-you-explain-the-connection-of-image-filters-and-kernels-and-transformers-multi-head-are-they-related" id="id22">Kernels and Multihead Can you explain the connection of image filters and kernels and transformers multi head, are they related</a></p>
<ul>
<li><p><a class="reference internal" href="#local-and-global-relationships" id="id23">1. <strong>Local and Global Relationships</strong></a></p></li>
<li><p><a class="reference internal" href="#weight-sharing" id="id24">2. <strong>Weight Sharing</strong></a></p></li>
<li><p><a class="reference internal" href="#multiple-representations" id="id25">3. <strong>Multiple Representations</strong></a></p></li>
<li><p><a class="reference internal" href="#compositionality" id="id26">4. <strong>Compositionality</strong></a></p></li>
<li><p><a class="reference internal" href="#dimensionality-reduction-and-expansion" id="id27">5. <strong>Dimensionality Reduction and Expansion</strong></a></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
<section id="analogy-for-self-attention-03-march-2024">
<h2><a class="toc-backref" href="#id1" role="doc-backlink">Analogy for self attention (03-march-2024)</a><a class="headerlink" href="#analogy-for-self-attention-03-march-2024" title="Permalink to this heading">#</a></h2>
<p>Let’s use the sentence “cat walks by the bank” to walk through the
self-attention mechanism with analogies and to clarify how it works step by
step. The sentence is tokenized into <code class="docutils literal notranslate"><span class="pre">[&quot;cat&quot;,</span> <span class="pre">&quot;walks&quot;,</span> <span class="pre">&quot;by&quot;,</span> <span class="pre">&quot;the&quot;,</span> <span class="pre">&quot;bank&quot;]</span></code> and
can be represented as an input sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> of <span class="math notranslate nohighlight">\(T=5\)</span> tokens, where we
consider each word as a token.</p>
<p>For each token, we have a vector representation of it, which is called the
embedding of the token. We can represent the embedding of the <span class="math notranslate nohighlight">\(t\)</span>-th token as
<span class="math notranslate nohighlight">\(\mathbf{z}_t \in \mathbb{R}^{1 \times D}\)</span>, where <span class="math notranslate nohighlight">\(D\)</span> is the dimension of the
embedding space. For example, the embedding of the token “cat” is represented as
<span class="math notranslate nohighlight">\(\mathbf{z}_1\)</span>, and the embedding of the token “walks” is represented as
<span class="math notranslate nohighlight">\(\mathbf{z}_2\)</span>, and so on.</p>
<p>The self-attention mechanism aims to project each of our embedding
<span class="math notranslate nohighlight">\(\mathbf{z}_t\)</span> in the input sequence into a new embedding vector, which we call
the <strong>context vector</strong> <span class="math notranslate nohighlight">\(\mathbf{c}_t \in \mathbb{R}^{1 \times D}\)</span>, which still
lies in the same dimension <span class="math notranslate nohighlight">\(D\)</span>, albeit going through some projections to
different subspaces during the process, which we will explain later.</p>
<p>This context vector <span class="math notranslate nohighlight">\(\mathbf{c}_t\)</span> is a representation of the <span class="math notranslate nohighlight">\(t\)</span>-th token in
the input sequence, which is a weighted sum of all the embeddings of the tokens
in the input sequence. The weights are calculated by the attention mechanism.</p>
<p>More concretely, the initial embedding <span class="math notranslate nohighlight">\(\mathbf{z}_1\)</span> of the token “cat” is only
holding semantic information about the token “cat” itself. However, after going
through the self-attention mechanism, the context vector <span class="math notranslate nohighlight">\(\mathbf{c}_1\)</span> of the
token “cat” is a weighted sum of all the embeddings of the tokens in the input
sequence, which means that the context vector <span class="math notranslate nohighlight">\(\mathbf{c}_1\)</span> of the token “cat”
is holding information about the token “cat” itself as well as the information
about the other tokens in the input sequence. It allows the token “cat” to have
a better understanding of itself in the context of the whole sentence (i.e.
should I, the token “cat”, pay more attention to the token “bank” as a financial
institution or a river bank and pay less attention to the token “by”?).</p>
<p>Through the self-attention process, we also project each token embedding
<span class="math notranslate nohighlight">\(\mathbf{z}_t\)</span> into a new context vector in different <strong>subspaces</strong>. This is
achieved by using three different <strong>linear projections</strong>, which are represented
by three different weight matrices <span class="math notranslate nohighlight">\(\mathbf{W}^{Q} \in \mathbb{R}^{D \times D}\)</span>,
<span class="math notranslate nohighlight">\(\mathbf{W}^{K} \in \mathbb{R}^{D \times D}\)</span>, and
<span class="math notranslate nohighlight">\(\mathbf{W}^{V} \in \mathbb{R}^{D \times D}\)</span>, where <span class="math notranslate nohighlight">\(D\)</span> is the dimension of the
embedding space. More concretely, we project the token embedding <span class="math notranslate nohighlight">\(\mathbf{z}_t\)</span>
into three different subspaces to obtain the query vector <span class="math notranslate nohighlight">\(\mathbf{q}_t\)</span>, key
vector <span class="math notranslate nohighlight">\(\mathbf{k}_t\)</span>, and value vector <span class="math notranslate nohighlight">\(\mathbf{v}_t\)</span> for the <span class="math notranslate nohighlight">\(t\)</span>-th token in
the input sequence - all of them resides in the same dimension <span class="math notranslate nohighlight">\(D\)</span>, but in
different subspaces.</p>
<div class="math notranslate nohighlight">
\[
\mathbf{q}_t = \mathbf{z}_t \mathbf{W}^{Q} \in \mathbb{R}^{1 \times D}, \quad
\mathbf{k}_t = \mathbf{z}_t \mathbf{W}^{K} \in \mathbb{R}^{1 \times D}, \quad
\mathbf{v}_t = \mathbf{z}_t \mathbf{W}^{V} \in \mathbb{R}^{1 \times D}
\]</div>
<p>And since there are <span class="math notranslate nohighlight">\(T\)</span> such tokens in the input sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, we can
stack all the query vectors <span class="math notranslate nohighlight">\(\mathbf{q}_t\)</span>, key vectors <span class="math notranslate nohighlight">\(\mathbf{k}_t\)</span>, and
value vectors <span class="math notranslate nohighlight">\(\mathbf{v}_t\)</span> into matrices
<span class="math notranslate nohighlight">\(\mathbf{Q} \in \mathbb{R}^{T \times D}\)</span>,
<span class="math notranslate nohighlight">\(\mathbf{K} \in \mathbb{R}^{T \times D}\)</span>, and
<span class="math notranslate nohighlight">\(\mathbf{V} \in \mathbb{R}^{T
\times D}\)</span>, where each row of the matrices
represents the query, key, and value vectors for each token in the input
sequence.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{Q} = \begin{bmatrix} \mathbf{q}\_1 \\ \mathbf{q}\_2 \\ \vdots \\
\mathbf{q}\_T \end{bmatrix} \in \mathbb{R}^{T \times D}, \quad \mathbf{K} =
\begin{bmatrix} \mathbf{k}\_1 \\ \mathbf{k}\_2 \\ \vdots \\ \mathbf{k}\_T
\end{bmatrix} \in \mathbb{R}^{T \times D}, \quad \mathbf{V} = \begin{bmatrix}
\mathbf{v}\_1 \\ \mathbf{v}\_2 \\ \vdots \\ \mathbf{v}\_T \end{bmatrix} \in
\mathbb{R}^{T \times D}
\end{split}\]</div>
<p>The reason is a few folds, without these <span class="math notranslate nohighlight">\(3\)</span> different linear projections, we
would not be able to calculate the attention scores between the query and key
vectors, and we would not be able to calculate the weighted sum of the value
vectors. Also, by projecting the token embeddings into different subspaces, we
are able to capture different aspects of the token embeddings, which allows the
self-attention mechanism to capture more complex relationships between the
tokens in the input sequence.</p>
<p>In essence:</p>
<ul class="simple">
<li><p><strong>Q (Query):</strong> What does “cat” want to know?</p></li>
<li><p><strong>K (Key):</strong> Who has relevant information to “cat”’s curiosity?</p></li>
<li><p><strong>V (Value):</strong> What stories does “cat” gather from others, and how much does
it take from each to understand its role in the sentence?</p></li>
</ul>
<p>The output of self-attention for “cat” now encapsulates not just “cat” but its
relationship and relevance to “walks,” “by,” “the,” “bank” in a way that no
single word could convey alone. This output then becomes the input to the next
layer, where the process can repeat, enabling the model to develop an even more
nuanced understanding.</p>
</section>
<section id="bmm-versus-matmul">
<h2><a class="toc-backref" href="#id2" role="doc-backlink"><code class="docutils literal notranslate"><span class="pre">bmm</span></code> versus <code class="docutils literal notranslate"><span class="pre">matmul</span></code></a><a class="headerlink" href="#bmm-versus-matmul" title="Permalink to this heading">#</a></h2>
<p>Both <code class="docutils literal notranslate"><span class="pre">torch.bmm</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.matmul</span></code> can be used for matrix multiplication in
PyTorch, but their use cases and behaviors are somewhat different, especially
with higher-dimensional tensors. Let’s break this down:</p>
<ol class="arabic">
<li><p><strong>torch.bmm</strong>:</p>
<ul class="simple">
<li><p>It stands for “batch matrix multiplication”.</p></li>
<li><p>It expects tensors to be of rank 3: <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">rows,</span> <span class="pre">cols)</span></code>.</p></li>
<li><p>It performs matrix multiplication for each batch between corresponding
matrices.</p></li>
</ul>
<p>For example, given two tensors <code class="docutils literal notranslate"><span class="pre">A</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(B,</span> <span class="pre">M,</span> <span class="pre">N)</span></code> and <code class="docutils literal notranslate"><span class="pre">B</span></code> of shape
<code class="docutils literal notranslate"><span class="pre">(B,</span> <span class="pre">N,</span> <span class="pre">P)</span></code>, the output will be of shape <code class="docutils literal notranslate"><span class="pre">(B,</span> <span class="pre">M,</span> <span class="pre">P)</span></code>.</p>
</li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>  <span class="c1"># Shape: (B=10, M=3, N=4)</span>
<span class="linenos">2</span><span class="n">B</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>  <span class="c1"># Shape: (B=10, N=4, P=5)</span>
<span class="linenos">3</span>
<span class="linenos">4</span><span class="c1"># Batch matrix multiplication</span>
<span class="linenos">5</span><span class="n">result_bmm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>  <span class="c1"># Output shape: (10, 3, 5)</span>
<span class="linenos">6</span><span class="n">result_matmul</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
<span class="linenos">7</span>
<span class="linenos">8</span><span class="n">torch</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_close</span><span class="p">(</span><span class="n">result_bmm</span><span class="p">,</span> <span class="n">result_matmul</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ol class="arabic" start="2">
<li><p><strong>torch.matmul</strong>:</p>
<ul class="simple">
<li><p>It’s a more general-purpose matrix multiplication function.</p></li>
<li><p>When two 3D tensors are passed, it behaves like <code class="docutils literal notranslate"><span class="pre">torch.bmm</span></code>.</p></li>
<li><p>However, it can handle tensors of rank &gt; 3 as well. When given
higher-dimensional tensors, it considers the last two dimensions as
matrices to be multiplied and broadcasts over the remaining dimensions.</p></li>
</ul>
<p>Given two tensors <code class="docutils literal notranslate"><span class="pre">A</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(X,</span> <span class="pre">Y,</span> <span class="pre">M,</span> <span class="pre">N)</span></code> and <code class="docutils literal notranslate"><span class="pre">B</span></code> of shape
<code class="docutils literal notranslate"><span class="pre">(X,</span> <span class="pre">Y,</span> <span class="pre">N,</span> <span class="pre">P)</span></code>, the output will be of shape <code class="docutils literal notranslate"><span class="pre">(X,</span> <span class="pre">Y,</span> <span class="pre">M,</span> <span class="pre">P)</span></code>.</p>
</li>
</ol>
<p>In the context of the provided code, both methods achieve the same result
because:</p>
<ul class="simple">
<li><p>The shape of the <code class="docutils literal notranslate"><span class="pre">queries</span></code> and transposed <code class="docutils literal notranslate"><span class="pre">keys</span></code> tensors matches the
expected input shape for <code class="docutils literal notranslate"><span class="pre">torch.bmm</span></code> in the <code class="docutils literal notranslate"><span class="pre">DotProductAttention</span></code> class.</p></li>
<li><p>In the <code class="docutils literal notranslate"><span class="pre">attention</span></code> function, the shape of <code class="docutils literal notranslate"><span class="pre">query</span></code> and transposed <code class="docutils literal notranslate"><span class="pre">key</span></code>
tensors is also compatible with both <code class="docutils literal notranslate"><span class="pre">torch.bmm</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.matmul</span></code>.</p></li>
</ul>
<p>So, when used for 3D tensors, <code class="docutils literal notranslate"><span class="pre">torch.bmm</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.matmul</span></code> can give the same
result. The discrepancy arises primarily with higher-dimensional tensors, where
the broadcasting behavior of <code class="docutils literal notranslate"><span class="pre">torch.matmul</span></code> distinguishes it from <code class="docutils literal notranslate"><span class="pre">torch.bmm</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="c1"># Example higher-dimensional tensors</span>
<span class="linenos">2</span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>  <span class="c1"># Shape: (X=2, Y=3, M=3, N=4)</span>
<span class="linenos">3</span><span class="n">B</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>  <span class="c1"># Shape: (X=2, Y=3, N=4, P=5)</span>
<span class="linenos">4</span>
<span class="linenos">5</span><span class="c1"># General-purpose matrix multiplication</span>
<span class="linenos">6</span><span class="n">result_matmul</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>  <span class="c1"># Output shape: (2, 3, 3, 5)</span>
<span class="linenos">7</span><span class="nb">print</span><span class="p">(</span><span class="n">result_matmul</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([2, 3, 3, 5])
</pre></div>
</div>
</div>
</div>
<p>In both cases, the last two dimensions of the tensors are treated as matrices to
be multiplied. The key difference lies in how <code class="docutils literal notranslate"><span class="pre">torch.matmul</span></code> can handle tensors
with more than three dimensions, applying the operation in a broadcasted manner
across the leading dimensions.</p>
</section>
<section id="multi-head-is-similar-to-kernels-in-cnn">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Multi-Head is similar to kernels in CNN</a><a class="headerlink" href="#multi-head-is-similar-to-kernels-in-cnn" title="Permalink to this heading">#</a></h2>
<p>The multi-head attention mechanism is similar to the convolutional layer in
convolutional neural networks. In a convolutional layer, you apply multiple
kernels to the input to obtain multiple feature maps. Similarly, in the
multi-head attention mechanism, you apply multiple attention heads to the input
to obtain multiple output vectors.</p>
</section>
<section id="the-role-of-feed-forward-network">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">The Role of Feed-Forward Network</a><a class="headerlink" href="#the-role-of-feed-forward-network" title="Permalink to this heading">#</a></h2>
<p>The Feed-Forward Network (FFN) layer, also known as the position-wise
feed-forward network, is a crucial component in the architecture of Transformer
models, found in both the encoder and decoder sections. Its primary roles and
significance are as follows:</p>
<ol class="arabic simple">
<li><p><strong>Non-linearity and Model Complexity</strong>: The FFN introduces non-linearity into
the otherwise linear operations of the Transformer model (such as
self-attention). This non-linearity is crucial for the model to learn complex
patterns. FFNs usually consist of two linear transformations with a
non-linear activation function (like ReLU or GELU) in between.</p></li>
<li><p><strong>Feature Transformation</strong>: Each position in the Transformer’s encoder and
decoder contains a vector representing the information at that position. The
FFN takes these vectors and applies the same dense layers to each position
independently. This is akin to having a separate, identical neural network
for each position in the sequence, allowing the model to modify the
representation at each position.</p></li>
<li><p><strong>Role in Different Components</strong>:</p>
<ul class="simple">
<li><p><strong>Encoder</strong>: In the encoder, the FFN works on the output of the
self-attention mechanism. It helps in integrating the information
collected through self-attention and further transforms the feature
representation.</p></li>
<li><p><strong>Decoder</strong>: In the decoder, the FFN operates after the decoder has
integrated information from the encoder output and the previous positions
in the output sequence (via masked self-attention). It aids in refining
the representation of the output sequence before generating the next
token.</p></li>
<li><p><strong>Encoder-Decoder Attention</strong>: In the part of the decoder that processes
encoder outputs (encoder-decoder attention), the FFN again serves to
transform and refine these representations, incorporating information from
the encoder into the decoder’s context.</p></li>
</ul>
</li>
<li><p><strong>Positional Independence</strong>: An important characteristic of the FFN layer is
that it does not mix information across positions. Each position is processed
independently, which is different from the attention mechanisms that mix
information across different positions. This means that while attention
layers help the model understand the relationships between different
positions in the sequence, the FFN layers allow the model to process each
position’s information more deeply.</p></li>
<li><p><strong>Increase Capacity</strong>: The FFN layers substantially increase the capacity of
the model (i.e., the model’s ability to represent complex functions) without
significantly increasing the computational complexity, since they are
relatively simple feed-forward neural networks.</p></li>
</ol>
</section>
<section id="how-w-q-h-is-implemented-in-practice">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">How <span class="math notranslate nohighlight">\(W^{q}_h\)</span> is implemented in practice?</a><a class="headerlink" href="#how-w-q-h-is-implemented-in-practice" title="Permalink to this heading">#</a></h2>
<p>The notation <span class="math notranslate nohighlight">\(W^{q}_h\)</span> is used in the paper to denote the weight matrix for the
queries (Q) of the <span class="math notranslate nohighlight">\(h\)</span>-th head. However, it’s essential to understand how this
is implemented in practice.</p>
<p>The entire process can be seen as a two-step operation:</p>
<ol class="arabic">
<li><p><strong>Apply Linear Transformations</strong>: You apply linear transformations to the
whole embeddings to create larger matrices for Q, K, V. These matrices have
dimensions that account for all heads. In practice, this can be implemented
using a single linear layer, such as:</p>
<div class="math notranslate nohighlight">
\[
    \mathbf{Q} = \mathbf{E} &#64; \mathbf{W}^q
    \]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{W}^q\)</span> has dimensions <span class="math notranslate nohighlight">\(D \times (h \cdot d_q)\)</span>.</p>
</li>
<li><p><strong>Reshape and Split</strong>: After applying the linear transformations, you reshape
and split the result into individual heads. The reshaping ensures that the
final dimensions are <span class="math notranslate nohighlight">\([\mathcal{B}, H, S, d_q]\)</span>, where <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> is the
batch size, <span class="math notranslate nohighlight">\(H\)</span> is the number of heads, <span class="math notranslate nohighlight">\(S\)</span> is the sequence length, and <span class="math notranslate nohighlight">\(d_q\)</span>
is the dimension of queries per head.</p></li>
</ol>
<p>So, while the paper uses notation like <span class="math notranslate nohighlight">\(W^{q}_h\)</span>, this doesn’t mean that you
directly apply a different linear transformation to different parts of the
embeddings. Instead, you apply a single large linear transformation to the whole
embeddings and then reshape the result to obtain the individual heads.</p>
<p>In mathematical terms, the overall operation can be seen as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
Q_{\text{{all heads}}} &amp; = \mathbf{E} &#64; \mathbf{W}^q \\
Q_{\text{{head h}}} &amp; = Q_{\text{{all heads}}}[:, h \cdot d_q : (h + 1) \cdot d_q]
\end{align*}
\end{split}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(Q_{\text{{all heads}}}\)</span> is the result of applying the linear
transformation, and <span class="math notranslate nohighlight">\(Q_{\text{{head h}}}\)</span> is the portion corresponding to the
<span class="math notranslate nohighlight">\(h\)</span>-th head, obtained by slicing along the last dimension.</p>
<section id="do-not-split-the-embeddings-in-h-heads-instead-you-split-the-linear-transformed-embeddings">
<h3><a class="toc-backref" href="#id6" role="doc-backlink">Do not split the embeddings in H heads, instead you split the linear transformed embeddings.</a><a class="headerlink" href="#do-not-split-the-embeddings-in-h-heads-instead-you-split-the-linear-transformed-embeddings" title="Permalink to this heading">#</a></h3>
<blockquote>
<div><p>Apply linear transformations to compute Q, K, V NOTE: here is an important
misconception that if you have 8 heads, then you SPLIT the embeddings into 8
parts and then apply linear transformations to each part. This is WRONG. You
apply linear transformations to the whole embeddings and then split the result
into 8 parts.</p>
</div></blockquote>
<p>You don’t split the original embeddings into <span class="math notranslate nohighlight">\(H\)</span> heads; instead, you apply
linear transformations to the original embeddings and then split the transformed
embeddings into <span class="math notranslate nohighlight">\(H\)</span> heads.</p>
<p>Here’s the step-by-step process again, highlighting this specific aspect:</p>
<ol class="arabic simple">
<li><p>Apply linear transformations for queries, keys, and values to the entire
embeddings, creating matrices <span class="math notranslate nohighlight">\(Q, K,\)</span> and <span class="math notranslate nohighlight">\(V\)</span>.</p></li>
<li><p>Split these transformed matrices into <span class="math notranslate nohighlight">\(H\)</span> different heads, each having lower
dimensions (e.g., if the original dimension is 512 and there are 8 heads,
each head will have a dimension of 64).</p></li>
<li><p>Process each head through the Scaled Dot-Product Attention mechanism.</p></li>
<li><p>Concatenate the outputs from all the heads and pass through a final linear
layer.</p></li>
</ol>
<p>The split after the linear transformations allows the model to create multiple
different projections of the input and process them independently. This enables
the model to focus on different aspects of the input across the different heads,
enhancing its ability to model complex relationships.</p>
</section>
<section id="the-two-approaches">
<h3><a class="toc-backref" href="#id7" role="doc-backlink">The Two Approaches</a><a class="headerlink" href="#the-two-approaches" title="Permalink to this heading">#</a></h3>
<p>The notation and explanation in the original papers and many articles do indeed
mention separate weight matrices for each head, such as <span class="math notranslate nohighlight">\(W^{Q}_i\)</span>, but in
implementation, it’s common to represent these separate weights within a single
large weight matrix. The notation might be different, but the mathematical
operation is equivalent.</p>
<p>Here’s how the two approaches relate:</p>
<ol class="arabic simple">
<li><p><strong>Separate Weight Matrices Notation (Paper Notation):</strong> In the theoretical
description, you can imagine having separate weight matrices <span class="math notranslate nohighlight">\(W^{Q}_i\)</span> for
each head, and then you multiply the input embeddings by each weight matrix,
applying the transformation separately for each head.</p></li>
<li><p><strong>Single Large Weight Matrix Implementation (Code):</strong> In your implementation,
you create one large weight matrix, <span class="math notranslate nohighlight">\(W_q\)</span>, that combines all the individual
weight matrices for each head. When you multiply the input embeddings by
<span class="math notranslate nohighlight">\(W_q\)</span>, you create a large transformed matrix. Then, by slicing this large
matrix, you separate it into <span class="math notranslate nohighlight">\(H\)</span> different heads, effectively applying the
individual weight matrices <span class="math notranslate nohighlight">\(W^{Q}_i\)</span> for each head.</p></li>
</ol>
<p>The two approaches are mathematically equivalent. In the second approach, the
separate weight matrices for each head are not explicitly defined as learnable
parameters in the code. Instead, they are implicitly represented within the
single large weight matrix <span class="math notranslate nohighlight">\(W_q\)</span> and separated by slicing after the linear
transformation.</p>
<p>This approach can be more efficient computationally and often aligns better with
hardware and library optimizations, but it may create confusion when comparing
to the paper’s notation. The key is to understand that the mathematical
relationships and learning dynamics are the same, even though the notation and
coding structure might differ.</p>
<section id="approach-1-single-large-weight-matrix-implementation-paper-s-code">
<h4><a class="toc-backref" href="#id8" role="doc-backlink">Approach 1: Single Large Weight Matrix Implementation (Paper’s Code)</a><a class="headerlink" href="#approach-1-single-large-weight-matrix-implementation-paper-s-code" title="Permalink to this heading">#</a></h4>
<p>In this approach, we concatenate all the individual weight matrices <span class="math notranslate nohighlight">\(W^{q}_{h}\)</span>
into one large weight matrix <span class="math notranslate nohighlight">\(W_q\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
W_q =  \begin{bmatrix} w^{q}_{1,1} &amp; w^{q}_{1,2} &amp; \ldots &amp; w^{q}_{1,D} \\
w^{q}_{2,1} &amp; w^{q}_{2,2} &amp; \ldots &amp; w^{q}_{2,D} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
w^{q}_{D,1} &amp; w^{q}_{D,2} &amp; \ldots &amp; w^{q}_{D,D} \end{bmatrix}_{D \times D}
&amp;= \begin{bmatrix} W^{q}_1 &amp; W^{q}_2 &amp; \ldots &amp; W^{q}_H \end{bmatrix} \in \mathbb{R}^{D \times D}
\end{aligned}
\end{split}\]</div>
<p>where each <span class="math notranslate nohighlight">\(W^{q}_h\)</span> is a matrix with dimensions
<span class="math notranslate nohighlight">\(D \times \frac{D}{H} = D \times d_q\)</span>.</p>
<p>In other words, if the embedding dimension is 512 and there are 8 heads, the
original <span class="math notranslate nohighlight">\(W^q\)</span> matrix is of size <span class="math notranslate nohighlight">\(512 \times 512\)</span>, and we can decompose it into
8 matrices of size <span class="math notranslate nohighlight">\(512 \times 64\)</span>, each forming a column of the original
matrix.</p>
<hr class="docutils" />
<p>Side note: if users wanna see jacobian like block matrics:</p>
<p>We can represent the matrix in blocks by grouping its elements. Here’s an
example that might suit your purpose:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
W_q &amp;=  \begin{bmatrix}
B^{q}_{1,1} &amp; B^{q}_{1,2} &amp; \ldots &amp; B^{q}_{1,H} \\
B^{q}_{2,1} &amp; B^{q}_{2,2} &amp; \ldots &amp; B^{q}_{2,H} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
B^{q}_{H,1} &amp; B^{q}_{H,2} &amp; \ldots &amp; B^{q}_{H,H} \\
\end{bmatrix}_{D \times D}
\end{aligned}
\end{split}\]</div>
<p>Where each block <span class="math notranslate nohighlight">\(B^{q}_{i,j}\)</span> is a sub-matrix of size <span class="math notranslate nohighlight">\(m \times m\)</span> (assuming
<span class="math notranslate nohighlight">\(D\)</span> is divisible by <span class="math notranslate nohighlight">\(H\)</span> and <span class="math notranslate nohighlight">\(m = \frac{D}{H}\)</span>) and can be represented as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
B^{q}_{i,j} =  \begin{bmatrix}
w^{q}_{i \cdot m - m + 1, j \cdot m - m + 1} &amp; \ldots &amp; w^{q}_{i \cdot m - m + 1, j \cdot m} \\
\vdots &amp; \ddots &amp; \vdots \\
w^{q}_{i \cdot m, j \cdot m - m + 1} &amp; \ldots &amp; w^{q}_{i \cdot m, j \cdot m}
\end{bmatrix}_{m \times m}
\end{aligned}
\end{split}\]</div>
<p>This representation can help visualize the matrix as a composition of smaller
blocks, which might be useful in certain contexts, such as when dealing with
partitioned matrices in numerical computations.</p>
<hr class="docutils" />
<p>We then multiply the embeddings by this large weight matrix:</p>
<div class="math notranslate nohighlight">
\[
Q = \mathbf{Z} \cdot W^{q}
\]</div>
<p>Then, we slice the result into <span class="math notranslate nohighlight">\(H\)</span> parts:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
Q_{h} \in \mathbb{R}^{B \times L \times d_q} &amp;= Q\left[:, :, h \cdot \frac{D}{H} : (h+1) \cdot \frac{D}{H}\right] \\
&amp;= Q\left[:, :, h \cdot d_q : (h+1) \cdot d_q\right] \\
&amp;= \mathbf{Z} \cdot W^{q}_{h}
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(W^{q}_{h}\)</span> is the submatrix of <span class="math notranslate nohighlight">\(W^{q}\)</span> that corresponds to the <span class="math notranslate nohighlight">\(h\)</span>-th
head, or in other words, let’s say <span class="math notranslate nohighlight">\(W^{q}_1\)</span>, the first head, it means
subsetting the <span class="math notranslate nohighlight">\(W^q\)</span> with rows dimension unchanged (i.e. 512), and taking the
first 64 columns, resulting in a matrix of size <span class="math notranslate nohighlight">\(512 \times 64\)</span>.</p>
</section>
<section id="approach-2-separate-weight-matrices-notation-paper-notation">
<h4><a class="toc-backref" href="#id9" role="doc-backlink">Approach 2: Separate Weight Matrices Notation (Paper Notation)</a><a class="headerlink" href="#approach-2-separate-weight-matrices-notation-paper-notation" title="Permalink to this heading">#</a></h4>
<p>Suppose we have <span class="math notranslate nohighlight">\(H\)</span> heads and our embedding matrix <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> has dimensions
<span class="math notranslate nohighlight">\(B \times L \times D\)</span>, where <span class="math notranslate nohighlight">\(B\)</span> is the batch size, <span class="math notranslate nohighlight">\(L\)</span> is the sequence length,
and <span class="math notranslate nohighlight">\(D\)</span> is the embedding dimension.</p>
<p>For each head <span class="math notranslate nohighlight">\(h\)</span>, we have a weight matrix <span class="math notranslate nohighlight">\(W^{q}_{h}\)</span> with dimensions
<span class="math notranslate nohighlight">\(D
\times \frac{D}{H} = D \times d_q\)</span>, and we apply this transformation to the
embeddings:</p>
<div class="math notranslate nohighlight">
\[
Q_{h} \in \mathbb{R}^{B \times L \times d_q} = \mathbf{Z} \cdot W^{q}_{h}
\]</div>
<p>So the confusion arises because in the code implementation we do not see an
explicit definition of the separate weight matrices <span class="math notranslate nohighlight">\(W^{q}_{h}\)</span>, but they are
implicitly represented within the single large weight matrix <span class="math notranslate nohighlight">\(W^{q}\)</span>. But
actually you can see from approach 1, the <span class="math notranslate nohighlight">\(W^q\)</span> is just a concatenation of all
the <span class="math notranslate nohighlight">\(W^{q}_{h}\)</span>, so it’s just a different way of representing the same thing.</p>
</section>
</section>
</section>
<section id="transformer-s-weight-sharing">
<h2><a class="toc-backref" href="#id10" role="doc-backlink">Transformer’s Weight Sharing</a><a class="headerlink" href="#transformer-s-weight-sharing" title="Permalink to this heading">#</a></h2>
<section id="qn-1-if-there-are-more-than-1-sample-does-it-mean-for-each-sample-there-is-a-different-weight-matrix-for-q-k-and-v">
<h3><a class="toc-backref" href="#id11" role="doc-backlink">Qn 1 If there are more than 1 sample, does it mean for each sample, there is a different weight matrix for Q, K and V?</a><a class="headerlink" href="#qn-1-if-there-are-more-than-1-sample-does-it-mean-for-each-sample-there-is-a-different-weight-matrix-for-q-k-and-v" title="Permalink to this heading">#</a></h3>
<blockquote>
<div><p>If there are more than 1 sample, does it mean for each sample, there is a
different weight matrix for Q, K and V?</p>
</div></blockquote>
<p>No, you are not correct in saying that for each sample there is a different
weight in the transformer model. The weights used to transform the input
embeddings into the query (Q), key (K), and value (V) matrices are shared across
all samples within the batch.</p>
<p>The shared weights enable the model to generalize across different samples,
allowing it to apply the learned relationships and patterns to unseen data. This
is consistent with the principles of weight sharing in deep learning, where the
same parameters are used across different parts of the input data, ensuring that
the model learns consistent representations.</p>
<ol class="arabic simple">
<li><p><strong>Shared Weights for Q, K, V:</strong></p>
<ul class="simple">
<li><p>The matrices Q, K, and V are derived from the input embeddings using three
different weight matrices <span class="math notranslate nohighlight">\(W_Q, W_K, W_V\)</span>, respectively.</p></li>
<li><p>These weight matrices are parameters that are learned during training, and
they are shared across all samples in a batch.</p></li>
<li><p>The dimensions of these matrices are the same, e.g., <span class="math notranslate nohighlight">\(W_Q: [d, d]\)</span> if the
input and query dimensions are both <span class="math notranslate nohighlight">\(d\)</span>.</p></li>
</ul>
</li>
<li><p><strong>Batch Processing:</strong></p>
<ul class="simple">
<li><p>Suppose you have a batch of <span class="math notranslate nohighlight">\(N\)</span> samples, each with sequence length <span class="math notranslate nohighlight">\(T\)</span>.</p></li>
<li><p>The input embeddings for the entire batch have dimensions <span class="math notranslate nohighlight">\([N, T, d]\)</span>.</p></li>
<li><p>When you multiply the input embeddings with the weight matrices
<span class="math notranslate nohighlight">\(W_Q, W_K, W_V\)</span>, you use the same weights for every sample in the batch.</p></li>
<li><p>So, the transformed Q, K, V matrices for the entire batch would have
dimensions <span class="math notranslate nohighlight">\([N, T, d]\)</span>, using the same <span class="math notranslate nohighlight">\(W_Q, W_K, W_V\)</span> for each sample.</p></li>
</ul>
</li>
<li><p><strong>Why Share Weights?</strong></p>
<ul class="simple">
<li><p>Sharing weights allows the model to generalize patterns learned across
different samples.</p></li>
<li><p>If each sample had different weights, the model would have a massive
number of parameters, leading to overfitting and poor generalization to
unseen data.</p></li>
<li><p>Weight sharing simplifies the model and enforces that the same
transformation is applied to all samples, which helps the model learn
consistent relationships across different sequences.</p></li>
</ul>
</li>
<li><p><strong>Backpropagation:</strong></p>
<ul class="simple">
<li><p>During training, the shared weights are updated based on the aggregated
loss across the entire batch.</p></li>
<li><p>Gradients are computed with respect to this combined loss, and the
optimization algorithm updates the shared weights accordingly.</p></li>
<li><p>Even though the weights are updated based on all samples in the batch,
they remain shared across all samples.</p></li>
</ul>
</li>
</ol>
<p>In summary, the concept of shared weights is vital in the transformer
architecture. The same weight matrices are used to transform the input
embeddings into the Q, K, V matrices for every sample in a batch. This approach
allows the model to generalize well, prevents overfitting, and makes the
training process more computationally efficient.</p>
</section>
<section id="qn-2">
<h3><a class="toc-backref" href="#id12" role="doc-backlink">Qn 2</a><a class="headerlink" href="#qn-2" title="Permalink to this heading">#</a></h3>
<p>Just like our good old image classification</p>
<p>The concept of shared weights is also true in the context of image
classification, particularly in the use of convolutional neural networks (CNNs).</p>
<ol class="arabic simple">
<li><p><strong>Convolutional Layers:</strong></p>
<ul class="simple">
<li><p>In a CNN, the convolutional layers consist of a set of learnable filters
or kernels.</p></li>
<li><p>These filters are applied to different regions of the input image,
effectively scanning the entire image to detect specific features like
edges, textures, or more complex patterns.</p></li>
<li><p>The key point is that the same filter is used across the entire image.
This means the weights within that filter are shared across different
spatial locations of the input.</p></li>
</ul>
</li>
<li><p><strong>Benefits of Weight Sharing:</strong></p>
<ul class="simple">
<li><p>The sharing of weights drastically reduces the number of parameters, as
you don’t need a separate set of weights for each location in the image.</p></li>
<li><p>This reduction in parameters not only makes the model more computationally
efficient but also helps prevent overfitting.</p></li>
<li><p>Weight sharing enables the model to detect a specific feature regardless
of its location in the image, contributing to the translation invariance
of the model.</p></li>
</ul>
</li>
<li><p><strong>Pooling Layers:</strong></p>
<ul class="simple">
<li><p>Pooling layers, often used in CNNs, also exhibit a form of weight sharing.
A pooling operation (e.g., max pooling) applies the same operation to
different regions of the feature map, further contributing to translation
invariance.</p></li>
</ul>
</li>
<li><p><strong>Fully Connected Layers:</strong></p>
<ul class="simple">
<li><p>In the fully connected layers of a CNN, the weights are not shared across
different parts of the input, as they are in the convolutional layers.</p></li>
<li><p>However, the principles of batching and shared weights across different
samples in a batch still apply.</p></li>
</ul>
</li>
</ol>
<p>In summary, the concept of weight sharing in image classification, specifically
within CNNs, is central to their efficiency and effectiveness. By using the same
set of weights across different regions of the input image, CNNs can learn
spatial hierarchies of features with a manageable number of parameters, enabling
them to generalize well to unseen images.</p>
<section id="so-if-i-have-4-images-in-a-batch-the-weight-is-the-same-weight-and-not-4-different-weights">
<h4><a class="toc-backref" href="#id13" role="doc-backlink">So if I have 4 images in a batch the weight is the same weight and not 4 different weights</a><a class="headerlink" href="#so-if-i-have-4-images-in-a-batch-the-weight-is-the-same-weight-and-not-4-different-weights" title="Permalink to this heading">#</a></h4>
<p>If you have a batch of 4 images and you are using a neural network model such as
a Convolutional Neural Network (CNN) for image classification, the weights
within the layers of the network are shared across all 4 images in the batch.</p>
<p>In other words, the same filters in the convolutional layers, the same weights
in the fully connected layers, and other learnable parameters are applied to
each of the 4 images. There are not 4 different sets of weights for the 4
different images.</p>
<p>This weight sharing across different samples in the batch is what enables the
model to generalize across various instances of data and is fundamental to the
training of neural networks. It ensures that the patterns and features learned
from one image are applicable to other images as well, contributing to the
model’s ability to classify unseen images accurately.</p>
</section>
<section id="so-if-my-image-has-40-images-and-batch-size-of-4-will-there-be-8-different-weights">
<h4><a class="toc-backref" href="#id14" role="doc-backlink">So if my image has 40 images and batch size of 4. Will there be 8 different weights</a><a class="headerlink" href="#so-if-my-image-has-40-images-and-batch-size-of-4-will-there-be-8-different-weights" title="Permalink to this heading">#</a></h4>
<p>No, even if your dataset consists of 40 images and you are training with a batch
size of 4, there will not be 8 different sets of weights for the 8 different
batches. The same weights are used across all batches and all images in the
entire dataset.</p>
<p>Here’s how the process works:</p>
<ol class="arabic simple">
<li><p><strong>Initialization</strong>: The weights in the model (e.g., filters in convolutional
layers, weights in fully connected layers) are initialized, often randomly.</p></li>
<li><p><strong>Batch Processing</strong>: You divide the 40 images into 8 batches, each
containing 4 images.</p>
<ul class="simple">
<li><p>For each batch, the forward pass computes the predictions using the same
weights for all 4 images in the batch.</p></li>
<li><p>The loss is computed for that batch, and then the backpropagation
algorithm calculates the gradients based on that loss.</p></li>
<li><p>The gradients are used to update the weights.</p></li>
</ul>
</li>
<li><p><strong>Weight Updates</strong>: The key point is that these weight updates are applied to
the same set of weights, regardless of which batch you are processing. There
are not separate weights for each batch.</p></li>
<li><p><strong>Iterative Process</strong>: As you iterate through multiple epochs, continually
cycling through the batches, the weights continue to be updated in a way that
minimizes the loss on the entire dataset of 40 images.</p></li>
<li><p><strong>Generalization</strong>: By using the same weights across all images, the model
learns to generalize the patterns and features that are relevant for the
classification task across the entire dataset.</p></li>
</ol>
<p>So, in summary, there is only one set of weights in the model, and these weights
are shared across all 40 images, regardless of how they are batched. The concept
of weight sharing is fundamental to neural network training, and it applies
across all images and all batches.</p>
</section>
<section id="after-training-transformer-is-the-q-weight-matrix-wq-fixed">
<h4><a class="toc-backref" href="#id15" role="doc-backlink">After training transformer is the Q weight matrix WQ fixed</a><a class="headerlink" href="#after-training-transformer-is-the-q-weight-matrix-wq-fixed" title="Permalink to this heading">#</a></h4>
<p>Yes, after training a transformer model, the weight matrix <span class="math notranslate nohighlight">\(W_Q\)</span> used to
generate the query (Q) representations becomes fixed. The same applies to the
weight matrices <span class="math notranslate nohighlight">\(W_K\)</span> and <span class="math notranslate nohighlight">\(W_V\)</span> used for the keys (K) and values (V),
respectively.</p>
<p>During the training phase, these weight matrices are learned and optimized to
minimize the loss function. They are updated iteratively based on the gradients
computed during backpropagation.</p>
<p>Once training is complete, the weights are no longer updated, and they become
fixed parameters of the model. These fixed weights can then be used to generate
Q, K, and V for any given input when using the model for inference (i.e., making
predictions on unseen data).</p>
<p>If you want to further adapt or fine-tune the model on new data, you would
continue the training process, and the weights would be updated accordingly. But
in a standard training scenario where you train the model and then use it for
inference, the weights, including <span class="math notranslate nohighlight">\(W_Q\)</span>, become fixed after training is
finished.</p>
</section>
<section id="then-how-does-it-learn-context-when-i-pass-in-new-sentence-if-weight-is-fixed">
<h4><a class="toc-backref" href="#id16" role="doc-backlink">Then how does it learn context when I pass in new sentence if weight is fixed</a><a class="headerlink" href="#then-how-does-it-learn-context-when-i-pass-in-new-sentence-if-weight-is-fixed" title="Permalink to this heading">#</a></h4>
<p>The ability to capture context when processing a new sentence, even with fixed
weights, is a result of the design of the transformer architecture. Here’s how
it works:</p>
<ol class="arabic simple">
<li><p><strong>Learned Representations</strong>: During training, the transformer learns to
create useful representations of the input data by optimizing the weights,
including <span class="math notranslate nohighlight">\(W_Q\)</span>, <span class="math notranslate nohighlight">\(W_K\)</span>, and <span class="math notranslate nohighlight">\(W_V\)</span>. These representations capture the
relationships and patterns in the training data.</p></li>
<li><p><strong>Contextual Encoding</strong>: The fixed weights are applied to new sentences to
generate Q, K, and V matrices. The self-attention mechanism then uses these
matrices to compute attention scores, allowing each token in the sentence to
attend to every other token. This captures the contextual relationships
within the sentence.</p></li>
<li><p><strong>Positional Encoding</strong>: Transformers include positional encodings, which are
added to the input embeddings to provide information about the position of
tokens in the sequence. This ensures that the model can recognize the order
of words, which is essential for understanding syntax and semantics.</p></li>
<li><p><strong>Layering and Stacking</strong>: The transformer consists of multiple layers of
self-attention and feed-forward neural networks. As the input progresses
through these layers, it becomes increasingly abstract and contextualized.
The higher layers are capable of capturing more complex relationships and
semantics.</p></li>
<li><p><strong>Generalization</strong>: The fixed weights have been trained to generalize from
the training data to unseen data. When you pass a new sentence through the
trained model, it uses the learned weights to encode the new sentence in a
way that reflects the patterns and relationships it learned during training.
This allows it to understand and generate context-aware representations of
new sentences.</p></li>
<li><p><strong>No Memory of Previous Sentences</strong>: It’s important to note that transformers
do not have a memory of previous sentences (they are not stateful like RNNs).
The context they capture is within the current input sentence or sequence of
sentences. They do not use information from previous unrelated inputs.</p></li>
</ol>
<p>In summary, the fixed weights in a trained transformer model are used to apply
the learned patterns and relationships to new sentences. The model’s
architecture, including the self-attention mechanism and positional encoding,
enables it to capture the context within a given sentence or sequence. Even with
fixed weights, the transformer can understand and respond to the contextual
relationships in new inputs.</p>
</section>
<section id="but-weight-matrix-is-lets-say-dxd-right-it-does-not-take-into-account-the-vocabulary-words-how-it-knows-context">
<h4><a class="toc-backref" href="#id17" role="doc-backlink">But weight matrix is let’s say DxD right it does not take into account the vocabulary words how it knows context</a><a class="headerlink" href="#but-weight-matrix-is-lets-say-dxd-right-it-does-not-take-into-account-the-vocabulary-words-how-it-knows-context" title="Permalink to this heading">#</a></h4>
<p>It is correct that the weight matrices for the queries (Q), keys (K), and values
(V), such as the <span class="math notranslate nohighlight">\(D \times D\)</span> matrix <span class="math notranslate nohighlight">\(W_Q\)</span>, don’t directly take into account the
vocabulary words. The contextual understanding in transformers comes from the
combination of the architecture’s design and the processing of input embeddings.
Here’s a step-by-step explanation of how it works:</p>
<ol class="arabic simple">
<li><p><strong>Input Embeddings</strong>: Each word (or subword) from the vocabulary is first
mapped to a continuous vector using an embedding layer. These embeddings are
typically of size <span class="math notranslate nohighlight">\(D\)</span>, and they capture the semantic meaning of the words.</p></li>
<li><p><strong>Positional Encoding</strong>: Positional encodings are added to the embeddings to
give the model information about the order of the words in the sentence.</p></li>
<li><p><strong>Linear Transformation</strong>: The embeddings are then linearly transformed using
the weight matrices <span class="math notranslate nohighlight">\(W_Q\)</span>, <span class="math notranslate nohighlight">\(W_K\)</span>, and <span class="math notranslate nohighlight">\(W_V\)</span>. These transformations project
the embeddings into a space where the self-attention mechanism can operate.</p></li>
<li><p><strong>Self-Attention Mechanism</strong>:</p>
<ul class="simple">
<li><p>The queries (Q), keys (K), and values (V) are used to calculate attention
scores.</p></li>
<li><p>The attention scores determine how much each word in the sentence should
pay attention to every other word.</p></li>
<li><p>This attention mechanism allows the model to capture relationships between
words, regardless of their position in the sentence, thus understanding
the context.</p></li>
</ul>
</li>
<li><p><strong>Aggregated Context</strong>: The attention scores are used to take a weighted sum
of the value vectors (V), creating an aggregated representation for each word
that takes into account the entire context of the sentence.</p></li>
<li><p><strong>Layering</strong>: As the information passes through multiple layers of the
transformer, these context-aware representations are further refined,
allowing the model to capture more complex relationships and semantics.</p></li>
<li><p><strong>No Direct Vocabulary Dependence</strong>: The weight matrices themselves don’t
have a direct dependence on specific words in the vocabulary. Instead, they
operate on continuous vector representations, and the understanding of
context comes from the architecture’s ability to model relationships between
these vectors.</p></li>
</ol>
<p>In summary, the transformer doesn’t “know” the context through the weight
matrices alone but through the combination of embeddings, linear
transformations, and the self-attention mechanism. The weight matrices are part
of this process, transforming the embeddings in a way that allows the model to
effectively apply self-attention and capture the contextual relationships within
the input sentence.</p>
</section>
<section id="so-in-a-sense-once-u-train-a-model-all-weights-are-fixed-and-weights-are-shared-across-all-data-samples">
<h4><a class="toc-backref" href="#id18" role="doc-backlink">So in a sense once u train a model all weights are fixed. And weights are shared across ALL data samples</a><a class="headerlink" href="#so-in-a-sense-once-u-train-a-model-all-weights-are-fixed-and-weights-are-shared-across-all-data-samples" title="Permalink to this heading">#</a></h4>
<p>Yes. Once you train a model, such as a transformer or a convolutional neural
network, all the weights are fixed unless you decide to fine-tune or continue
training on new data. These weights include not only those used in the
self-attention mechanism of a transformer but also any other learnable
parameters in the network.</p>
<p>Furthermore, the weights are shared across all data samples. This means that the
same set of weights is used to process every sample in your dataset, whether
it’s during training or inference.</p>
<p>This weight-sharing across samples is a fundamental aspect of how these models
generalize from the training data to unseen data. By using the same weights for
all samples, the model learns to recognize patterns and relationships that are
consistent across different instances of the data, allowing it to make accurate
predictions or representations even for new, unseen samples.</p>
</section>
<section id="so-if-i-have-40-samples-and-batch-size-8-they-accumulate-the-loss-and-back-prop-to-update-the-same-weight-matrix-for-all-40-samples-in">
<h4><a class="toc-backref" href="#id19" role="doc-backlink">So if I have 40 samples and batch size 8 they accumulate the loss and back prop to update the same weight matrix for all 40 samples in</a><a class="headerlink" href="#so-if-i-have-40-samples-and-batch-size-8-they-accumulate-the-loss-and-back-prop-to-update-the-same-weight-matrix-for-all-40-samples-in" title="Permalink to this heading">#</a></h4>
<p>Yes, that’s correct. If you have 40 samples and a batch size of 8, the training
process would generally proceed as follows:</p>
<ol class="arabic simple">
<li><p><strong>Divide into Batches</strong>: The 40 samples are divided into 5 batches, each
containing 8 samples.</p></li>
<li><p><strong>Forward Pass</strong>: For each batch, the model performs a forward pass,
processing the 8 samples using the same weight matrix and other parameters.
The predictions are compared to the true labels to compute the loss for that
batch.</p></li>
<li><p><strong>Accumulate Gradients</strong>: The gradients of the loss with respect to the
weights are computed using backpropagation. Depending on the optimization
algorithm, these gradients might be accumulated across batches or used
immediately to update the weights.</p></li>
<li><p><strong>Update Weights</strong>: The accumulated gradients are used to update the same
weight matrix and other learnable parameters in the model. This update is
done according to the optimization algorithm used, such as stochastic
gradient descent (SGD) or one of its variants like Adam.</p></li>
<li><p><strong>Iterate</strong>: This process is repeated for each of the 5 batches, cycling
through the entire dataset. The weights continue to be updated based on the
accumulated gradients from all the samples.</p></li>
<li><p><strong>Multiple Epochs</strong>: Often, this entire process is repeated for multiple
epochs, meaning that the entire dataset is passed through the network
multiple times, continually updating the weights to minimize the loss.</p></li>
</ol>
<p>The key is that the same weight matrix and other parameters are used for all 40
samples and updated based on the loss computed over all of them. By processing
the data in batches and updating the weights based on the accumulated gradients,
the model is able to learn from the entire dataset, generalizing the patterns
and relationships in the data to make accurate predictions on unseen samples.</p>
</section>
<section id="so-the-idea-is-to-learn-fixed-weights-that-represent-whole-data-set">
<h4><a class="toc-backref" href="#id20" role="doc-backlink">So the idea is to learn fixed weights that represent whole data set</a><a class="headerlink" href="#so-the-idea-is-to-learn-fixed-weights-that-represent-whole-data-set" title="Permalink to this heading">#</a></h4>
<p>Yes, exactly. The goal of training a neural network, including models like
transformers, is to learn a set of fixed weights that effectively represent the
underlying patterns and relationships in the entire dataset. These weights are
optimized to minimize a loss function, which quantifies the difference between
the model’s predictions and the true labels or values.</p>
<p>By learning fixed weights that capture the general characteristics of the data,
the model can:</p>
<ol class="arabic simple">
<li><p><strong>Generalize to Unseen Data</strong>: The learned weights allow the model to make
predictions or representations for new, unseen samples that were not part of
the training dataset. The model’s ability to generalize is crucial for its
performance on real-world tasks.</p></li>
<li><p><strong>Share Weights Across Samples</strong>: The same set of weights is used for all
samples in the dataset, both during training and inference. This
weight-sharing enables the model to recognize consistent patterns across
different instances of the data.</p></li>
<li><p><strong>Efficiently Represent Complex Relationships</strong>: Through the architecture’s
design and the training process, the model learns to encode complex
relationships and semantics within the fixed weights. This allows it to
understand and process new inputs in a context-aware manner, even though the
weights are no longer changing after training.</p></li>
<li><p><strong>Apply to Various Tasks</strong>: Once trained, the fixed weights can be used for
various tasks, such as classification, regression, translation, or other
forms of prediction, depending on the model’s design and training objective.</p></li>
</ol>
<p>In summary, the idea of learning fixed weights that represent the whole dataset
is central to the training and functioning of neural network models. It allows
them to capture and generalize the essential characteristics of the data,
enabling accurate and meaningful predictions or representations for a wide range
of applications.</p>
</section>
<section id="you-imagine-your-dataset-has-2-samples-cat-eat-mouse-and-mouse-eat-cat-the-eat-word-embedding-is-captured-such-that-the-model-knows-both-context-so-the-weight-matrix-is-also-adjusted">
<h4><a class="toc-backref" href="#id21" role="doc-backlink">You imagine your dataset has 2 samples cat eat mouse and mouse eat cat, the eat word embedding is captured such that the model knows both context, so the weight matrix is also adjusted</a><a class="headerlink" href="#you-imagine-your-dataset-has-2-samples-cat-eat-mouse-and-mouse-eat-cat-the-eat-word-embedding-is-captured-such-that-the-model-knows-both-context-so-the-weight-matrix-is-also-adjusted" title="Permalink to this heading">#</a></h4>
<p>Yes. In the scenario you described, with the two samples “cat eat mouse” and
“mouse eat cat,” the model would learn to understand the context in which the
word “eat” appears, even though it’s common to both sentences.</p>
<p>Here’s how it would typically work:</p>
<ol class="arabic simple">
<li><p><strong>Word Embeddings</strong>: The individual words are first mapped to continuous
vector representations (embeddings). The embedding for the word “eat” may
start with a pre-trained or randomly initialized value.</p></li>
<li><p><strong>Contextual Understanding</strong>: Through the training process, the model’s
architecture, such as the self-attention mechanism in a transformer, allows
it to recognize the relationships between the words in each sentence. It
understands that “eat” has different subjects and objects in the two samples.</p></li>
<li><p><strong>Weight Adjustment</strong>: The weights, including those in the matrices used for
the self-attention mechanism, are adjusted during training to capture these
contextual relationships. They are optimized to minimize the loss across both
samples, ensuring that the model learns to recognize the different contexts
in which “eat” appears.</p></li>
<li><p><strong>Generalized Representations</strong>: As a result, the model learns to represent
the word “eat” in a way that takes into account its context within each
sentence. The fixed weights capture this understanding, allowing the model to
recognize and respond to the varying relationships between words, even in
unseen data.</p></li>
<li><p><strong>Weight Sharing</strong>: The same set of weights is used to process both samples,
and all other samples in the dataset, allowing the model to generalize the
patterns and relationships it learns.</p></li>
</ol>
<p>In this way, the model learns to represent the word “eat” (and all other words
in the vocabulary) in a context-aware manner. The fixed weights that are learned
during training enable it to understand the different ways that “eat” relates to
other words in the sentences, recognizing both “cat eat mouse” and “mouse eat
cat” as valid and distinct constructions.</p>
</section>
<section id="kernels-and-multihead-can-you-explain-the-connection-of-image-filters-and-kernels-and-transformers-multi-head-are-they-related">
<h4><a class="toc-backref" href="#id22" role="doc-backlink">Kernels and Multihead Can you explain the connection of image filters and kernels and transformers multi head, are they related</a><a class="headerlink" href="#kernels-and-multihead-can-you-explain-the-connection-of-image-filters-and-kernels-and-transformers-multi-head-are-they-related" title="Permalink to this heading">#</a></h4>
<p>While image filters and kernels in convolutional neural networks (CNNs) and the
multi-head attention mechanism in transformers have different purposes and
architectures, they do share some underlying principles and connections. Here’s
how they relate:</p>
<section id="local-and-global-relationships">
<h5><a class="toc-backref" href="#id23" role="doc-backlink">1. <strong>Local and Global Relationships</strong></a><a class="headerlink" href="#local-and-global-relationships" title="Permalink to this heading">#</a></h5>
<ul class="simple">
<li><p><strong>Image Filters and Kernels (CNNs)</strong>: Convolutional filters (also known as
kernels) capture local spatial patterns in images. By sliding over the input
image, they learn to detect features such as edges, textures, and shapes
within a localized region.</p></li>
<li><p><strong>Multi-Head Attention (Transformers)</strong>: Multi-head attention allows the
model to focus on different parts of the input simultaneously, capturing
both local and global relationships between words. Each head can attend to
different relationships, allowing for a more nuanced understanding of the
context.</p></li>
</ul>
</section>
<section id="weight-sharing">
<h5><a class="toc-backref" href="#id24" role="doc-backlink">2. <strong>Weight Sharing</strong></a><a class="headerlink" href="#weight-sharing" title="Permalink to this heading">#</a></h5>
<ul class="simple">
<li><p><strong>CNNs</strong>: Convolutional kernels share weights across spatial locations,
allowing the network to recognize the same feature regardless of its
position in the image.</p></li>
<li><p><strong>Transformers</strong>: In multi-head attention, the weights are shared across the
entire sequence, allowing the model to recognize patterns and relationships
regardless of the position of words in the sequence.</p></li>
</ul>
</section>
<section id="multiple-representations">
<h5><a class="toc-backref" href="#id25" role="doc-backlink">3. <strong>Multiple Representations</strong></a><a class="headerlink" href="#multiple-representations" title="Permalink to this heading">#</a></h5>
<ul class="simple">
<li><p><strong>CNNs</strong>: Different filters learn to recognize different features within an
image, leading to multiple feature maps that collectively represent various
aspects of the image.</p></li>
<li><p><strong>Transformers</strong>: Different attention heads in multi-head attention can
learn to focus on different relationships within the input, providing
multiple, parallel representations of the context.</p></li>
</ul>
</section>
<section id="compositionality">
<h5><a class="toc-backref" href="#id26" role="doc-backlink">4. <strong>Compositionality</strong></a><a class="headerlink" href="#compositionality" title="Permalink to this heading">#</a></h5>
<ul class="simple">
<li><p><strong>CNNs</strong>: Hierarchical stacking of convolutional layers allows the model to
build increasingly complex and abstract representations of the image,
starting from simple edges to higher-level concepts like objects.</p></li>
<li><p><strong>Transformers</strong>: The layered structure of transformers also allows for
increasingly complex and abstract representations of the input, starting
from individual word embeddings to higher-level syntactic and semantic
understandings.</p></li>
</ul>
</section>
<section id="dimensionality-reduction-and-expansion">
<h5><a class="toc-backref" href="#id27" role="doc-backlink">5. <strong>Dimensionality Reduction and Expansion</strong></a><a class="headerlink" href="#dimensionality-reduction-and-expansion" title="Permalink to this heading">#</a></h5>
<ul class="simple">
<li><p><strong>CNNs</strong>: Pooling layers and striding can reduce the spatial dimensions,
while different numbers of filters can expand the feature dimensions.</p></li>
<li><p><strong>Transformers</strong>: The dimensionality of the attention heads and the number
of heads can be configured to control the size and complexity of the
representations.</p></li>
</ul>
<p>In summary, while the specific mechanisms and purposes differ between image
filters and kernels in CNNs and the multi-head attention in transformers, they
do share principles of capturing local and global relationships, weight sharing,
multiple parallel representations, compositionality, and control over
dimensionality. These connections reflect broader themes in deep learning, where
different architectures leverage similar concepts to learn from various types of
data.</p>
</section>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./transformer"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="concept.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Concept</p>
      </div>
    </a>
    <a class="right-next"
       href="decoder/shakespeare.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Shakespeare</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analogy-for-self-attention-03-march-2024">Analogy for self attention (03-march-2024)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bmm-versus-matmul"><code class="docutils literal notranslate"><span class="pre">bmm</span></code> versus <code class="docutils literal notranslate"><span class="pre">matmul</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-is-similar-to-kernels-in-cnn">Multi-Head is similar to kernels in CNN</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-role-of-feed-forward-network">The Role of Feed-Forward Network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-w-q-h-is-implemented-in-practice">How <span class="math notranslate nohighlight">\(W^{q}_h\)</span> is implemented in practice?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#do-not-split-the-embeddings-in-h-heads-instead-you-split-the-linear-transformed-embeddings">Do not split the embeddings in H heads, instead you split the linear transformed embeddings.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-two-approaches">The Two Approaches</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#approach-1-single-large-weight-matrix-implementation-paper-s-code">Approach 1: Single Large Weight Matrix Implementation (Paper’s Code)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#approach-2-separate-weight-matrices-notation-paper-notation">Approach 2: Separate Weight Matrices Notation (Paper Notation)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer-s-weight-sharing">Transformer’s Weight Sharing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#qn-1-if-there-are-more-than-1-sample-does-it-mean-for-each-sample-there-is-a-different-weight-matrix-for-q-k-and-v">Qn 1 If there are more than 1 sample, does it mean for each sample, there is a different weight matrix for Q, K and V?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#qn-2">Qn 2</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#so-if-i-have-4-images-in-a-batch-the-weight-is-the-same-weight-and-not-4-different-weights">So if I have 4 images in a batch the weight is the same weight and not 4 different weights</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#so-if-my-image-has-40-images-and-batch-size-of-4-will-there-be-8-different-weights">So if my image has 40 images and batch size of 4. Will there be 8 different weights</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#after-training-transformer-is-the-q-weight-matrix-wq-fixed">After training transformer is the Q weight matrix WQ fixed</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#then-how-does-it-learn-context-when-i-pass-in-new-sentence-if-weight-is-fixed">Then how does it learn context when I pass in new sentence if weight is fixed</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#but-weight-matrix-is-lets-say-dxd-right-it-does-not-take-into-account-the-vocabulary-words-how-it-knows-context">But weight matrix is let’s say DxD right it does not take into account the vocabulary words how it knows context</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#so-in-a-sense-once-u-train-a-model-all-weights-are-fixed-and-weights-are-shared-across-all-data-samples">So in a sense once u train a model all weights are fixed. And weights are shared across ALL data samples</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#so-if-i-have-40-samples-and-batch-size-8-they-accumulate-the-loss-and-back-prop-to-update-the-same-weight-matrix-for-all-40-samples-in">So if I have 40 samples and batch size 8 they accumulate the loss and back prop to update the same weight matrix for all 40 samples in</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#so-the-idea-is-to-learn-fixed-weights-that-represent-whole-data-set">So the idea is to learn fixed weights that represent whole data set</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#you-imagine-your-dataset-has-2-samples-cat-eat-mouse-and-mouse-eat-cat-the-eat-word-embedding-is-captured-such-that-the-model-knows-both-context-so-the-weight-matrix-is-also-adjusted">You imagine your dataset has 2 samples cat eat mouse and mouse eat cat, the eat word embedding is captured such that the model knows both context, so the weight matrix is also adjusted</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#kernels-and-multihead-can-you-explain-the-connection-of-image-filters-and-kernels-and-transformers-multi-head-are-they-related">Kernels and Multihead Can you explain the connection of image filters and kernels and transformers multi head, are they related</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#local-and-global-relationships">1. <strong>Local and Global Relationships</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#weight-sharing">2. <strong>Weight Sharing</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#multiple-representations">3. <strong>Multiple Representations</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#compositionality">4. <strong>Compositionality</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensionality-reduction-and-expansion">5. <strong>Dimensionality Reduction and Expansion</strong></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gao Hongnan
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>