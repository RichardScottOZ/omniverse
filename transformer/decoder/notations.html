

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Notations &#8212; Omniverse</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script src="../../_static/tabs.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'transformer/decoder/notations';</script>
    <link rel="canonical" href="https://www.gaohongnan.com/transformer/decoder/notations.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="The Concept of Generative Pre-trained Transformers (GPT)" href="concept.html" />
    <link rel="prev" title="Generative Pre-trained Transformers" href="intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Omniverse - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Omniverse - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Notations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../notations/machine_learning.html">Machine Learning Notations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Generative Pre-trained Transformer</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="intro.html">Generative Pre-trained Transformers</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Notations</a></li>
<li class="toctree-l2"><a class="reference internal" href="concept.html">The Concept of Generative Pre-trained Transformers (GPT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="implementation.html">The Implementation of Generative Pre-trained Transformers (GPT)</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../deep_learning/training_chronicles/intro.html">Training Chronicles</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../deep_learning/training_chronicles/loss.html">The Loss Landscape</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Software Engineering</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/devops/continuous-integration/concept.html">Continuous Integration (CI) Workflow</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/devops/continuous-integration/styling.html">Styling, Formatting, and Linting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/devops/continuous-integration/testing.html">Testing</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../software_engineering/design_patterns/dependency-inversion-principle.html">Dependency Inversion Principle</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/concurrency_parallelism_asynchronous/intro.html">Concurrency, Parallelism and Asynchronous Programming</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/concurrency_parallelism_asynchronous/generator_yield.html">A Rudimentary Introduction to Generator and Yield in Python</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/serving/restful_api/intro.html">RESTful API</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/serving/restful_api/application_banking.html">Application: Designing a RESTful Banking API with FastAPI and SQLAlchemy</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Computer Science</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../computer_science/type_theory/intro.html">Type Theory, A Very Rudimentary Introduction</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/01-subtypes.html">Subtypes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/02-type-safety.html">Type Safety</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/03-subsumption.html">Subsumption</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/04-generics.html">Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/05-typevar-bound-constraints.html">Bound and Constraint in Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/06-invariance-covariance-contravariance.html">Invariance, Covariance and Contravariance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/07-pep-3124-overloading.html">Function Overloading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/08-pep-661-sentinel-values.html">Sentinel Types</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data Structures and Algorithms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/complexity_analysis/intro.html">Complexity Analysis</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/complexity_analysis/master_theorem.html">Master Theorem </a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/stack/intro.html">Stack</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/stack/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/searching_algorithms/linear_search/intro.html">Linear Search</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/linear_search/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/intro.html">Binary Search</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/problems/875-koko-eating-bananas.html">Koko Eating Bananas</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Linear Algebra</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../linear_algebra/01_preliminaries/intro.html">Preliminaries</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/01_preliminaries/01-fields.html">Fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/01_preliminaries/02-systems-of-linear-equations.html">Systems of Linear Equations</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../linear_algebra/02_vectors/intro.html">Vectors</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/01-vector-definition.html">Vector and Its Definition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/02-vector-operation.html">Vector and Its Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/03-vector-norm.html">Vector Norm and Distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/04-vector-products.html">A First Look at Vector Products</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References, Resources and Roadmap</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../bibliography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/transformer/decoder/notations.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Notations</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensions-and-indexing">Dimensions and Indexing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#general-notations">General Notations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vocabulary">Vocabulary</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-sequence">Input Sequence</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#token-to-index-and-index-to-token-mappings">Token to Index, and Index to Token Mappings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot-representation-of-input-sequence-mathbf-x">One-Hot Representation of Input Sequence <span class="math notranslate nohighlight">\(\mathbf{X}\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathbf-w-e-embedding-matrix"><span class="math notranslate nohighlight">\(\mathbf{W}_{e}\)</span>: Embedding Matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathbf-z-output-of-the-embedding-layer"><span class="math notranslate nohighlight">\(\mathbf{Z}\)</span>: Output of the Embedding Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathbf-p-positional-encoding-layer"><span class="math notranslate nohighlight">\(\mathbf{P}\)</span>: Positional Encoding Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tilde-mathbf-z-output-of-the-positional-encoding-layer"><span class="math notranslate nohighlight">\(\tilde{\mathbf{Z}}\)</span>: Output of the Positional Encoding Layer</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-notations">Attention Notations</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="notations">
<h1>Notations<a class="headerlink" href="#notations" title="Permalink to this heading">#</a></h1>
<p><a class="reference external" href="https://twitter.com/gaohongnan"><img alt="Twitter Handle" src="https://img.shields.io/badge/Twitter-&#64;gaohongnan-blue?style=social&amp;logo=twitter" /></a>
<a class="reference external" href="https://linkedin.com/in/gao-hongnan"><img alt="LinkedIn Profile" src="https://img.shields.io/badge/&#64;gaohongnan-blue?style=social&amp;logo=linkedin" /></a>
<a class="reference external" href="https://github.com/gao-hongnan"><img alt="GitHub Profile" src="https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&amp;logo=github" /></a></p>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#dimensions-and-indexing" id="id2">Dimensions and Indexing</a></p></li>
<li><p><a class="reference internal" href="#general-notations" id="id3">General Notations</a></p>
<ul>
<li><p><a class="reference internal" href="#vocabulary" id="id4">Vocabulary</a></p></li>
<li><p><a class="reference internal" href="#input-sequence" id="id5">Input Sequence</a></p></li>
<li><p><a class="reference internal" href="#token-to-index-and-index-to-token-mappings" id="id6">Token to Index, and Index to Token Mappings</a></p></li>
<li><p><a class="reference internal" href="#one-hot-representation-of-input-sequence-mathbf-x" id="id7">One-Hot Representation of Input Sequence <span class="math notranslate nohighlight">\(\mathbf{X}\)</span></a></p></li>
<li><p><a class="reference internal" href="#mathbf-w-e-embedding-matrix" id="id8"><span class="math notranslate nohighlight">\(\mathbf{W}_{e}\)</span>: Embedding Matrix</a></p></li>
<li><p><a class="reference internal" href="#mathbf-z-output-of-the-embedding-layer" id="id9"><span class="math notranslate nohighlight">\(\mathbf{Z}\)</span>: Output of the Embedding Layer</a></p></li>
<li><p><a class="reference internal" href="#mathbf-p-positional-encoding-layer" id="id10"><span class="math notranslate nohighlight">\(\mathbf{P}\)</span>: Positional Encoding Layer</a></p></li>
<li><p><a class="reference internal" href="#tilde-mathbf-z-output-of-the-positional-encoding-layer" id="id11"><span class="math notranslate nohighlight">\(\tilde{\mathbf{Z}}\)</span>: Output of the Positional Encoding Layer</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#attention-notations" id="id12">Attention Notations</a></p></li>
</ul>
</nav>
<section id="dimensions-and-indexing">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Dimensions and Indexing</a><a class="headerlink" href="#dimensions-and-indexing" title="Permalink to this heading">#</a></h2>
<p>This section outlines the common dimensions and indexing conventions utilized in
the Transformer model. For specific notations related to attention mechanisms,
refer to the <a class="reference internal" href="#attention-notations"><span class="std std-ref">Attention Notations</span></a> section.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{B}\)</span>: The minibatch size.</p></li>
<li><p><span class="math notranslate nohighlight">\(D\)</span>: Embedding dimension. In the original Transformer paper, this is
represented as <span class="math notranslate nohighlight">\(d_{\text{model}}\)</span>.</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(d\)</span>: Index within the embedding vector, where <span class="math notranslate nohighlight">\(0 \leq d &lt; D\)</span>.</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(L\)</span>: Sequence length.</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(i\)</span>: Positional index of a token within the sequence, where
<span class="math notranslate nohighlight">\(0 \leq i &lt; L\)</span>.</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(V\)</span>: Size of the vocabulary.</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(j\)</span>: Index of a word in the vocabulary, where <span class="math notranslate nohighlight">\(0 \leq j &lt; V\)</span>.</p></li>
</ul>
</li>
</ul>
</section>
<section id="general-notations">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">General Notations</a><a class="headerlink" href="#general-notations" title="Permalink to this heading">#</a></h2>
<section id="vocabulary">
<h3><a class="toc-backref" href="#id4" role="doc-backlink">Vocabulary</a><a class="headerlink" href="#vocabulary" title="Permalink to this heading">#</a></h3>
<p><span class="math notranslate nohighlight">\(\mathcal{V}\)</span>: The set of all unique words in the vocabulary, defined as:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{V} = \{v_1, v_2, \ldots, v_V\}
\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(V\)</span> (denoted as <span class="math notranslate nohighlight">\(|\mathcal{V}|\)</span>): The size of the vocabulary.</p></li>
<li><p><span class="math notranslate nohighlight">\(v_j\)</span>: A unique word in the vocabulary <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>, where
<span class="math notranslate nohighlight">\(v_j \in \mathcal{V}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(j\)</span>: The index of a word in <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>, explicitly defined as
<span class="math notranslate nohighlight">\(1 \leq j \leq V\)</span>.</p></li>
</ul>
<p>For example, consider the following sentences in the training set:</p>
<ul class="simple">
<li><p>“cat eat mouse”</p></li>
<li><p>“dog chase cat”</p></li>
<li><p>“mouse eat cheese”</p></li>
</ul>
<p>The resulting vocabulary <span class="math notranslate nohighlight">\(\mathcal{V}\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{V} = \{\text{cat}, \text{eat}, \text{mouse}, \text{dog}, \text{chase}, \text{cheese}\}
\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(V = 6\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(v_1 = \text{cat}, v_2 = \text{eat}, v_3 = \text{mouse}, v_4 = \text{dog}, v_5 = \text{chase}, v_6 = \text{cheese}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(j = 1, 2, \ldots, 6\)</span>.</p></li>
</ul>
<p>Note: Depending on the transformer model, special tokens (e.g., <code class="docutils literal notranslate"><span class="pre">[PAD]</span></code>,
<code class="docutils literal notranslate"><span class="pre">[CLS]</span></code>, <code class="docutils literal notranslate"><span class="pre">[BOS]</span></code>, <code class="docutils literal notranslate"><span class="pre">[EOS]</span></code>, <code class="docutils literal notranslate"><span class="pre">[UNK]</span></code>, etc.) may also be included in <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>.</p>
</section>
<section id="input-sequence">
<h3><a class="toc-backref" href="#id5" role="doc-backlink">Input Sequence</a><a class="headerlink" href="#input-sequence" title="Permalink to this heading">#</a></h3>
<p><span class="math notranslate nohighlight">\(\mathbf{X}\)</span>: The input sequence, defined as:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{X} = (x_1, x_2, \ldots, x_L)
\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(L\)</span>: The length of the sequence.</p></li>
<li><p><span class="math notranslate nohighlight">\(x_i\)</span>: A <strong>token</strong> at position <span class="math notranslate nohighlight">\(i\)</span> in the sequence, represented as an
integer in the set <span class="math notranslate nohighlight">\(\{0, 1, \ldots, V-1\}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(i\)</span>: The index of a token in <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, where <span class="math notranslate nohighlight">\(1 \leq i \leq L\)</span>.</p></li>
</ul>
</section>
<section id="token-to-index-and-index-to-token-mappings">
<h3><a class="toc-backref" href="#id6" role="doc-backlink">Token to Index, and Index to Token Mappings</a><a class="headerlink" href="#token-to-index-and-index-to-token-mappings" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f_{\text{stoi}}\)</span>: The function mapping a token in the sequence to its index
in the vocabulary. For a token <span class="math notranslate nohighlight">\(x_i\)</span>, <span class="math notranslate nohighlight">\(f_{\text{stoi}}(x_i) = j\)</span> means the
token <span class="math notranslate nohighlight">\(x_i\)</span> corresponds to the <span class="math notranslate nohighlight">\(j\)</span>-th word in the vocabulary <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>.</p></li>
</ul>
</section>
<section id="one-hot-representation-of-input-sequence-mathbf-x">
<h3><a class="toc-backref" href="#id7" role="doc-backlink">One-Hot Representation of Input Sequence <span class="math notranslate nohighlight">\(\mathbf{X}\)</span></a><a class="headerlink" href="#one-hot-representation-of-input-sequence-mathbf-x" title="Permalink to this heading">#</a></h3>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{O}\)</span>: one-hot representation of the input sequence <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.
This is a <span class="math notranslate nohighlight">\(L \times V\)</span> matrix, where each row represents a token in the
sequence and each column corresponds to a unique word in the vocabulary
<span class="math notranslate nohighlight">\(\mathcal{V}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{aligned}
    \mathbf{O} &amp;= \begin{bmatrix} o_{1,1} &amp; o_{1,2} &amp; \cdots &amp; o_{1,V} \\ o_{2,1} &amp; o_{2,2} &amp; \cdots &amp; o_{2,V} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ o_{L,1} &amp; o_{L,2} &amp; \cdots &amp; o_{L,V} \end{bmatrix} \in \mathbb{R}^{L \times V} \\
    &amp;= \begin{bmatrix} \text{---} &amp; \mathbf{o}_{1, :} &amp; \text{---} \\ \text{---} &amp; \mathbf{o}_{2, :} &amp; \text{---} \\ &amp; \vdots &amp; \\ \text{---} &amp; \mathbf{o}_{L, :} &amp; \text{---} \end{bmatrix} \in \mathbb{R}^{L \times V}
    \end{aligned}
    \end{split}\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(L\)</span>: is the sequence length.</p></li>
<li><p><span class="math notranslate nohighlight">\(V\)</span>: is the vocabulary size.</p></li>
<li><p><span class="math notranslate nohighlight">\(o_{i, j}\)</span>: is the one-hot encoded element at position <span class="math notranslate nohighlight">\(i, j\)</span>. For a
given token <span class="math notranslate nohighlight">\(x_i\)</span> at the <span class="math notranslate nohighlight">\(i\)</span>-th position in the sequence <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>,
if <span class="math notranslate nohighlight">\(f_{\text{stoi}}(x_i)=j\)</span>, then the element at position <span class="math notranslate nohighlight">\(j\)</span> in the
one-hot vector for token <span class="math notranslate nohighlight">\(x_i\)</span> is 1, and all other elements are 0.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{o}_{i, :}\)</span>: is the one-hot encoded vector for the token <span class="math notranslate nohighlight">\(x_i\)</span>
at the <span class="math notranslate nohighlight">\(i\)</span>-th position in the sequence <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. This row form is
more important than column form.</p></li>
</ul>
</li>
</ul>
</section>
<section id="mathbf-w-e-embedding-matrix">
<h3><a class="toc-backref" href="#id8" role="doc-backlink"><span class="math notranslate nohighlight">\(\mathbf{W}_{e}\)</span>: Embedding Matrix</a><a class="headerlink" href="#mathbf-w-e-embedding-matrix" title="Permalink to this heading">#</a></h3>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}_{e}\)</span>: is the embedding matrix defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \mathbf{W}_{e} = \begin{bmatrix} e_{1,1} &amp; e_{1,2} &amp; \cdots &amp; e_{1,D} \\ e_{2,1} &amp; e_{2,2} &amp; \cdots &amp; e_{2,D} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ e_{V,1} &amp; e_{V,2} &amp; \cdots &amp; e_{V,D} \end{bmatrix} \in \mathbb{R}^{V \times D}
    \end{split}\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(V\)</span>: is the vocabulary size.</p></li>
<li><p><span class="math notranslate nohighlight">\(D\)</span>: is the embedding dimension.</p></li>
<li><p><span class="math notranslate nohighlight">\(e_{j, d}\)</span>: is the embedding element at position <span class="math notranslate nohighlight">\(j, d\)</span>. For a word
<span class="math notranslate nohighlight">\(v_j\)</span> in the vocabulary <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>, the corresponding row in
<span class="math notranslate nohighlight">\(\mathbf{W}_{e}\)</span> is the embedding vector for that word.</p></li>
</ul>
</li>
</ul>
</section>
<section id="mathbf-z-output-of-the-embedding-layer">
<h3><a class="toc-backref" href="#id9" role="doc-backlink"><span class="math notranslate nohighlight">\(\mathbf{Z}\)</span>: Output of the Embedding Layer</a><a class="headerlink" href="#mathbf-z-output-of-the-embedding-layer" title="Permalink to this heading">#</a></h3>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{Z}\)</span>: is the output tensor of the embedding layer, obtained by
matrix multiplying <span class="math notranslate nohighlight">\(\mathbf{O}\)</span> with <span class="math notranslate nohighlight">\(\mathbf{W}_{e}\)</span>, and it is defined as:</p>
<div class="math notranslate nohighlight">
\[
    \mathbf{Z} = \mathbf{O} \cdot \mathbf{W}_{e}
    \]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{aligned}
    \mathbf{Z} &amp;= \mathbf{O} \cdot \mathbf{W}_{e} \\
    &amp;= \begin{bmatrix} z_{1,1} &amp; z_{1,2} &amp; \cdots &amp; z_{1,D} \\ z_{2,1} &amp; z_{2,2} &amp; \cdots &amp; z_{2,D} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ z_{L,1} &amp; z_{L,2} &amp; \cdots &amp; z_{L,D} \end{bmatrix} \in \mathbb{R}^{L \times D} \\
    &amp;= \begin{bmatrix} \text{---} &amp; \mathbf{z}_{1,:} &amp; \text{---} \\ \text{---} &amp; \mathbf{z}_{2,:} &amp; \text{---} \\ &amp; \vdots &amp; \\ \text{---} &amp; \mathbf{z}_{L,:} &amp; \text{---} \end{bmatrix} \in \mathbb{R}^{L \times D}
    \end{aligned}
    \end{split}\]</div>
<p>where</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(L\)</span>: is the sequence length.</p></li>
<li><p><span class="math notranslate nohighlight">\(D\)</span>: is the embedding dimension.</p></li>
<li><p><span class="math notranslate nohighlight">\(z_{i, d}\)</span>: is the element at position <span class="math notranslate nohighlight">\(i, d\)</span> in the tensor
<span class="math notranslate nohighlight">\(\mathbf{Z}\)</span>. For a token <span class="math notranslate nohighlight">\(x_i\)</span> at the <span class="math notranslate nohighlight">\(i\)</span>-th position in the sequence,
<span class="math notranslate nohighlight">\(z_{i, :}\)</span> is the <span class="math notranslate nohighlight">\(D\)</span> dimensional embedding vector for that token.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{z}_{i, :}\)</span>: is the <span class="math notranslate nohighlight">\(D\)</span> dimensional embedding vector for the
token <span class="math notranslate nohighlight">\(x_i\)</span> at the <span class="math notranslate nohighlight">\(i\)</span>-th position in the sequence.</p>
<p>In this context, each token in the sequence is represented by a <span class="math notranslate nohighlight">\(D\)</span>
dimensional vector. So, the output tensor <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> captures the
dense representation of the sequence. Each token in the sequence is
replaced by its corresponding embedding vector from the embedding matrix
<span class="math notranslate nohighlight">\(\mathbf{W}_{e}\)</span>.</p>
<p>As before, the output tensor <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> carries semantic information
about the tokens in the sequence. The closer two vectors are in this
embedding space, the more semantically similar they are.</p>
</li>
</ul>
</li>
</ul>
</section>
<section id="mathbf-p-positional-encoding-layer">
<h3><a class="toc-backref" href="#id10" role="doc-backlink"><span class="math notranslate nohighlight">\(\mathbf{P}\)</span>: Positional Encoding Layer</a><a class="headerlink" href="#mathbf-p-positional-encoding-layer" title="Permalink to this heading">#</a></h3>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{P}\)</span>: is the positional encoding tensor, created with sinusoidal
functions of different frequencies:</p>
<p>Each position <span class="math notranslate nohighlight">\(i\)</span> in the sequence has a corresponding positional encoding
vector <span class="math notranslate nohighlight">\(p_{i, :}\)</span> of length <span class="math notranslate nohighlight">\(D\)</span> (the same as the embedding dimension). The
elements of this vector are generated as follows:</p>
<div class="math notranslate nohighlight">
\[
    p_{i, 2i} = \sin\left(\frac{i}{10000^{2i / D}}\right)
    \]</div>
<div class="math notranslate nohighlight">
\[
    p_{i, 2i + 1} = \cos\left(\frac{i}{10000^{2i / D}}\right)
    \]</div>
<p>for each <span class="math notranslate nohighlight">\(i\)</span> such that <span class="math notranslate nohighlight">\(2i &lt; D\)</span> and <span class="math notranslate nohighlight">\(2i + 1 &lt; D\)</span>.</p>
<p>Thus, the entire tensor <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \mathbf{P} = \begin{bmatrix} p_{1,1} &amp; p_{1,2} &amp; \cdots &amp; p_{1,D} \\ p_{2,1} &amp; p_{2,2} &amp; \cdots &amp; p_{2,D} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ p_{L,1} &amp; p_{L,2} &amp; \cdots &amp; p_{L,D} \end{bmatrix} \in \mathbb{R}^{L \times D}
    \end{split}\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(L\)</span>: is the sequence length.</p></li>
<li><p><span class="math notranslate nohighlight">\(D\)</span>: is the embedding dimension.</p></li>
<li><p><span class="math notranslate nohighlight">\(p_{i, d}\)</span>: is the element at position <span class="math notranslate nohighlight">\(i, d\)</span> in the tensor
<span class="math notranslate nohighlight">\(\mathbf{P}\)</span>.</p></li>
</ul>
<p>Note that <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> is independent of <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span>, and it’s computed
based on the positional encoding formula used in transformers, which uses
sinusoidal functions of different frequencies.</p>
</li>
</ul>
</section>
<section id="tilde-mathbf-z-output-of-the-positional-encoding-layer">
<h3><a class="toc-backref" href="#id11" role="doc-backlink"><span class="math notranslate nohighlight">\(\tilde{\mathbf{Z}}\)</span>: Output of the Positional Encoding Layer</a><a class="headerlink" href="#tilde-mathbf-z-output-of-the-positional-encoding-layer" title="Permalink to this heading">#</a></h3>
<ul>
<li><p><span class="math notranslate nohighlight">\(\tilde{\mathbf{Z}}\)</span>: After computing the positional encoding tensor
<span class="math notranslate nohighlight">\(\mathbf{P}\)</span>, we can update our original embeddings tensor <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> to
include positional information:</p>
<div class="math notranslate nohighlight">
\[
    \tilde{\mathbf{Z}} := \mathbf{Z} + \mathbf{P}
    \]</div>
<p>This operation adds the positional encodings to the original embeddings,
giving the final embeddings that are passed to subsequent layers in the
Transformer model.</p>
<p>For simplicity of notation, we will use <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> to refer to
<span class="math notranslate nohighlight">\(\tilde{\mathbf{Z}}\)</span>.</p>
</li>
</ul>
</section>
</section>
<section id="attention-notations">
<span id="id1"></span><h2><a class="toc-backref" href="#id12" role="doc-backlink">Attention Notations</a><a class="headerlink" href="#attention-notations" title="Permalink to this heading">#</a></h2>
<ul>
<li><p><span class="math notranslate nohighlight">\(H\)</span>: Number of attention heads.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(h\)</span>: Index of the attention head.</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(d_k = D/H\)</span>: Dimension of the keys. In the multi-head attention case, this
would typically be <span class="math notranslate nohighlight">\(D/H\)</span> where <span class="math notranslate nohighlight">\(D\)</span> is the dimensionality of input embeddings
and <span class="math notranslate nohighlight">\(H\)</span> is the number of attention heads.</p></li>
<li><p><span class="math notranslate nohighlight">\(d_q = D/H\)</span>: Dimension of the queries. Also usually set equal to <span class="math notranslate nohighlight">\(d_k\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(d_v = D/H\)</span>: Dimension of the values. Usually set equal to <span class="math notranslate nohighlight">\(d_k\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}^{Q} \in \mathbb{R}^{D \times H \cdot d_q = D \times D}\)</span>: The
query weight matrix for all heads. It is used to transform the embeddings
<span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> into query representations.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}^{K} \in \mathbb{R}^{D \times H \cdot d_k = D \times D}\)</span>: The key
weight matrix for all heads. It is used to transform the embeddings
<span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> into key representations.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}^{V} \in \mathbb{R}^{D \times H \cdot d_v = D \times D}\)</span>: The
value weight matrix for all heads. It is used to transform the embeddings
<span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> into value representations.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}_{h}^{Q} \in \mathbb{R}^{D \times d_q}\)</span>: The query weight matrix
for the <span class="math notranslate nohighlight">\(h\)</span>-th head. It is used to transform the embeddings <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span>
into query representations for the <span class="math notranslate nohighlight">\(h\)</span>-th head.</p>
<ul class="simple">
<li><p>Important that this matrix collapses to <span class="math notranslate nohighlight">\(\mathbf{W}_{1}^q\)</span> when <span class="math notranslate nohighlight">\(H=1\)</span>
and has shape <span class="math notranslate nohighlight">\(\mathbb{R}^{D \times D}\)</span>.</p></li>
<li><p>Note that this weight matrix is derived from <span class="math notranslate nohighlight">\(W^q\)</span>.</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}_{h}^{K} \in \mathbb{R}^{D \times d_k}\)</span>: The key weight matrix
for the <span class="math notranslate nohighlight">\(h\)</span>-th head. It is used to transform the embeddings <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span>
into key representations for the <span class="math notranslate nohighlight">\(h\)</span>-th head.</p>
<ul class="simple">
<li><p>Important that this matrix collapses to <span class="math notranslate nohighlight">\(\mathbf{W}_{1}^k\)</span> when <span class="math notranslate nohighlight">\(H=1\)</span>
and has shape <span class="math notranslate nohighlight">\(\mathbb{R}^{D \times D}\)</span> since <span class="math notranslate nohighlight">\(d_k = D/H = D/1 = D\)</span>.</p></li>
<li><p>Note that this weight matrix is derived from <span class="math notranslate nohighlight">\(W^k\)</span>.</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}_{h}^{V} \in \mathbb{R}^{D \times d_v}\)</span>: The value weight matrix
for the <span class="math notranslate nohighlight">\(h\)</span>-th head. It is used to transform the embeddings <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span>
into value representations for the <span class="math notranslate nohighlight">\(h\)</span>-th head.</p>
<ul class="simple">
<li><p>Important that this matrix collapses to <span class="math notranslate nohighlight">\(\mathbf{W}_{1}^v\)</span> when <span class="math notranslate nohighlight">\(H=1\)</span>
and has shape <span class="math notranslate nohighlight">\(\mathbb{R}^{D \times D}\)</span>.</p></li>
<li><p>Note that this weight matrix is derived from <span class="math notranslate nohighlight">\(W^v\)</span>.</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{Q} = \mathbf{Z} \mathbf{W}^{Q} \in \mathbb{R}^{L \times D}\)</span>: The
query matrix. It contains the query representations for all the tokens in
the sequence. This is the matrix that is used to compute the attention
scores.</p>
<ul class="simple">
<li><p>Each row of the matrix <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span> is a query vector <span class="math notranslate nohighlight">\(\mathbf{q}_{i}\)</span>
for the token at position <span class="math notranslate nohighlight">\(i\)</span> in the sequence.</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{Q}_h = \mathbf{Z} \mathbf{W}_h^q \in \mathbb{R}^{L \times d_q}\)</span>:
The query matrix for the <span class="math notranslate nohighlight">\(h\)</span>-th head. It contains the query representations
for all the tokens in the sequence. This is the matrix that is used to
compute the attention scores for the <span class="math notranslate nohighlight">\(h\)</span>-th head.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{K} = \mathbf{Z} \mathbf{W}^{K} \in \mathbb{R}^{L \times D}\)</span>: The
key matrix. It contains the key representations for all the tokens in the
sequence. This is the matrix that is used to compute the attention scores.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{K}_h = \mathbf{Z} \mathbf{W}_h^k \in \mathbb{R}^{L \times d_k}\)</span>:
The key matrix for the <span class="math notranslate nohighlight">\(h\)</span>-th head. It contains the key representations for
all the tokens in the sequence. This is the matrix that is used to compute
the attention scores for the <span class="math notranslate nohighlight">\(h\)</span>-th head.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{V} = \mathbf{Z} \mathbf{W}^{V} \in \mathbb{R}^{L \times D}\)</span>: The
value matrix. It contains the value representations for all the tokens in
the sequence. This is the matrix where we apply the attention scores to
compute the weighted average of the values.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{V}_h = \mathbf{Z} \mathbf{W}_h^v \in \mathbb{R}^{L \times d_v}\)</span>:
The value matrix for the <span class="math notranslate nohighlight">\(h\)</span>-th head. It contains the value representations
for all the tokens in the sequence. This is the matrix where we apply the
attention scores to compute the weighted average of the values for the
<span class="math notranslate nohighlight">\(h\)</span>-th head.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{q}_{i} = \mathbf{Q}_{i, :} \in \mathbb{R}^{d}\)</span>: The query vector
for the token at position <span class="math notranslate nohighlight">\(i\)</span> in the sequence.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{k}_{i} = \mathbf{K}_{i, :} \in \mathbb{R}^{d}\)</span>: The key vector for
the token at position <span class="math notranslate nohighlight">\(i\)</span> in the sequence.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{v}_{i} = \mathbf{V}_{i, :} \in \mathbb{R}^{d}\)</span>: The value vector
for the token at position <span class="math notranslate nohighlight">\(i\)</span> in the sequence.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{L \times L}\)</span>: The attention matrix. It contains
the attention scores for all the tokens in the sequence. It is computed as:</p>
<div class="math notranslate nohighlight">
\[
    \mathbf{A} = \text{softmax}\left( \frac{\mathbf{Q} \mathbf{K}^T}{\sqrt{d_k}} \right)
    \]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(L\)</span>: is the sequence length.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{Q} \in \mathbb{R}^{L \times D}\)</span>: is the query matrix.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{K} \in \mathbb{R}^{L \times D}\)</span>: is the key matrix.</p></li>
<li><p><span class="math notranslate nohighlight">\(\sqrt{d_k}\)</span>: is the scaling factor.</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{softmax}(\cdot)\)</span>: is the softmax function applied row-wise.</p></li>
<li><p>More concretely, this is the <strong>self-attention matrix</strong> between an input
sequence <span class="math notranslate nohighlight">\(\mathbf{X} = (x_1, x_2, ..., x_L)\)</span> and itself. Each row in the
matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is the attention scores for a token in the sequence.
The attention scores are computed by comparing the query vector for a
token with the key vectors for all the tokens in the sequence.</p></li>
<li><p>For instance, if the input sequence is “cat eat mouse”, then the <span class="math notranslate nohighlight">\(L=3\)</span>,
and the attention matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>’s first row is the attention
scores of the word cat with all other words, (cat &amp; cat, cat &amp; eat, cat
&amp; mouse). Similarly, the second row is the attention scores of the word
eat with all other words, (eat &amp; cat, eat &amp; eat, eat &amp; mouse). Lastly,
the third row is the attention scores of the word mouse with all other
words, (mouse &amp; cat, mouse &amp; eat, mouse &amp; mouse).</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(a_{i, j} \in \mathbf{A}\)</span>: The attention score between the query <span class="math notranslate nohighlight">\(i\)</span> and the
key <span class="math notranslate nohighlight">\(j\)</span> in the sequence (please do not be confused with the <span class="math notranslate nohighlight">\(j\)</span> index in
vocabulary!). It is computed as:</p>
<div class="math notranslate nohighlight">
\[
    a_{i, j} = \text{softmax}\left(\frac{\mathbf{q}_{i} \mathbf{k}_{j}^T}{\sqrt{d_k}}\right)
    \]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{q}_{i} \in \mathbb{R}^{d}\)</span>: is the query vector for the <span class="math notranslate nohighlight">\(i\)</span>-th
token in the sequence.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{k}_{j} \in \mathbb{R}^{d}\)</span>: is the key vector for the <span class="math notranslate nohighlight">\(j\)</span>-th
token in the sequence.</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(f(\cdot)\)</span>: Attention function (such as additive attention or scaled
dot-product attention).</p>
<ul>
<li><p>Should we find a better notation?</p>
<p>The scaled dot-product attention function <span class="math notranslate nohighlight">\(f(\cdot)\)</span> can be formulated
as:</p>
<div class="math notranslate nohighlight">
\[
        \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) := f(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left( \frac{\mathbf{Q} \mathbf{K}^T}{\sqrt{d_k}} \right) \mathbf{V} \in \mathbb{R}^{L \times D}
        \]</div>
<p>or you can also substitute <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> to get the same result:</p>
<div class="math notranslate nohighlight">
\[
        \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) := f(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \mathbf{A} \mathbf{V}
        \]</div>
<p>For the <span class="math notranslate nohighlight">\(h\)</span>-th head, it can be represented as:</p>
<div class="math notranslate nohighlight">
\[
        f(\mathbf{Q}_h, \mathbf{K}_h, \mathbf{V}_h) = \text{softmax}\left( \frac{\mathbf{Q}_h \mathbf{K}_h^T}{\sqrt{d_k}} \right) \mathbf{V}_h
        \]</div>
<p>In these formulas, <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{K}\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{V}\)</span> are the
query, key, and value matrices, respectively. The function
<span class="math notranslate nohighlight">\(\text{softmax}(\cdot)\)</span> is applied row-wise. The division by
<span class="math notranslate nohighlight">\(\sqrt{d_k}\)</span> is a scaling factor that helps in training stability.</p>
</li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}^{O} \in \mathbb{R}^{D \times D}\)</span>: This is the output weight
matrix that linearly transforms the concatenation of the outputs from all
attention heads to produce the final output of the multi-head attention
mechanism.</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{context_vector}\)</span> is the output of one single head.</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{context_vector_concat}\)</span> is the concatenation of all heads. Consider
use symbols here.</p></li>
<li><p>MHA outputs:
<span class="math notranslate nohighlight">\(\text{context_vector_concat} \in \mathbb{R}^{L \times D} &#64; \mathbf{W}^{O} \in \mathbb{R}^{D \times D} = \mathbf{O} \in \mathbb{R}^{L \times D}\)</span>
represents the final output of the multi-head attention layer. It’s the
result of applying the linear transformation defined by <span class="math notranslate nohighlight">\(\mathbf{W}^{O}\)</span> to
the concatenated outputs of all attention heads.</p>
<p>Abuse of notation again with <span class="math notranslate nohighlight">\(\mathbf{O}\)</span>.</p>
</li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./transformer/decoder"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Generative Pre-trained Transformers</p>
      </div>
    </a>
    <a class="right-next"
       href="concept.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">The Concept of Generative Pre-trained Transformers (GPT)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensions-and-indexing">Dimensions and Indexing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#general-notations">General Notations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vocabulary">Vocabulary</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-sequence">Input Sequence</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#token-to-index-and-index-to-token-mappings">Token to Index, and Index to Token Mappings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot-representation-of-input-sequence-mathbf-x">One-Hot Representation of Input Sequence <span class="math notranslate nohighlight">\(\mathbf{X}\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathbf-w-e-embedding-matrix"><span class="math notranslate nohighlight">\(\mathbf{W}_{e}\)</span>: Embedding Matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathbf-z-output-of-the-embedding-layer"><span class="math notranslate nohighlight">\(\mathbf{Z}\)</span>: Output of the Embedding Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathbf-p-positional-encoding-layer"><span class="math notranslate nohighlight">\(\mathbf{P}\)</span>: Positional Encoding Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tilde-mathbf-z-output-of-the-positional-encoding-layer"><span class="math notranslate nohighlight">\(\tilde{\mathbf{Z}}\)</span>: Output of the Positional Encoding Layer</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-notations">Attention Notations</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gao Hongnan
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>