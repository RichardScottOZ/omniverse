
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Training a Mini-GPT to Learn Two-Digit Addition &#8212; Omniverse</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=bb35926c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=888ff710"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script src="../../_static/tabs.js?v=3ee01567"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'transformer/decoder/adder';</script>
    <link rel="canonical" href="https://www.gaohongnan.com/transformer/decoder/adder.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="How to Calculate the Number of FLOPs in GPT-2" href="../../playbook/how_to_calculate_flops_in_gpt2.html" />
    <link rel="prev" title="The Implementation of Generative Pre-trained Transformers (GPT)" href="implementation.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Omniverse - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Omniverse - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    🌌 Omniverse: A Journey Through Knowledge
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Notations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../notations/machine_learning.html">Machine Learning Notations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Generative Pre-trained Transformer</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="intro.html">Generative Pre-trained Transformers</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="notations.html">Notations</a></li>
<li class="toctree-l2"><a class="reference internal" href="concept.html">The Concept of Generative Pre-trained Transformers (GPT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="implementation.html">The Implementation of Generative Pre-trained Transformers (GPT)</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Training a Mini-GPT to Learn Two-Digit Addition</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Playbook</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../playbook/how_to_calculate_flops_in_gpt2.html">How to Calculate the Number of FLOPs in GPT-2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/how_to_inspect_function_and_class_signatures.html">How to Inspect Function and Class Signatures in Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/why_cosine_warmup_scheduler.html">Stabilizing Training with Warmup and Gradual Cosine Annealing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/softmax_preserves_order_not_invariant_scaling.html">Softmax Preserves Order, But Not Invariant Under Scaling</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../deep_learning/training_chronicles/intro.html">Training Chronicles</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../deep_learning/training_chronicles/loss.html">The Loss Landscape</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Software Engineering</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/devops/continuous-integration/concept.html">Continuous Integration (CI) Workflow</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/devops/continuous-integration/styling.html">Styling, Formatting, and Linting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/devops/continuous-integration/testing.html">Testing</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../software_engineering/design_patterns/dependency-inversion-principle.html">Dependency Inversion Principle</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/concurrency_parallelism_asynchronous/intro.html">Concurrency, Parallelism and Asynchronous Programming</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/concurrency_parallelism_asynchronous/generator_yield.html">A Rudimentary Introduction to Generator and Yield in Python</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/serving/restful_api/intro.html">RESTful API</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/serving/restful_api/application_banking.html">Application: Designing a RESTful Banking API with FastAPI and SQLAlchemy</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Computer Science</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../computer_science/type_theory/intro.html">Type Theory, A Very Rudimentary Introduction</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/01-subtypes.html">Subtypes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/02-type-safety.html">Type Safety</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/03-subsumption.html">Subsumption</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/04-generics.html">Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/05-typevar-bound-constraints.html">Bound and Constraint in Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/06-invariance-covariance-contravariance.html">Invariance, Covariance and Contravariance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/07-pep-3124-overloading.html">Function Overloading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/08-pep-661-sentinel-values.html">Sentinel Types</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data Structures and Algorithms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/complexity_analysis/intro.html">Complexity Analysis</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/complexity_analysis/master_theorem.html">Master Theorem </a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/stack/intro.html">Stack</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/stack/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/searching_algorithms/linear_search/intro.html">Linear Search</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/linear_search/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/intro.html">Binary Search</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/problems/875-koko-eating-bananas.html">Koko Eating Bananas</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Linear Algebra</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../linear_algebra/01_preliminaries/intro.html">Preliminaries</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/01_preliminaries/01-fields.html">Fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/01_preliminaries/02-systems-of-linear-equations.html">Systems of Linear Equations</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../linear_algebra/02_vectors/intro.html">Vectors</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/01-vector-definition.html">Vector and Its Definition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/02-vector-operation.html">Vector and Its Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/03-vector-norm.html">Vector Norm and Distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/04-vector-products.html">A First Look at Vector Products</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References, Resources and Roadmap</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../bibliography.html">Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../citations.html">IEEE (Style) Citations</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/gao-hongnan/omniverse/blob/main/omniverse/transformer/decoder/adder.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/gao-hongnan/omniverse" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/gao-hongnan/omniverse/issues/new?title=Issue%20on%20page%20%2Ftransformer/decoder/adder.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/transformer/decoder/adder.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Training a Mini-GPT to Learn Two-Digit Addition</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">Motivation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#config">Config</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reproducibility">Reproducibility</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vocabulary">Vocabulary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenization">Tokenization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-dataset">Create Dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#encoding-strategy-overview">Encoding Strategy Overview</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#construct-batches-collate-function-and-dataloader">Construct Batches, Collate Function and DataLoader</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-and-target">Input and Target</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#target-padding-mask">Target Padding Mask</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#future-mask">Future Mask</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#merge-padding-and-future-masks">Merge Padding and Future Masks</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#first-sample-first-token">First Sample First Token</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#first-sample-fourth-token">First Sample Fourth Token</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#further-add-a-singleton-dimension-in-target-masks">Further Add a Singleton Dimension in Target Masks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-mask-our-target-in-adder">Why mask our target in Adder?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#split-to-train-valid-test">Split to Train-Valid-Test</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-dataloader">Create DataLoader</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model">Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-paradigm">Training Paradigm</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizer">Optimizer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-scheduler">Learning Rate Scheduler</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss">Loss</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multiheadattention"><a id="toc1_10_5_"></a><span class="xref myst">MultiHeadAttention</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-primer"><a id="toc1_10_5_1_"></a><span class="xref myst">A Primer</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-loss-is-computed"><a id="toc1_10_7_"></a><span class="xref myst">How Loss is Computed?</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#masking-and-ignore-index">Masking and Ignore Index</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-with-gpt-like-model"><a id="toc1_12_"></a><span class="xref myst">Training with GPT-like Model</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-computation"><a id="toc1_12_1_"></a><span class="xref myst">Loss Computation</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example"><a id="toc1_12_2_"></a><span class="xref myst">Example</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-training-versus-inference"><a id="toc1_12_3_"></a><span class="xref myst">Confusion: Training versus Inference</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-vs-inference"><a id="toc1_12_4_"></a><span class="xref myst">Training vs Inference</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#questions">Questions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-masked-0-in-some">Why Masked == 0 in some?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-reason-of-setting-the-attention-scores-s-mask-indexes-to-negative-infinity">what is the reason of setting the attention scores’s mask indexes to negative infinity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-need-both-ignore-index-in-loss-and-also-negative-infinity-mask"><a id="toc1_13_3_"></a><span class="xref myst">Why do we need both ignore index in Loss and also negative infinity mask</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#target-and-preds-logits-shape"><a id="toc1_13_4_"></a><span class="xref myst">Target and Preds/Logits Shape</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-flatten-prediction-and-target-logits"><a id="toc1_13_5_"></a><span class="xref myst">Why do we flatten prediction and target (logits)?</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#background"><a id="toc1_13_5_1_"></a><span class="xref myst">Background</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#traditional-loss-computation"><a id="toc1_13_5_2_"></a><span class="xref myst">Traditional Loss Computation</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#why-flatten"><a id="toc1_13_5_3_"></a><span class="xref myst">Why Flatten?</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-by-step-flattening"><a id="toc1_13_5_4_"></a><span class="xref myst">Step-by-step Flattening</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-sometimes-unsqueeze-masks"><a id="toc1_13_6_"></a><span class="xref myst">Why sometimes unsqueeze masks?</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-does-sequence-length-differ-for-source-and-target-usually-i-thought-it-is-just-all-l-same"><a id="toc1_13_7_"></a><span class="xref myst">Why does sequence length differ for source and target, usually I thought it is just all L, same.</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#am-i-right-to-assume-that-the-core-idea-of-autoregressive-model-like-decoder-only-gpt-like-is-that-for-a-given-sample-there-will-eventually-be-l-rows-where-l-is-the-seq-length-and-therefore-i-can-intuitively-view-it-as-1-sample-having-l-samples-since-for-each-row-we-will-compute-the-loss-am-i-right-in-my-understanding-do-not-hesistate-to-correct-me"><a id="toc1_13_8_"></a><span class="xref myst">Am i right to assume that the core idea of autoregressive model like decoder only (GPT like) is that for a given sample, there will eventually be L rows where L is the seq length, and therefore I can intuitively view it as 1 sample having L samples, since for each row, we will compute the loss. Am I right in my understanding? Do not hesistate to correct me.</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#some-implementation-details">Some Implementation Details</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input">Input</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-encodings">Positional Encodings</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-hardcode-batch-size-of-1-when-creating-p">Why do we hardcode batch size of 1 when creating P?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-register-p-as-a-buffer-in-pytorch">Why do we register P as a buffer in PyTorch?</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attention">Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-call-contiguous-on-q-k-and-v">Why do we call contiguous on Q, K and V?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-want-to-transpose-q-k-and-v">Why do we want to transpose Q, K, and V?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-want-to-reverse-transpose-q-k-and-v">Why do we want to reverse transpose Q, K, and V?</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-we-need-positional-vector">Why we need Positional Vector</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#todo">TODO</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references-and-further-readings"><a id="toc1_15_"></a><span class="xref myst">References and Further Readings</span></a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="training-a-mini-gpt-to-learn-two-digit-addition">
<h1>Training a Mini-GPT to Learn Two-Digit Addition<a class="headerlink" href="#training-a-mini-gpt-to-learn-two-digit-addition" title="Link to this heading">#</a></h1>
<p><a class="reference external" href="https://twitter.com/gaohongnan"><img alt="Twitter Handle" src="https://img.shields.io/badge/Twitter-&#64;gaohongnan-blue?style=social&amp;logo=twitter" /></a>
<a class="reference external" href="https://linkedin.com/in/gao-hongnan"><img alt="LinkedIn Profile" src="https://img.shields.io/badge/&#64;gaohongnan-blue?style=social&amp;logo=linkedin" /></a>
<a class="reference external" href="https://github.com/gao-hongnan"><img alt="GitHub Profile" src="https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&amp;logo=github" /></a>
<img alt="Tag" src="https://img.shields.io/badge/Tag-Organized_Chaos-orange" />
<a class="reference external" href="https://github.com/gao-hongnan/omniverse/tree/5221d5d8b9bd845568b2e323d908be282c6e8434/omnivault/transformer/projects/adder"><img alt="Code" src="https://img.shields.io/badge/View-Code-blue?style=flat-square&amp;logo=github" /></a></p>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#motivation" id="id4">Motivation</a></p></li>
<li><p><a class="reference internal" href="#config" id="id5">Config</a></p></li>
<li><p><a class="reference internal" href="#reproducibility" id="id6">Reproducibility</a></p></li>
<li><p><a class="reference internal" href="#vocabulary" id="id7">Vocabulary</a></p></li>
<li><p><a class="reference internal" href="#tokenization" id="id8">Tokenization</a></p></li>
<li><p><a class="reference internal" href="#create-dataset" id="id9">Create Dataset</a></p>
<ul>
<li><p><a class="reference internal" href="#encoding-strategy-overview" id="id10">Encoding Strategy Overview</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#dataset" id="id11">Dataset</a></p></li>
<li><p><a class="reference internal" href="#construct-batches-collate-function-and-dataloader" id="id12">Construct Batches, Collate Function and DataLoader</a></p>
<ul>
<li><p><a class="reference internal" href="#input-and-target" id="id13">Input and Target</a></p></li>
<li><p><a class="reference internal" href="#target-padding-mask" id="id14">Target Padding Mask</a></p></li>
<li><p><a class="reference internal" href="#future-mask" id="id15">Future Mask</a></p></li>
<li><p><a class="reference internal" href="#merge-padding-and-future-masks" id="id16">Merge Padding and Future Masks</a></p>
<ul>
<li><p><a class="reference internal" href="#first-sample-first-token" id="id17">First Sample First Token</a></p></li>
<li><p><a class="reference internal" href="#first-sample-fourth-token" id="id18">First Sample Fourth Token</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#further-add-a-singleton-dimension-in-target-masks" id="id19">Further Add a Singleton Dimension in Target Masks</a></p></li>
<li><p><a class="reference internal" href="#why-mask-our-target-in-adder" id="id20">Why mask our target in Adder?</a></p></li>
<li><p><a class="reference internal" href="#split-to-train-valid-test" id="id21">Split to Train-Valid-Test</a></p></li>
<li><p><a class="reference internal" href="#create-dataloader" id="id22">Create DataLoader</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#model" id="id23">Model</a></p></li>
<li><p><a class="reference internal" href="#training-paradigm" id="id24">Training Paradigm</a></p>
<ul>
<li><p><a class="reference internal" href="#optimizer" id="id25">Optimizer</a></p></li>
<li><p><a class="reference internal" href="#learning-rate-scheduler" id="id26">Learning Rate Scheduler</a></p></li>
<li><p><a class="reference internal" href="#loss" id="id27">Loss</a></p></li>
<li><p><a class="reference internal" href="#multiheadattention" id="id28"><a id='toc1_10_5_'></a>MultiHeadAttention</a></p>
<ul>
<li><p><a class="reference internal" href="#a-primer" id="id29"><a id='toc1_10_5_1_'></a>A Primer</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#how-loss-is-computed" id="id30"><a id='toc1_10_7_'></a>How Loss is Computed?</a></p>
<ul>
<li><p><a class="reference internal" href="#masking-and-ignore-index" id="id31">Masking and Ignore Index</a></p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference internal" href="#training-with-gpt-like-model" id="id32"><a id='toc1_12_'></a>Training with GPT-like Model</a></p>
<ul>
<li><p><a class="reference internal" href="#loss-computation" id="id33"><a id='toc1_12_1_'></a>Loss Computation</a></p></li>
<li><p><a class="reference internal" href="#example" id="id34"><a id='toc1_12_2_'></a>Example</a></p></li>
<li><p><a class="reference internal" href="#confusion-training-versus-inference" id="id35"><a id='toc1_12_3_'></a>Confusion: Training versus Inference</a></p></li>
<li><p><a class="reference internal" href="#training-vs-inference" id="id36"><a id='toc1_12_4_'></a>Training vs Inference</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#questions" id="id37">Questions</a></p>
<ul>
<li><p><a class="reference internal" href="#why-masked-0-in-some" id="id38">Why Masked == 0 in some?</a></p></li>
<li><p><a class="reference internal" href="#what-is-the-reason-of-setting-the-attention-scores-s-mask-indexes-to-negative-infinity" id="id39">what is the reason of setting the attention scores’s mask indexes to negative infinity</a></p></li>
<li><p><a class="reference internal" href="#why-do-we-need-both-ignore-index-in-loss-and-also-negative-infinity-mask" id="id40"><a id='toc1_13_3_'></a>Why do we need both ignore index in Loss and also negative infinity mask</a></p></li>
<li><p><a class="reference internal" href="#target-and-preds-logits-shape" id="id41"><a id='toc1_13_4_'></a>Target and Preds/Logits Shape</a></p></li>
<li><p><a class="reference internal" href="#why-do-we-flatten-prediction-and-target-logits" id="id42"><a id='toc1_13_5_'></a>Why do we flatten prediction and target (logits)?</a></p>
<ul>
<li><p><a class="reference internal" href="#background" id="id43"><a id='toc1_13_5_1_'></a>Background</a></p></li>
<li><p><a class="reference internal" href="#traditional-loss-computation" id="id44"><a id='toc1_13_5_2_'></a>Traditional Loss Computation</a></p></li>
<li><p><a class="reference internal" href="#why-flatten" id="id45"><a id='toc1_13_5_3_'></a>Why Flatten?</a></p></li>
<li><p><a class="reference internal" href="#step-by-step-flattening" id="id46"><a id='toc1_13_5_4_'></a>Step-by-step Flattening</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#why-sometimes-unsqueeze-masks" id="id47"><a id='toc1_13_6_'></a>Why sometimes unsqueeze masks?</a></p></li>
<li><p><a class="reference internal" href="#why-does-sequence-length-differ-for-source-and-target-usually-i-thought-it-is-just-all-l-same" id="id48"><a id='toc1_13_7_'></a>Why does sequence length differ for source and target, usually I thought it is just all L, same.</a></p></li>
<li><p><a class="reference internal" href="#am-i-right-to-assume-that-the-core-idea-of-autoregressive-model-like-decoder-only-gpt-like-is-that-for-a-given-sample-there-will-eventually-be-l-rows-where-l-is-the-seq-length-and-therefore-i-can-intuitively-view-it-as-1-sample-having-l-samples-since-for-each-row-we-will-compute-the-loss-am-i-right-in-my-understanding-do-not-hesistate-to-correct-me" id="id49"><a id='toc1_13_8_'></a>Am i right to assume that the core idea of autoregressive model like decoder only (GPT like) is that for a given sample, there will eventually be L rows where L is the seq length, and therefore I can intuitively view it as 1 sample having L samples, since for each row, we will compute the loss. Am I right in my understanding? Do not hesistate to correct me.</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#some-implementation-details" id="id50">Some Implementation Details</a></p>
<ul>
<li><p><a class="reference internal" href="#input" id="id51">Input</a></p></li>
<li><p><a class="reference internal" href="#positional-encodings" id="id52">Positional Encodings</a></p>
<ul>
<li><p><a class="reference internal" href="#why-do-we-hardcode-batch-size-of-1-when-creating-p" id="id53">Why do we hardcode batch size of 1 when creating P?</a></p></li>
<li><p><a class="reference internal" href="#why-do-we-register-p-as-a-buffer-in-pytorch" id="id54">Why do we register P as a buffer in PyTorch?</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#attention" id="id55">Attention</a></p>
<ul>
<li><p><a class="reference internal" href="#why-do-we-call-contiguous-on-q-k-and-v" id="id56">Why do we call contiguous on Q, K and V?</a></p></li>
<li><p><a class="reference internal" href="#why-do-we-want-to-transpose-q-k-and-v" id="id57">Why do we want to transpose Q, K, and V?</a></p></li>
<li><p><a class="reference internal" href="#why-do-we-want-to-reverse-transpose-q-k-and-v" id="id58">Why do we want to reverse transpose Q, K, and V?</a></p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference internal" href="#why-we-need-positional-vector" id="id59">Why we need Positional Vector</a></p></li>
<li><p><a class="reference internal" href="#todo" id="id60">TODO</a></p></li>
<li><p><a class="reference internal" href="#references-and-further-readings" id="id61"><a id='toc1_15_'></a>References and Further Readings</a></p></li>
</ul>
</nav>
<section id="motivation">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">Motivation</a><a class="headerlink" href="#motivation" title="Link to this heading">#</a></h2>
<p>Generative Pre-trained Transformer (GPT) are well known to perform bad on
arithmetic tasks such as addition. This should not come as a surprise since GPT
is a <em>language</em> model and not a <em>math</em> model. It is designed to train on a large
corpus of text and learn the patterns and structure of natural language. While
we do encounter many arithmetic operations in corpus, the encoding of these
operations are often in a form that is in the text sense, not in the
mathematical sense. After all, what GPT does best is to predict the next token
over the entire <strong>vocabulary</strong> distribution.</p>
<p>In one of the examples provided from the repository
<a class="reference external" href="https://github.com/karpathy/minGPT/tree/master">minGPT</a>, Karpathy demonstrates
training a GPT model to learn the addition of two numbers presented as strings.
This is a simple task designed to illustrate how a decoder-only model can be
trained to learn “addition”. Thus, the input is a sequence of characters
representing an addition operation (like “12 + 35”) and the output is the
sequence of characters representing the result of the addition (like “47”).</p>
<p>To this end, we replicate his example, which serves as a proof-of-concept to
show that decoder only models, which are often used for language-related tasks,
can learn other patterns or “languages,” such as the “language” of arithmetic.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">rich</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">rich.pretty</span> <span class="kn">import</span> <span class="n">pprint</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Optimizer</span>
<span class="kn">from</span> <span class="nn">torch.optim.lr_scheduler</span> <span class="kn">import</span> <span class="n">LRScheduler</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">Subset</span><span class="p">,</span> <span class="n">random_split</span>
<span class="kn">from</span> <span class="nn">omegaconf</span> <span class="kn">import</span> <span class="n">OmegaConf</span> <span class="k">as</span> <span class="n">om</span>
<span class="kn">import</span> <span class="nn">math</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>


<span class="k">def</span> <span class="nf">find_root_dir</span><span class="p">(</span><span class="n">current_path</span><span class="p">:</span> <span class="n">Path</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">marker</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;.git&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Path</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Find the root directory by searching for a directory or file that serves as a</span>
<span class="sd">    marker.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    current_path : Path | None</span>
<span class="sd">        The starting path to search from. If None, the current working directory</span>
<span class="sd">        `Path.cwd()` is used.</span>
<span class="sd">    marker : str</span>
<span class="sd">        The name of the file or directory that signifies the root.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    Path | None</span>
<span class="sd">        The path to the root directory. Returns None if the marker is not found.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">current_path</span><span class="p">:</span>
        <span class="n">current_path</span> <span class="o">=</span> <span class="n">Path</span><span class="o">.</span><span class="n">cwd</span><span class="p">()</span>
    <span class="n">current_path</span> <span class="o">=</span> <span class="n">current_path</span><span class="o">.</span><span class="n">resolve</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">parent</span> <span class="ow">in</span> <span class="p">[</span><span class="n">current_path</span><span class="p">,</span> <span class="o">*</span><span class="n">current_path</span><span class="o">.</span><span class="n">parents</span><span class="p">]:</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">parent</span> <span class="o">/</span> <span class="n">marker</span><span class="p">)</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
            <span class="k">return</span> <span class="n">parent</span>
    <span class="k">return</span> <span class="kc">None</span>

<span class="n">current_file_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">())</span>
<span class="n">root_dir</span>          <span class="o">=</span> <span class="n">find_root_dir</span><span class="p">(</span><span class="n">current_file_path</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;omnivault&#39;</span><span class="p">)</span>

<span class="k">if</span> <span class="n">root_dir</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">root_dir</span><span class="p">))</span>
    <span class="kn">from</span> <span class="nn">omnivault._types._alias</span> <span class="kn">import</span> <span class="n">Accuracy</span><span class="p">,</span> <span class="n">Loss</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.config.composer</span> <span class="kn">import</span> <span class="n">Composer</span><span class="p">,</span> <span class="n">DataConfig</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.config.constants</span> <span class="kn">import</span> <span class="n">MaybeConstant</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.config.decoder</span> <span class="kn">import</span> <span class="p">(</span>
        <span class="n">AddNormConfig</span><span class="p">,</span>
        <span class="n">DecoderBlockConfig</span><span class="p">,</span>
        <span class="n">DecoderConfig</span><span class="p">,</span>
        <span class="n">MultiHeadedAttentionConfig</span><span class="p">,</span>
        <span class="n">PositionwiseFeedForwardConfig</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.config.global_</span> <span class="kn">import</span> <span class="n">MaybeGlobal</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.config.trainer</span> <span class="kn">import</span> <span class="n">TrainerConfig</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.config.optim</span> <span class="kn">import</span> <span class="n">AdamConfig</span><span class="p">,</span> <span class="n">OptimizerConfig</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.core.dataset</span> <span class="kn">import</span> <span class="n">AdderDataset</span><span class="p">,</span> <span class="n">create_loader</span><span class="p">,</span> <span class="n">split_dataset</span><span class="p">,</span> <span class="n">construct_dummy_batch_future_masks</span><span class="p">,</span> <span class="n">construct_dummy_batch_target_padding_masks</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.core.trainer</span> <span class="kn">import</span> <span class="n">Trainer</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.core.vocabulary</span> <span class="kn">import</span> <span class="n">AdderVocabulary</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.decoder.core</span> <span class="kn">import</span> <span class="n">GPTDecoder</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.modules.attention.core</span> <span class="kn">import</span> <span class="n">ScaledDotProductAttention</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.utils.reproducibility</span> <span class="kn">import</span> <span class="n">seed_all</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.core.tokenizer</span> <span class="kn">import</span> <span class="n">AdderTokenizer</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.config.optim</span> <span class="kn">import</span> <span class="n">OPTIMIZER_REGISTRY</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.config.scheduler</span> <span class="kn">import</span> <span class="n">SCHEDULER_REGISTRY</span><span class="p">,</span> <span class="n">LambdaLRConfig</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.utils.general_utils</span> <span class="kn">import</span> <span class="n">create_directory</span><span class="p">,</span> <span class="n">download_file</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.core.optim</span> <span class="kn">import</span> <span class="n">apply_weight_decay_to_different_param_groups</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.utils.config_utils</span> <span class="kn">import</span> <span class="n">load_yaml_config</span><span class="p">,</span> <span class="n">merge_configs</span>
    <span class="kn">from</span> <span class="nn">omnivault.core.logger</span> <span class="kn">import</span> <span class="n">RichLogger</span>
    <span class="kn">from</span> <span class="nn">omnivault.utils.inspector.core</span> <span class="kn">import</span> <span class="n">get_field_annotations</span>
    <span class="kn">import</span> <span class="nn">inspect</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;Root directory not found.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="config">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">Config</a><a class="headerlink" href="#config" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yaml_cfg</span> <span class="o">=</span> <span class="n">load_yaml_config</span><span class="p">(</span><span class="n">yaml_path</span><span class="o">=</span><span class="n">root_dir</span> <span class="o">/</span> <span class="s2">&quot;omnivault/transformer/projects/adder/config.yaml&quot;</span><span class="p">)</span>
<span class="n">cfg</span> <span class="o">=</span> <span class="n">merge_configs</span><span class="p">(</span><span class="n">yaml_cfg</span><span class="p">,</span> <span class="n">args_list</span><span class="o">=</span><span class="p">[])</span>
<span class="n">om</span><span class="o">.</span><span class="n">resolve</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>  <span class="c1"># inplace ops</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">constants</span><span class="p">:</span> <span class="n">MaybeConstant</span> <span class="o">=</span> <span class="n">MaybeConstant</span><span class="p">(</span><span class="n">NUM_DIGITS</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">TOKENS</span><span class="o">=</span><span class="p">[</span>
            <span class="s2">&quot;0&quot;</span><span class="p">,</span>
            <span class="s2">&quot;1&quot;</span><span class="p">,</span>
            <span class="s2">&quot;2&quot;</span><span class="p">,</span>
            <span class="s2">&quot;3&quot;</span><span class="p">,</span>
            <span class="s2">&quot;4&quot;</span><span class="p">,</span>
            <span class="s2">&quot;5&quot;</span><span class="p">,</span>
            <span class="s2">&quot;6&quot;</span><span class="p">,</span>
            <span class="s2">&quot;7&quot;</span><span class="p">,</span>
            <span class="s2">&quot;8&quot;</span><span class="p">,</span>
            <span class="s2">&quot;9&quot;</span><span class="p">,</span>
            <span class="s2">&quot;+&quot;</span><span class="p">,</span>
            <span class="s2">&quot;*&quot;</span><span class="p">,</span>
            <span class="s2">&quot;-&quot;</span><span class="p">,</span>
            <span class="s2">&quot;=&quot;</span><span class="p">,</span>
            <span class="s2">&quot;&lt;BOS&gt;&quot;</span><span class="p">,</span>
            <span class="s2">&quot;&lt;EOS&gt;&quot;</span><span class="p">,</span>
            <span class="s2">&quot;&lt;PAD&gt;&quot;</span><span class="p">,</span>
            <span class="s2">&quot;&lt;UNK&gt;&quot;</span><span class="p">,</span>
        <span class="p">]</span>
<span class="p">)</span>
<span class="n">global_config</span><span class="p">:</span> <span class="n">MaybeGlobal</span> <span class="o">=</span> <span class="n">MaybeGlobal</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">debug_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">data_config</span><span class="p">:</span> <span class="n">DataConfig</span> <span class="o">=</span> <span class="n">DataConfig</span><span class="p">(</span><span class="o">**</span><span class="n">cfg</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">optimizer_config</span> <span class="o">=</span> <span class="n">AdamConfig</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;torch.optim.Adam&quot;</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.98</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-9</span><span class="p">)</span>
<span class="n">trainer_config</span> <span class="o">=</span> <span class="n">TrainerConfig</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

<span class="n">composer</span> <span class="o">=</span> <span class="n">Composer</span><span class="p">(</span><span class="n">constants</span><span class="o">=</span><span class="n">constants</span><span class="p">,</span> <span class="n">global_</span><span class="o">=</span><span class="n">global_config</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data_config</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer_config</span><span class="p">,</span> <span class="n">trainer</span><span class="o">=</span><span class="n">trainer_config</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">composer</span><span class="p">)</span>

<span class="n">LOGGER</span> <span class="o">=</span> <span class="n">RichLogger</span><span class="p">(</span><span class="o">**</span><span class="n">composer</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">model_dump</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s2">&quot;python&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">logger</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Composer</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">constants</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">MaybeConstant</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">NUM_DIGITS</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">TOKENS</span>=<span style="font-weight: bold">[</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'0'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'1'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'2'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'3'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'4'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'5'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'6'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'7'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'8'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'9'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'+'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'*'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'-'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'='</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'&lt;BOS&gt;'</span><span style="color: #000000; text-decoration-color: #000000">,</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'&lt;EOS&gt;'</span><span style="color: #000000; text-decoration-color: #000000">,</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'&lt;PAD&gt;'</span><span style="color: #000000; text-decoration-color: #000000">,</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'&lt;UNK&gt;'</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">]</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="font-weight: bold">)</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">logger</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">LoggerConfig</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">log_file</span>=<span style="color: #800080; text-decoration-color: #800080; font-style: italic">None</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">module_name</span>=<span style="color: #800080; text-decoration-color: #800080; font-style: italic">None</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">propagate</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">log_root_dir</span>=<span style="color: #800080; text-decoration-color: #800080; font-style: italic">None</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">rich_handler_config</span>=<span style="font-weight: bold">{</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'level'</span>: <span style="color: #008000; text-decoration-color: #008000">'INFO'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'console'</span>: MISSING,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'show_level'</span>: <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'show_path'</span>: <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'show_time'</span>: <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'rich_tracebacks'</span>: <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'markup'</span>: <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'log_time_format'</span>: <span style="color: #008000; text-decoration-color: #008000">'[%Y-%m-%d %H:%M:%S]'</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">}</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="font-weight: bold">)</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">global_</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">MaybeGlobal</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">seed</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">42</span>, <span style="color: #808000; text-decoration-color: #808000">debug</span>=<span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #808000; text-decoration-color: #808000">debug_samples</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">100</span><span style="font-weight: bold">)</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">data</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">DataConfig</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">context_length</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">dataset_name</span>=<span style="color: #008000; text-decoration-color: #008000">'adder_dataset'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">dataset_size</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10000</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">dataset_path</span>=<span style="color: #008000; text-decoration-color: #008000">'./data/adder/adder_dataset.txt'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">dataset_dir</span>=<span style="color: #008000; text-decoration-color: #008000">'./data/adder'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">dataset_url</span>=<span style="color: #008000; text-decoration-color: #008000">'https://raw.githubusercontent.com/gao-hongnan/omniverse/dev/omnivault/transformer/projects/adder/assets/adder_dataset.txt'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">split</span>=<span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.7</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">collate_fn</span>=<span style="font-weight: bold">{</span><span style="color: #008000; text-decoration-color: #008000">'batch_first'</span>: <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #008000; text-decoration-color: #008000">'pad_token_id'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span><span style="font-weight: bold">}</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">train_loader</span>=<span style="font-weight: bold">{</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'batch_size'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'shuffle'</span>: <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'num_workers'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'pin_memory'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'drop_last'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">}</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">valid_loader</span>=<span style="font-weight: bold">{</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'batch_size'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'shuffle'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'num_workers'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'pin_memory'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'drop_last'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">}</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">test_loader</span>=<span style="font-weight: bold">{</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'batch_size'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'shuffle'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'num_workers'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'pin_memory'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'drop_last'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">}</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="font-weight: bold">)</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">model</span>=<span style="color: #800080; text-decoration-color: #800080">MISSING</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">optimizer</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">AdamConfig</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">name</span>=<span style="color: #008000; text-decoration-color: #008000">'torch.optim.Adam'</span>, <span style="color: #808000; text-decoration-color: #808000">lr</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2</span>, <span style="color: #808000; text-decoration-color: #808000">betas</span>=<span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.98</span><span style="font-weight: bold">)</span>, <span style="color: #808000; text-decoration-color: #808000">eps</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1e-09</span>, <span style="color: #808000; text-decoration-color: #808000">weight_decay</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0</span><span style="font-weight: bold">)</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">criterion</span>=<span style="color: #800080; text-decoration-color: #800080">MISSING</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">scheduler</span>=<span style="color: #800080; text-decoration-color: #800080">MISSING</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">trainer</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">TrainerConfig</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">device</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">device</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">type</span>=<span style="color: #008000; text-decoration-color: #008000">'cpu'</span><span style="font-weight: bold">)</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">max_epochs</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">log_every_n_steps</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">eval_every_n_steps</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">step_scheduler_on_batch_or_epoch</span>=<span style="color: #008000; text-decoration-color: #008000">'epoch'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">use_amp</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">autocast_config</span>=<span style="font-weight: bold">{</span><span style="color: #008000; text-decoration-color: #008000">'enabled'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #008000; text-decoration-color: #008000">'dtype'</span>: torch.float16, <span style="color: #008000; text-decoration-color: #008000">'cache_enabled'</span>: <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">}</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">scaler_config</span>=<span style="font-weight: bold">{</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'enabled'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'init_scale'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">65536.0</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'growth_factor'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.0</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'backoff_factor'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.5</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'growth_interval'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2000</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">}</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">gradient_accumulation_steps</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">clip_grad_norm</span>=<span style="font-weight: bold">{</span><span style="color: #008000; text-decoration-color: #008000">'max_norm'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.0</span>, <span style="color: #008000; text-decoration-color: #008000">'norm_type'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.0</span>, <span style="color: #008000; text-decoration-color: #008000">'error_if_nonfinite'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #008000; text-decoration-color: #008000">'foreach'</span>: <span style="color: #800080; text-decoration-color: #800080; font-style: italic">None</span><span style="font-weight: bold">}</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">apply_weight_decay_to_different_param_groups</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">save_dir</span>=<span style="color: #008000; text-decoration-color: #008000">'checkpoints'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">save_every_epoch</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">save_best_only</span>=<span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">monitor</span>=<span style="color: #008000; text-decoration-color: #008000">'valid_this_epoch_average_loss'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">mode</span>=<span style="color: #008000; text-decoration-color: #008000">'min'</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="font-weight: bold">)</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">generator</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">GeneratorConfig</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">max_tokens</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1000</span>, <span style="color: #808000; text-decoration-color: #808000">temperature</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.0</span>, <span style="color: #808000; text-decoration-color: #808000">greedy</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #808000; text-decoration-color: #808000">top_k</span>=<span style="color: #800080; text-decoration-color: #800080; font-style: italic">None</span>, <span style="color: #808000; text-decoration-color: #808000">top_p</span>=<span style="color: #800080; text-decoration-color: #800080; font-style: italic">None</span><span style="font-weight: bold">)</span>
<span style="font-weight: bold">)</span>
</pre>
</div></div>
</div>
</section>
<section id="reproducibility">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">Reproducibility</a><a class="headerlink" href="#reproducibility" title="Link to this heading">#</a></h2>
<p>Reproducibility in deep learning ensures that experiments can be repeated with
identical results, critical for verifying research findings and deploying
reliable models. Distributed training introduces complexity because it involves
multiple computation units which may not synchronize their random states
perfectly. If training is paused and resumed, ensuring each unit starts with the
correct seed to reproduce the exact computational path becomes challenging. To
address this, one can find more sophisticated examples in libraries like
Composer, where the whole library’s core is built around training deep neural
nets in any environment (distributed or not) with reproducibility in mind.</p>
<div class="seealso admonition">
<p class="admonition-title">References</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/mosaicml/composer/blob/dev/composer/utils/reproducibility.py">Composer</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/notes/randomness.html">PyTorch Reproducibility</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/notes/randomness.html#dataloader">PyTorch Worker</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html">PyTorch deterministic algorithms</a></p></li>
<li><p><a class="reference external" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility">CUBLAS reproducibility</a></p></li>
</ul>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">get_field_annotations</span><span class="p">(</span><span class="n">func_or_method</span> <span class="o">=</span> <span class="n">seed_all</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">inspect</span><span class="o">.</span><span class="n">getdoc</span><span class="p">(</span><span class="n">seed_all</span><span class="p">))</span>

<span class="n">seed_all</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">global_</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span> <span class="n">seed_torch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">set_torch_deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;seed&#39;, &lt;class &#39;int&#39;&gt;, 1992), (&#39;seed_torch&#39;, &lt;class &#39;bool&#39;&gt;, True), (&#39;set_torch_deterministic&#39;, &lt;class &#39;bool&#39;&gt;, True)]


Seeds all relevant random number generators to ensure reproducible
outcomes. Optionally seeds PyTorch and activates deterministic
behavior in PyTorch based on the flags provided.

Parameters
----------
seed : int, default 1992
    The seed number for reproducibility.
seed_torch : bool, default True
    If True, seeds PyTorch&#39;s RNGs.
set_torch_deterministic : bool, default True
    If True, activates deterministic mode in PyTorch.

Returns
-------
seed : int
    The seed used for reproducibility.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>42
</pre></div>
</div>
</div>
</div>
</section>
<section id="vocabulary">
<h2><a class="toc-backref" href="#id7" role="doc-backlink">Vocabulary</a><a class="headerlink" href="#vocabulary" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vocabulary</span> <span class="o">=</span> <span class="n">AdderVocabulary</span><span class="o">.</span><span class="n">from_tokens</span><span class="p">(</span><span class="n">tokens</span><span class="o">=</span><span class="n">constants</span><span class="o">.</span><span class="n">TOKENS</span><span class="p">,</span> <span class="n">num_digits</span><span class="o">=</span><span class="n">constants</span><span class="o">.</span><span class="n">NUM_DIGITS</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>
<span class="n">token_to_index</span> <span class="o">=</span> <span class="n">vocabulary</span><span class="o">.</span><span class="n">token_to_index</span>
<span class="n">index_to_token</span> <span class="o">=</span> <span class="n">vocabulary</span><span class="o">.</span><span class="n">index_to_token</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocabulary</span><span class="o">.</span><span class="n">vocab_size</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">token_to_index</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">index_to_token</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">{</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'0'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'1'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'2'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'3'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'4'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'5'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'6'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'7'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">7</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'8'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'9'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">9</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'+'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'*'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'-'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">12</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'='</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">13</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'&lt;BOS&gt;'</span><span style="color: #000000; text-decoration-color: #000000">: </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">14</span><span style="color: #000000; text-decoration-color: #000000">,</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'&lt;EOS&gt;'</span><span style="color: #000000; text-decoration-color: #000000">: </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">15</span><span style="color: #000000; text-decoration-color: #000000">,</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'&lt;PAD&gt;'</span><span style="color: #000000; text-decoration-color: #000000">: </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span><span style="color: #000000; text-decoration-color: #000000">,</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'&lt;UNK&gt;'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">17</span>
<span style="font-weight: bold">}</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">{</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>: <span style="color: #008000; text-decoration-color: #008000">'0'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>: <span style="color: #008000; text-decoration-color: #008000">'1'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>: <span style="color: #008000; text-decoration-color: #008000">'2'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>: <span style="color: #008000; text-decoration-color: #008000">'3'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>: <span style="color: #008000; text-decoration-color: #008000">'4'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>: <span style="color: #008000; text-decoration-color: #008000">'5'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6</span>: <span style="color: #008000; text-decoration-color: #008000">'6'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">7</span>: <span style="color: #008000; text-decoration-color: #008000">'7'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>: <span style="color: #008000; text-decoration-color: #008000">'8'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">9</span>: <span style="color: #008000; text-decoration-color: #008000">'9'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10</span>: <span style="color: #008000; text-decoration-color: #008000">'+'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11</span>: <span style="color: #008000; text-decoration-color: #008000">'*'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">12</span>: <span style="color: #008000; text-decoration-color: #008000">'-'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">13</span>: <span style="color: #008000; text-decoration-color: #008000">'='</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">14</span>: <span style="color: #008000; text-decoration-color: #008000">'&lt;BOS&gt;'</span><span style="color: #000000; text-decoration-color: #000000">,</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">15</span><span style="color: #000000; text-decoration-color: #000000">: </span><span style="color: #008000; text-decoration-color: #008000">'&lt;EOS&gt;'</span><span style="color: #000000; text-decoration-color: #000000">,</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span><span style="color: #000000; text-decoration-color: #000000">: </span><span style="color: #008000; text-decoration-color: #008000">'&lt;PAD&gt;'</span><span style="color: #000000; text-decoration-color: #000000">,</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">17</span><span style="color: #000000; text-decoration-color: #000000">: </span><span style="color: #008000; text-decoration-color: #008000">'&lt;UNK&gt;'</span>
<span style="font-weight: bold">}</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">18</span>
</pre>
</div></div>
</div>
<p>Assign <code class="docutils literal notranslate"><span class="pre">vocab_size</span></code> to <code class="docutils literal notranslate"><span class="pre">composer.model</span></code> because we don’t want to hardcode
<code class="docutils literal notranslate"><span class="pre">vocab_size</span></code> beforehand, and want to derive concrete values from the
<code class="docutils literal notranslate"><span class="pre">Vocabulary</span></code> object.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="n">composer</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
<span class="k">except</span> <span class="ne">AttributeError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
    <span class="n">LOGGER</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #7fbfbf; text-decoration-color: #7fbfbf">[2024-03-25 08:57:24] </span><span style="color: #800000; text-decoration-color: #800000">ERROR   </span> _Missing instances are immutable                                     <a href="file:///tmp/ipykernel_2514/2890644827.py" target="_blank"><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">2890644827.py</span></a><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">:</span><a href="file:///tmp/ipykernel_2514/2890644827.py#4" target="_blank"><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">4</span></a>
</pre>
</div></div>
</div>
<p>Ah okay haha, this is the price of writing overly complex and useless code to
look fancy and you end up a mess. Anyways, we will handle this later on where
we can explicitly instantiate the model config class.</p>
</section>
<section id="tokenization">
<h2><a class="toc-backref" href="#id8" role="doc-backlink">Tokenization</a><a class="headerlink" href="#tokenization" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AdderTokenizer</span><span class="p">(</span><span class="n">vocabulary</span><span class="o">=</span><span class="n">vocabulary</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">vocabulary</span><span class="o">.</span><span class="n">token_to_index</span> <span class="o">==</span> <span class="n">token_to_index</span>
<span class="k">assert</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">vocabulary</span><span class="o">.</span><span class="n">index_to_token</span> <span class="o">==</span> <span class="n">index_to_token</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pprint</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;1&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">14</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">15</span><span style="font-weight: bold">]</span>
</pre>
</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sequence</span> <span class="o">=</span> <span class="s2">&quot;15+57=072&quot;</span>
<span class="n">sequences</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;15+57=072&quot;</span><span class="p">,</span> <span class="s2">&quot;01+02=003&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">encoded_sentence</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Encoded sentence: </span><span class="si">{</span><span class="n">encoded_sentence</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">decoded_sentence</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">encoded_sentence</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Decoded sentence: </span><span class="si">{</span><span class="n">decoded_sentence</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Encoded sentence: [14, 1, 5, 10, 5, 7, 13, 0, 7, 2, 15]
Decoded sentence: 15+57=072
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">encoded_sentences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode_batch</span><span class="p">(</span><span class="n">sequences</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Encoded sentences: </span><span class="si">{</span><span class="n">encoded_sentences</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">decoded_sentences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode_batch</span><span class="p">(</span><span class="n">encoded_sentences</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Decoded sentences: </span><span class="si">{</span><span class="n">decoded_sentences</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Encoded sentences: [[14, 1, 5, 10, 5, 7, 13, 0, 7, 2, 15], [14, 0, 1, 10, 0, 2, 13, 0, 0, 3, 15]]
Decoded sentences: [&#39;15+57=072&#39;, &#39;01+02=003&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># PAD = vocabulary.token_to_index[vocabulary.PAD]</span>
<span class="c1"># UNK = vocabulary.token_to_index[vocabulary.UNK]</span>
<span class="c1"># ADD = vocabulary.token_to_index[vocabulary.ADD]</span>
<span class="c1"># EQUAL = vocabulary.token_to_index[vocabulary.EQUAL]</span>
<span class="c1"># BOS = vocabulary.token_to_index[vocabulary.BOS]</span>
<span class="c1"># EOS = vocabulary.token_to_index[vocabulary.EOS]</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="create-dataset">
<h2><a class="toc-backref" href="#id9" role="doc-backlink">Create Dataset</a><a class="headerlink" href="#create-dataset" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">pad_number</span><span class="p">(</span><span class="n">num</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">length</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pad numbers with zeros in front so that they have uniform length.</span>

<span class="sd">    Note, if a + b = c and num digits allowed to add is 2, then for</span>
<span class="sd">    a and b we always pad to length 2, but for c we always pad to length 3.</span>

<span class="sd">    Example</span>
<span class="sd">    -------</span>
<span class="sd">    6 + 90 = 96 -&gt; 06 + 90 = 096</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    num : int</span>
<span class="sd">        Number to be padded.</span>
<span class="sd">    num_digits : int</span>
<span class="sd">        Length of the resulting padded number string.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    str</span>
<span class="sd">        Padded number string.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="n">num</span><span class="p">)</span><span class="o">.</span><span class="n">zfill</span><span class="p">(</span><span class="n">length</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">equation_to_string</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_digits</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Formats the addition equation as a string.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    a : int</span>
<span class="sd">        First addend.</span>
<span class="sd">    b : int</span>
<span class="sd">        Second addend.</span>
<span class="sd">    c : int</span>
<span class="sd">        Sum of a and b.</span>
<span class="sd">    num_digits : int</span>
<span class="sd">        Number of digits each number in the equation should have.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    str</span>
<span class="sd">        Formatted equation string.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">padded_a</span> <span class="o">=</span> <span class="n">pad_number</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">num_digits</span><span class="p">)</span>
    <span class="n">padded_b</span> <span class="o">=</span> <span class="n">pad_number</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">num_digits</span><span class="p">)</span>
    <span class="n">padded_c</span> <span class="o">=</span> <span class="n">pad_number</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">num_digits</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># note the padding here!</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">padded_a</span><span class="si">}</span><span class="s2">+</span><span class="si">{</span><span class="n">padded_b</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="n">padded_c</span><span class="si">}</span><span class="s2">&quot;</span>

<span class="k">def</span> <span class="nf">decode_equation</span><span class="p">(</span><span class="n">vocab</span><span class="p">:</span> <span class="n">AdderVocabulary</span><span class="p">,</span> <span class="n">equation</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Convert an equation in list format to string format.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    equation : List[int]</span>
<span class="sd">        The equation in list format.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    str</span>
<span class="sd">        The equation in string format.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">equation</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">equation</span> <span class="o">=</span> <span class="n">equation</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="n">UNK</span> <span class="o">=</span> <span class="n">vocab</span><span class="o">.</span><span class="n">token_to_index</span><span class="p">[</span><span class="n">vocab</span><span class="o">.</span><span class="n">UNK</span><span class="p">]</span>
    <span class="n">decoded_equation</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="n">index_to_token</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">UNK</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">equation</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">decoded_equation</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;&lt;BOS&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;&lt;EOS&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">batch_decode_equation</span><span class="p">(</span><span class="n">vocab</span><span class="p">:</span> <span class="n">AdderVocabulary</span><span class="p">,</span> <span class="n">equations</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="n">decoded_equations</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">equation</span> <span class="ow">in</span> <span class="n">equations</span><span class="p">:</span>
        <span class="n">decoded_equation</span> <span class="o">=</span> <span class="n">decode_equation</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">equation</span><span class="p">)</span>
        <span class="n">decoded_equations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">decoded_equation</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">decoded_equations</span>

<span class="k">def</span> <span class="nf">encode_equation</span><span class="p">(</span><span class="n">vocab</span><span class="p">:</span> <span class="n">AdderVocabulary</span><span class="p">,</span> <span class="n">equation</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">num_digits</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Convert an equation (up to the equal sign in it) in string format to a list.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    equation : str</span>
<span class="sd">        The equation in string format.</span>
<span class="sd">    num_digits : int</span>
<span class="sd">        Number of digits each number in the equation should have.</span>
<span class="sd">    device : torch.device</span>
<span class="sd">        The device to which the tensor should be sent.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    torch.Tensor</span>
<span class="sd">        The equation in list format as a tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">plus_idx</span> <span class="o">=</span> <span class="n">equation</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">vocab</span><span class="o">.</span><span class="n">ADD</span><span class="p">)</span>
    <span class="n">equal_idx</span> <span class="o">=</span> <span class="n">equation</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">vocab</span><span class="o">.</span><span class="n">EQUAL</span><span class="p">)</span>

    <span class="n">BOS</span> <span class="o">=</span> <span class="n">vocab</span><span class="o">.</span><span class="n">token_to_index</span><span class="p">[</span><span class="n">vocab</span><span class="o">.</span><span class="n">BOS</span><span class="p">]</span>
    <span class="n">UNK</span> <span class="o">=</span> <span class="n">vocab</span><span class="o">.</span><span class="n">token_to_index</span><span class="p">[</span><span class="n">vocab</span><span class="o">.</span><span class="n">UNK</span><span class="p">]</span>

    <span class="n">a</span> <span class="o">=</span> <span class="n">pad_number</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">equation</span><span class="p">[:</span><span class="n">plus_idx</span><span class="p">]),</span> <span class="n">num_digits</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">pad_number</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">equation</span><span class="p">[</span><span class="n">plus_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span><span class="n">equal_idx</span><span class="p">]),</span> <span class="n">num_digits</span><span class="p">)</span>

    <span class="n">new_equation</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s2">+</span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s2">=&quot;</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
        <span class="p">[</span><span class="n">BOS</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">token_to_index</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">UNK</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">new_equation</span><span class="p">],</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_add_dataset</span><span class="p">(</span>
    <span class="n">vocab</span><span class="p">:</span> <span class="n">AdderVocabulary</span><span class="p">,</span> <span class="n">num_digits</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dataset_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">rng_seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1337</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
    <span class="n">BOS</span> <span class="o">=</span> <span class="n">vocab</span><span class="o">.</span><span class="n">token_to_index</span><span class="p">[</span><span class="n">vocab</span><span class="o">.</span><span class="n">BOS</span><span class="p">]</span>
    <span class="n">EOS</span> <span class="o">=</span> <span class="n">vocab</span><span class="o">.</span><span class="n">token_to_index</span><span class="p">[</span><span class="n">vocab</span><span class="o">.</span><span class="n">EOS</span><span class="p">]</span>
    <span class="n">UNK</span> <span class="o">=</span> <span class="n">vocab</span><span class="o">.</span><span class="n">token_to_index</span><span class="p">[</span><span class="n">vocab</span><span class="o">.</span><span class="n">UNK</span><span class="p">]</span>

    <span class="n">rng</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span>
    <span class="n">rng</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">rng_seed</span><span class="p">)</span>

    <span class="n">max_num</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="n">num_digits</span> <span class="o">-</span> <span class="mi">1</span>

    <span class="n">dataset_str</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dataset_size</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">max_num</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">generator</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">max_num</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">generator</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>

        <span class="n">equation</span> <span class="o">=</span> <span class="n">equation_to_string</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">num_digits</span><span class="p">)</span>

        <span class="n">dataset_str</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">equation</span><span class="p">)</span>

    <span class="n">dataset_tensor</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">BOS</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">token_to_index</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">UNK</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">EOS</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">dataset_str</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="n">dataset_tensor</span><span class="p">,</span> <span class="n">dataset_str</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataset_tensor</span><span class="p">,</span> <span class="n">dataset_str</span> <span class="o">=</span> <span class="n">create_add_dataset</span><span class="p">(</span><span class="n">vocab</span><span class="o">=</span><span class="n">vocabulary</span><span class="p">,</span> <span class="n">num_digits</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dataset_size</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">dataset_tensor</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">dataset_str</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">[</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">14</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">7</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">13</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">7</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">15</span><span style="font-weight: bold">])</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">14</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">9</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">13</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">9</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">15</span><span style="font-weight: bold">])</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">14</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">9</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">13</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">15</span><span style="font-weight: bold">])</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">14</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">13</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">15</span><span style="font-weight: bold">])</span>
<span style="font-weight: bold">]</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">[</span><span style="color: #008000; text-decoration-color: #008000">'15+57=072'</span>, <span style="color: #008000; text-decoration-color: #008000">'92+00=092'</span>, <span style="color: #008000; text-decoration-color: #008000">'95+53=148'</span>, <span style="color: #008000; text-decoration-color: #008000">'15+10=025'</span><span style="font-weight: bold">]</span>
</pre>
</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Decoded equation: </span><span class="si">{</span><span class="n">decode_equation</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">,</span><span class="w"> </span><span class="n">dataset_tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span>
    <span class="n">decode_equation</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">,</span> <span class="n">dataset_tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="o">==</span> <span class="n">dataset_str</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="o">==</span> <span class="n">decode_equation</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">,</span> <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">14</span><span class="p">])</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Decoded equation: 15+57=072
</pre></div>
</div>
</div>
</div>
<p>if we encode equation, we can encode up to equal sign like below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Encoded equation: </span><span class="si">{</span><span class="n">encode_equation</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">,</span><span class="w"> </span><span class="n">dataset_str</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="n">num_digits</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="n">device</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_close</span><span class="p">(</span>
    <span class="n">encode_equation</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">,</span> <span class="n">dataset_str</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">num_digits</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">14</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">13</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Encoded equation: tensor([14,  1,  5, 10,  5,  7, 13], dtype=torch.int32)
</pre></div>
</div>
</div>
</div>
<p>Uncomment the below code to generate the dataset into a text file and yes, I am
lazy to add a config variable for whether to generate the dataset or not.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># dataset, dataset_str = create_add_dataset(vocab, self.num_digits, self.dataset_size)</span>

<span class="c1"># write dataset_str to a file</span>
<span class="c1"># with open(&quot;dataset_str.txt&quot;, &quot;w&quot;) as f:</span>
<span class="c1">#     for item in dataset_str:</span>
<span class="c1">#         f.write(&quot;%s\n&quot; % item)</span>
</pre></div>
</div>
</div>
</div>
<section id="encoding-strategy-overview">
<h3><a class="toc-backref" href="#id10" role="doc-backlink">Encoding Strategy Overview</a><a class="headerlink" href="#encoding-strategy-overview" title="Link to this heading">#</a></h3>
<p>Our strategy for encoding arithmetic expressions is pretty self-explanatory,
where given a string <code class="docutils literal notranslate"><span class="pre">D1</span> <span class="pre">+</span> <span class="pre">D2</span> <span class="pre">=</span> <span class="pre">D3</span></code>, we encode it as <code class="docutils literal notranslate"><span class="pre">&lt;BOS&gt;D1+D2=0D3&lt;EOS&gt;</span></code>.
However, this is verbose for clarity sake. In fact, Karpathy’s encoding strategy
simplifies arithmetic expressions by concatenating the digits of operands and
the result into a single string without explicit symbols for operations or
equality. This method relies on a fixed number of digits (<code class="docutils literal notranslate"><span class="pre">num_digits</span></code>) for
operands, which streamlines the model’s interpretation of the sequence. For
example, if <code class="docutils literal notranslate"><span class="pre">num_digits</span></code> is set to 2, every encoded expression is structured to
follow a predictable pattern: the first two digits represent the first operand,
the next two digits represent the second operand, and the final digits are
encoded as 3 digits because the max sum of two 2-digit numbers is 199, which is
3 digits. The digits of the result are encoded in reverse order. This
counterintuitive approach is designed to align with the GPT model’s learning
algorithm, facilitating easier learning of the addition operation by mimicking
the traditional right-to-left calculation process in addition.</p>
<p>To illustrate, let’s examine the encoding of arithmetic expressions with
<code class="docutils literal notranslate"><span class="pre">num_digits=2</span></code>:</p>
<p>For the expression <code class="docutils literal notranslate"><span class="pre">6</span> <span class="pre">+</span> <span class="pre">39</span> <span class="pre">=</span> <span class="pre">45</span></code>, we have the following:</p>
<ul class="simple">
<li><p>The first two digits <code class="docutils literal notranslate"><span class="pre">06</span></code> represent the number 6, zero-padded to adhere to
the <code class="docutils literal notranslate"><span class="pre">num_digits=2</span></code> requirement.</p></li>
<li><p>The next two digits <code class="docutils literal notranslate"><span class="pre">39</span></code> represent the number 39, already fitting the digit
requirement.</p></li>
<li><p>The final part <code class="docutils literal notranslate"><span class="pre">054</span></code> represents the result 45, reversed to <code class="docutils literal notranslate"><span class="pre">54</span></code> and preceded
by a zero to maintain the total length of <span class="math notranslate nohighlight">\(2n + (n + 1) = 7 \)</span> digits for
<code class="docutils literal notranslate"><span class="pre">num_digits=2</span></code>.</p></li>
</ul>
</section>
</section>
<section id="dataset">
<h2><a class="toc-backref" href="#id11" role="doc-backlink">Dataset</a><a class="headerlink" href="#dataset" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">create_directory</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dataset_dir</span><span class="p">)</span>
<span class="n">download_file</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dataset_url</span><span class="p">,</span> <span class="n">output_path</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dataset_path</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100   97k  100   97k    0     0   376k      0 --:--:-- --:--:-- --:--:--  378k
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dataset_path</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
    <span class="n">sequences</span> <span class="o">=</span> <span class="p">[</span><span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">file</span><span class="p">]</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">AdderDataset</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">sequences</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>

<span class="n">pprint</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataset</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">14</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">7</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">13</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">7</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span><span style="font-weight: bold">])</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">7</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">15</span><span style="font-weight: bold">])</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([</span><span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">])</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">]])</span>
<span style="font-weight: bold">)</span>
</pre>
</div></div>
</div>
</section>
<section id="construct-batches-collate-function-and-dataloader">
<h2><a class="toc-backref" href="#id12" role="doc-backlink">Construct Batches, Collate Function and DataLoader</a><a class="headerlink" href="#construct-batches-collate-function-and-dataloader" title="Link to this heading">#</a></h2>
<p>We first reverse engineer what our dataset is returning. The disclaimer here is
that for decoder only models like GPT, many people often omit the padding mask
since all the samples <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> are chunked to sequence/context length of
window size <span class="math notranslate nohighlight">\(T\)</span>, and future masks are usually handled within the <code class="docutils literal notranslate"><span class="pre">Attention</span></code>
class since we will never attend to the future tokens. However, for the sake of
clarity, we will include the padding and future mask in the dataset (i.e.
actually it is for the sake of my own understanding when I started to implement
decoder from scratch).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">target_padding_mask</span><span class="p">,</span> <span class="n">future_mask</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<section id="input-and-target">
<h3><a class="toc-backref" href="#id13" role="doc-backlink">Input and Target</a><a class="headerlink" href="#input-and-target" title="Link to this heading">#</a></h3>
<p>I think if you’ve read my
<a class="reference external" href="https://www.gaohongnan.com/transformer/decoder/implementation.html#construction-of-input-and-target-sequences">section here</a>,
then we would easily see that given an input sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, the target
sequence <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> is simply the input sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> shifted by one
time step to the left.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input : </span><span class="si">{</span><span class="nb">input</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Target: </span><span class="si">{</span><span class="n">target</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Input : tensor([14,  1,  5, 10,  5,  7, 13,  0,  7,  2])
Target: tensor([16, 16, 16, 16, 16, 16,  0,  7,  2, 15])
</pre></div>
</div>
</div>
</div>
</section>
<section id="target-padding-mask">
<h3><a class="toc-backref" href="#id14" role="doc-backlink">Target Padding Mask</a><a class="headerlink" href="#target-padding-mask" title="Link to this heading">#</a></h3>
<p>When you’re dealing with sequences of different lengths, you pad the shorter
sequences with a special token <code class="docutils literal notranslate"><span class="pre">PAD</span></code> (usually <span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(-100\)</span>) to make them the
same length as the longest one in the batch. These paddings should not
contribute to the model’s learning, so you need to mask them out. In practice,
you’ll often see a mask argument in <code class="docutils literal notranslate"><span class="pre">Attention</span></code> layers in PyTorch where if
<code class="docutils literal notranslate"><span class="pre">True</span></code>, the attention scores are set to <code class="docutils literal notranslate"><span class="pre">-inf</span></code> for the padded positions so that
these positions become zero after the softmax operation, thereby not
contributing to the weighted sum of the input sequence.</p>
<p>In a decoder-only model like GPT, the input sequence is essentially the target.
The model aims to generate tokens that come after the given input, treating it
as the “history” or “context” for the task of text generation. Unlike
encoder-decoder models like the original Transformer, where the encoder
processes a source sequence and the decoder generates a target sequence, a
decoder-only model works solely with what would traditionally be considered the
target sequence.</p>
<p>Consequently, although the terminology “target padding mask” might seem more
intuitive in the context of encoder-decoder models, where the distinction
between source (input) and target (output) sequences is clear. The distinction
is blurred in decoder-only models like GPT as the model processes input to
predict the next token in a sequence. Here, the source is essentially the target
at different stages of processing: the model uses previous tokens (source) to
predict the next token (target). However, during my implementation, I was mainly
referring to transformer models that use encoder-decoder architecture, and the
terminology therefore stemmed from that context.</p>
<p>The definition of a target padding mask is a binary mark that ignores pad-tokens
in the source input (in decoder only model, the source is the target). And the
shape is <span class="math notranslate nohighlight">\((\mathcal{B}, T)\)</span>.</p>
<p>Let’s illustrate the target padding mask with an example. Suppose we have a
batch of sequences with different lengths:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">target_batch</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">17</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
<span class="p">]</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">target_batch</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">[[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">7</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">9</span><span style="font-weight: bold">]</span>, <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6</span><span style="font-weight: bold">]</span>, <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">12</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">17</span><span style="font-weight: bold">]</span>, <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span><span style="font-weight: bold">]]</span>
</pre>
</div></div>
</div>
<p>If we try to “batch” these sequences, PyTorch would throw an error indicating
that you need all sequences to have the same length.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="n">target_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">target_batch</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">ValueError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
    <span class="n">LOGGER</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #7fbfbf; text-decoration-color: #7fbfbf">[2024-03-25 08:57:25] </span><span style="color: #800000; text-decoration-color: #800000">ERROR   </span> expected sequence of length <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span> at dim <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span> <span style="font-weight: bold">(</span>got <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span><span style="font-weight: bold">)</span>                       <a href="file:///tmp/ipykernel_2514/1205213247.py" target="_blank"><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">1205213247.py</span></a><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">:</span><a href="file:///tmp/ipykernel_2514/1205213247.py#4" target="_blank"><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">4</span></a>
</pre>
</div></div>
</div>
<p>To address this issue, we could pad the sequences to the same length and create a mask to indicate
which positions are padded.  We pad the shorter sequences with a special token <code class="docutils literal notranslate"><span class="pre">PAD</span></code>
to make them the same length as the longest one in the batch.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">PAD</span> <span class="o">=</span> <span class="n">vocabulary</span><span class="o">.</span><span class="n">token_to_index</span><span class="p">[</span><span class="n">vocabulary</span><span class="o">.</span><span class="n">PAD</span><span class="p">]</span>

<span class="n">max_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span> <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">target_batch</span><span class="p">)</span>
<span class="n">target_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">seq</span> <span class="o">+</span> <span class="p">[</span><span class="n">PAD</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_len</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">seq</span><span class="p">))</span> <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">target_batch</span><span class="p">]</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">target_batch</span><span class="p">)</span>

<span class="n">target_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">target_batch</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">target_batch</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">[[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">7</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">9</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span><span style="font-weight: bold">]</span>, <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span><span style="font-weight: bold">]</span>, <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">12</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">17</span><span style="font-weight: bold">]</span>, <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span><span style="font-weight: bold">]]</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">7</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">9</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">12</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">17</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span><span style="font-weight: bold">]])</span>
</pre>
</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span> <span class="o">=</span> <span class="n">target_batch</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

<span class="n">target_padding_mask</span> <span class="o">=</span> <span class="n">target_batch</span> <span class="o">!=</span> <span class="n">PAD</span>

<span class="n">pprint</span><span class="p">(</span><span class="n">target_padding_mask</span><span class="p">)</span>

<span class="k">assert</span> <span class="n">target_padding_mask</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]])</span>
</pre>
</div></div>
</div>
<p>Of course, we would need a <em>batch</em> of these masks, so we would have a shape of
<span class="math notranslate nohighlight">\((\mathcal{B}, T)\)</span> like mentioned above. As we will see later, we will still
need to broadcast the shape to <span class="math notranslate nohighlight">\((\mathcal{B}, 1, T, T)\)</span> to match the shape of
the attention scores.</p>
<p>Theoretically speaking, it is possible for the sequence length <span class="math notranslate nohighlight">\(T\)</span> to vary
across samples <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. However, we usually have the same length for all
samples in GPT, and in this particular case, we do know that each sample
necessarily have the same length by <em>design</em>. However, for the sake of
explanation, we note that in our <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>, it will only generate 1 single
sample data point and do not worry about different sequence length across other
samples in the dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>, but in deep learning we train in
mini-batches <span class="math notranslate nohighlight">\(\mathcal{B}\)</span>, and with different batch sizes we may encounter
issues (i.e. matrix multiplication may not work).</p>
</section>
<section id="future-mask">
<h3><a class="toc-backref" href="#id15" role="doc-backlink">Future Mask</a><a class="headerlink" href="#future-mask" title="Link to this heading">#</a></h3>
<p>In the decoder, each position can only attend to positions that come before it
in the sequence to maintain the auto-regressive property. This is different from
the encoder, where all positions can attend to all other positions.</p>
<p>The definition of future mask is basically a look-ahead mask to ensure that each
position only attends to positions before it in the sequence where we mask out
future positions (i.e., positions that come after the current position) so that
they don’t contribute to the current attention scores. Before the softmax
operation, we’ll mark these positions as <code class="docutils literal notranslate"><span class="pre">-inf</span></code> so that they become zero after
the softmax operation - effectively zeroing out the attention scores for future
positions. What does zeroing out these masked logits actually does? Basically,
the attention mechanism can be thought of as a weighted average of all the
tokens in the input sequence. Each token is assigned a weight, with higher
weights indicating more relevance to the token under consideration. If a certain
token should not be considered at all (e.g., it’s a future token that should not
be visible to the current decoder step, or it’s a padding token), its weight
should be zero.</p>
<p>The shape of the future mask is <span class="math notranslate nohighlight">\((T, T)\)</span> for a target sequence/sample
<span class="math notranslate nohighlight">\(\mathbf{x}\)</span> of length <span class="math notranslate nohighlight">\(T\)</span>. Let’s see a concrete example to illustrate the
future mask.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">seq_len</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">future_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">future_mask</span> <span class="o">=</span> <span class="n">future_mask</span> <span class="o">==</span> <span class="mi">0</span>

<span class="n">pprint</span><span class="p">(</span><span class="n">future_mask</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">future_mask</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">]])</span>
</pre>
</div></div>
</div>
</section>
<section id="merge-padding-and-future-masks">
<h3><a class="toc-backref" href="#id16" role="doc-backlink">Merge Padding and Future Masks</a><a class="headerlink" href="#merge-padding-and-future-masks" title="Link to this heading">#</a></h3>
<p>We see from our <code class="docutils literal notranslate"><span class="pre">decoder</span></code> implementation below, that one of the method is
creating the target masks. In other words, we are creating the target padding
masks and future masks, and merging them together.</p>
<div class="highlight-md notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span>def create_target_masks(
<span class="linenos"> 2</span>    self,
<span class="linenos"> 3</span>    batch_size: int,
<span class="linenos"> 4</span>    seq_len: int,
<span class="linenos"> 5</span>    target_padding_masks: torch.BoolTensor | NotGiven = NOT_GIVEN,
<span class="linenos"> 6</span>    future_masks: torch.BoolTensor | NotGiven = NOT_GIVEN,
<span class="linenos"> 7</span>) -&gt; torch.BoolTensor:
<span class="linenos"> 8</span>    target_masks_shape = (batch_size, 1, seq_len, seq_len)
<span class="linenos"> 9</span>    if target_padding_masks is NOT_GIVEN and future_masks is NOT_GIVEN:
<span class="linenos">10</span>        target_padding_masks = cast(
<span class="linenos">11</span>            torch.BoolTensor, construct_dummy_batch_target_padding_masks(batch_size, seq_len)
<span class="linenos">12</span>        )
<span class="linenos">13</span>        future_masks = cast(torch.BoolTensor, construct_dummy_batch_future_masks(batch_size, seq_len))
<span class="linenos">14</span>
<span class="linenos">15</span>    if target_padding_masks is NOT_GIVEN:
<span class="linenos">16</span>        target_padding_masks = cast(
<span class="linenos">17</span>            torch.BoolTensor, construct_dummy_batch_target_padding_masks(batch_size, seq_len)
<span class="linenos">18</span>        )
<span class="linenos">19</span>
<span class="linenos">20</span>    if future_masks is NOT_GIVEN:
<span class="linenos">21</span>        future_masks = cast(torch.BoolTensor, construct_dummy_batch_future_masks(batch_size, seq_len))
<span class="linenos">22</span>
<span class="linenos">23</span>    assert target_padding_masks.shape == future_masks.shape == target_masks_shape  # type: ignore[union-attr]
<span class="linenos">24</span>
<span class="linenos">25</span>    return cast(
<span class="linenos">26</span>        torch.BoolTensor,
<span class="hll"><span class="linenos">27</span>        torch.logical_and(cast(torch.Tensor, target_padding_masks), cast(torch.Tensor, future_masks)).bool(),
</span><span class="linenos">28</span>    )
</pre></div>
</div>
<p>The purpose of applying <code class="docutils literal notranslate"><span class="pre">logical_and</span></code> between <code class="docutils literal notranslate"><span class="pre">target_padding_mask</span></code> and
<code class="docutils literal notranslate"><span class="pre">future_mask</span></code> is to combine the constraints from both masks when calculating
self-attention scores in the transformer’s decoder. The <code class="docutils literal notranslate"><span class="pre">target_padding_mask</span></code> is
designed to mask out the padding tokens in the input sequence, while the
<code class="docutils literal notranslate"><span class="pre">future_mask</span></code> ensures that a given position cannot attend to future positions in
the sequence. By combining these masks, you can perform the necessary masking
for both padding and future tokens in a single step.</p>
<p>Here’s how it works:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">target_padding_mask</span></code>: Masks out the padding tokens so that they don’t
contribute to the attention calculations. True values mean “attend to this
token,” and False values mean “ignore this token.”</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">future_mask</span></code>: The future mask is created as a lower triangular matrix, where
the lower triangle, including the diagonal, is filled with ones, and the
upper triangle is filled with zeros. Masks out future tokens in a sequence so
that a token at a given position can only attend to positions that come
before it (and itself). True values mean “attend to this token,” and False
values mean “ignore this token.”</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">logical_and(target_padding_mask,</span> <span class="pre">future_mask)</span></code>: Combines the two masks. A
True in the resulting mask means that the condition for both padding and
future attention is satisfied.</p></li>
</ol>
<p>By combining these two masks, the decoder obeys the autoregressive property,
ensuring it doesn’t see future tokens, while also ignoring padding tokens in the
input sequence. We may term it the <code class="docutils literal notranslate"><span class="pre">target_mask</span></code>.</p>
<section id="first-sample-first-token">
<h4><a class="toc-backref" href="#id17" role="doc-backlink">First Sample First Token</a><a class="headerlink" href="#first-sample-first-token" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">target_padding_mask</span></code> has size of <code class="docutils literal notranslate"><span class="pre">[4,</span> <span class="pre">5]</span></code>.</p>
<ul>
<li><p>We zoom in to the first row (sample) which is of length 5.</p></li>
<li><p>This length 5 is the sequence length, which is <code class="docutils literal notranslate"><span class="pre">T,</span> <span class="pre">T,</span> <span class="pre">T,</span> <span class="pre">F,</span> <span class="pre">F</span></code>
indicating the last 2 tokens being padded.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">future_mask</span></code> has size of <code class="docutils literal notranslate"><span class="pre">[5,</span> <span class="pre">5]</span></code>.</p>
<ul>
<li><p>We note that this is indepedent of batch size. Each sample should have
the same future mask shape of <code class="docutils literal notranslate"><span class="pre">[L,</span> <span class="pre">L]</span></code>.</p></li>
<li><p>This <code class="docutils literal notranslate"><span class="pre">L=5</span></code> should necessary be same for the sequence length in
<code class="docutils literal notranslate"><span class="pre">target_padding_mask</span></code>.</p></li>
</ul>
</li>
<li><p>First, let’s consider one batch of 4 samples. What we do first is to
broadcast <code class="docutils literal notranslate"><span class="pre">future_mask</span></code> to <code class="docutils literal notranslate"><span class="pre">[4,</span> <span class="pre">5,</span> <span class="pre">5]</span></code> because we want each sample/row in
the batch to have the same future mask. As shown below:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pprint</span><span class="p">(</span><span class="n">future_mask</span><span class="p">)</span>
<span class="n">future_mask</span> <span class="o">=</span> <span class="n">future_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">future_mask</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">future_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">]])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">]]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">]]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">]]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">]]])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">torch.Size</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span><span style="font-weight: bold">])</span>
</pre>
</div></div>
</div>
<ul class="simple">
<li><p>Now, we can zoom in to one particular sample since both
<code class="docutils literal notranslate"><span class="pre">target_padding_mask</span></code> and <code class="docutils literal notranslate"><span class="pre">future_mask</span></code> have the same first dimension of
batch size.</p></li>
<li><p>What is incomplete is that we need to broadcast <code class="docutils literal notranslate"><span class="pre">target_padding_mask</span></code>’s last
dimension to have the same dimensions as <code class="docutils literal notranslate"><span class="pre">future_mask</span></code>. This means we
broadcast <code class="docutils literal notranslate"><span class="pre">[4,</span> <span class="pre">5]</span></code> to <code class="docutils literal notranslate"><span class="pre">[4,</span> <span class="pre">5,</span> <span class="pre">5]</span></code>. But why?</p></li>
<li><p>For simplicity, we slice the first same of both below.</p></li>
<li><p>The first row of the <code class="docutils literal notranslate"><span class="pre">future_mask</span></code> of the first sample is <code class="docutils literal notranslate"><span class="pre">T,</span> <span class="pre">F,</span> <span class="pre">F,</span> <span class="pre">F,</span> <span class="pre">F</span></code>.
This corresponds to what? This is the future mask of the first token in the
sequence. Well, that is confusing, because it apparently have 5 elements,
and has “information” of the other 4 tokens in the sequence. Let’s explain
in details below:</p>
<ul>
<li><p>Regarding the first row of the <code class="docutils literal notranslate"><span class="pre">future_mask</span></code> in the first sample, which
is <code class="docutils literal notranslate"><span class="pre">[T,</span> <span class="pre">F,</span> <span class="pre">F,</span> <span class="pre">F,</span> <span class="pre">F]</span></code>, it might initially seem confusing why there are 5
elements. Each of these elements, in fact, corresponds to whether the
first token can attend to other tokens at each respective position in
the sequence. Here’s how to interpret it:</p>
<ul>
<li><p>The first element (<code class="docutils literal notranslate"><span class="pre">True</span></code>) indicates that the first token can attend
to itself.</p></li>
<li><p>The next four elements (<code class="docutils literal notranslate"><span class="pre">False</span></code>) specify that the first token should
not attend to any of the future tokens in the sequence.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Consequently, what is the first token in the sequence of the
<code class="docutils literal notranslate"><span class="pre">target_padding_mask</span></code>? Recall earlier we mentioned that the first sample’s
<code class="docutils literal notranslate"><span class="pre">target_padding_mask</span></code> is <code class="docutils literal notranslate"><span class="pre">T,</span> <span class="pre">T,</span> <span class="pre">T,</span> <span class="pre">F,</span> <span class="pre">F</span></code> and therefore the first token in
the sequence is <code class="docutils literal notranslate"><span class="pre">T</span></code>.</p></li>
<li><p>What do we want to achieve here? We want to make sure that the model does
not <strong>attend</strong> to tokens in the sequence that are masked with <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>In other words, the first token in the sequence of the first sample has
<code class="docutils literal notranslate"><span class="pre">target_padding_mask</span></code> of <code class="docutils literal notranslate"><span class="pre">T</span></code> and <code class="docutils literal notranslate"><span class="pre">future_masks</span></code> of <code class="docutils literal notranslate"><span class="pre">T,</span> <span class="pre">F,</span> <span class="pre">F,</span> <span class="pre">F,</span> <span class="pre">F</span></code>.</p></li>
<li><p>We need to broadcast this <code class="docutils literal notranslate"><span class="pre">T</span></code> to <code class="docutils literal notranslate"><span class="pre">T,</span> <span class="pre">T,</span> <span class="pre">T,</span> <span class="pre">T,</span> <span class="pre">T</span></code> to align with
<code class="docutils literal notranslate"><span class="pre">T,</span> <span class="pre">F,</span> <span class="pre">F,</span> <span class="pre">F,</span> <span class="pre">F</span></code> because? Because we need ensure that this first token in the
sequence is also able to considered in relation to every other token in the
sequence.</p></li>
<li><p>So the first token is not a padded token, which is <code class="docutils literal notranslate"><span class="pre">T</span></code>, similarly, the first
token needs to attend to itself at the first position, hence <code class="docutils literal notranslate"><span class="pre">T</span></code> and <code class="docutils literal notranslate"><span class="pre">T</span></code>
give <code class="docutils literal notranslate"><span class="pre">T</span></code>. But for the second <code class="docutils literal notranslate"><span class="pre">T</span></code> in the now broadcasted
<code class="docutils literal notranslate"><span class="pre">target_padding_mask</span></code>, it is still representing the first token or?</p></li>
<li><p>Broadcasting the first token’s <code class="docutils literal notranslate"><span class="pre">target_padding_mask</span></code> value of <code class="docutils literal notranslate"><span class="pre">T</span></code> to
<code class="docutils literal notranslate"><span class="pre">[T,</span> <span class="pre">T,</span> <span class="pre">T,</span> <span class="pre">T,</span> <span class="pre">T]</span></code> ensures that when this first token is being considered for
attention computations, it is free to attend to any position, barring any
restrictions set by <code class="docutils literal notranslate"><span class="pre">future_mask</span></code>.</p></li>
<li><p>Tricky: after broadcasting, each <code class="docutils literal notranslate"><span class="pre">T</span></code> in <code class="docutils literal notranslate"><span class="pre">[T,</span> <span class="pre">T,</span> <span class="pre">T,</span> <span class="pre">T,</span> <span class="pre">T]</span></code> is still
representing the first token. They indicate that when the first token is
compared with <em>any</em> token in the sequence (including itself), it is not a
padding token. The element-wise <code class="docutils literal notranslate"><span class="pre">AND</span></code> with the <code class="docutils literal notranslate"><span class="pre">future_mask</span></code> then further
refines this by restricting it from attending to future tokens.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pprint</span><span class="p">(</span><span class="n">target_padding_mask</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">target_padding_mask</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">target_padding_mask</span> <span class="o">=</span> <span class="n">target_padding_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">))</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">target_padding_mask</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">target_padding_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">]]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]]])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">torch.Size</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span><span style="font-weight: bold">])</span>
</pre>
</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pprint</span><span class="p">(</span><span class="n">target_padding_mask</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">future_mask</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">target_padding_mask</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&amp;</span> <span class="n">future_mask</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">]])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]])</span>
</pre>
</div></div>
</div>
</section>
<section id="first-sample-fourth-token">
<h4><a class="toc-backref" href="#id18" role="doc-backlink">First Sample Fourth Token</a><a class="headerlink" href="#first-sample-fourth-token" title="Link to this heading">#</a></h4>
<p>Now let’s look at another example—the 4th token in the sequence, where
<code class="docutils literal notranslate"><span class="pre">target_padding_mask</span> <span class="pre">=</span> <span class="pre">[T,</span> <span class="pre">T,</span> <span class="pre">T,</span> <span class="pre">F,</span> <span class="pre">F]</span></code> and <code class="docutils literal notranslate"><span class="pre">future_mask</span></code> is a lower triangular
matrix with <code class="docutils literal notranslate"><span class="pre">True</span></code>s.</p>
<ol class="arabic simple">
<li><p><strong>4th Token’s target_padding_mask</strong>: The 4th token has a value of <code class="docutils literal notranslate"><span class="pre">F</span></code> in
<code class="docutils literal notranslate"><span class="pre">target_padding_mask</span></code>, indicating it’s a padding token.</p></li>
<li><p><strong>4th Row of future_mask</strong>: The 4th row in <code class="docutils literal notranslate"><span class="pre">future_mask</span></code> is
<code class="docutils literal notranslate"><span class="pre">[True,</span> <span class="pre">True,</span> <span class="pre">True,</span> <span class="pre">True,</span> <span class="pre">False]</span></code>. This means that if this token were not a
padding token, it would be allowed to attend to all the previous tokens in
the sequence and itself, but not to any future token.</p></li>
<li><p><strong>Broadcast target_padding_mask</strong>: To align <code class="docutils literal notranslate"><span class="pre">target_padding_mask</span></code> with
<code class="docutils literal notranslate"><span class="pre">future_mask</span></code>, we’d broadcast <code class="docutils literal notranslate"><span class="pre">F</span></code> from the <code class="docutils literal notranslate"><span class="pre">target_padding_mask</span></code> to
<code class="docutils literal notranslate"><span class="pre">[F,</span> <span class="pre">F,</span> <span class="pre">F,</span> <span class="pre">F,</span> <span class="pre">F]</span></code>. This way, when we consider the 4th token in relation to
any other token in the sequence, it’s still marked as a padding token.</p></li>
<li><p><strong>Element-wise AND with future_mask</strong>: After broadcasting, you’d perform an
element-wise AND between <code class="docutils literal notranslate"><span class="pre">[F,</span> <span class="pre">F,</span> <span class="pre">F,</span> <span class="pre">F,</span> <span class="pre">F]</span></code> and
<code class="docutils literal notranslate"><span class="pre">[True,</span> <span class="pre">True,</span> <span class="pre">True,</span> <span class="pre">True,</span> <span class="pre">False]</span></code>, resulting in <code class="docutils literal notranslate"><span class="pre">[F,</span> <span class="pre">F,</span> <span class="pre">F,</span> <span class="pre">F,</span> <span class="pre">F]</span></code>.</p></li>
<li><p><strong>Interpretation</strong>: This effectively means that the 4th token won’t attend to
any other token in the sequence, and no token will attend to it either, as it
is a padding token.</p></li>
</ol>
<p>So, the masks are doing their jobs correctly: the <code class="docutils literal notranslate"><span class="pre">target_padding_mask</span></code>
indicates whether each token is a padding token or not, and <code class="docutils literal notranslate"><span class="pre">future_mask</span></code>
dictates the “rules” of attention regarding what each token can attend to.
Combining them ensures that both conditions are met.</p>
</section>
</section>
<section id="further-add-a-singleton-dimension-in-target-masks">
<h3><a class="toc-backref" href="#id19" role="doc-backlink">Further Add a Singleton Dimension in Target Masks</a><a class="headerlink" href="#further-add-a-singleton-dimension-in-target-masks" title="Link to this heading">#</a></h3>
<p>Now both masks are of shape: <code class="docutils literal notranslate"><span class="pre">(B,</span> <span class="pre">L,</span> <span class="pre">L)</span></code> but we need to add a singleton
dimension to the last dimension to make it <code class="docutils literal notranslate"><span class="pre">(B,</span> <span class="pre">1,</span> <span class="pre">L,</span> <span class="pre">L)</span></code>.</p>
<p>In deep learning frameworks like PyTorch, the dimensions of the tensors involved
in operations like matrix multiplication or attention mechanisms often have
specific semantic meanings. In the context of attention mechanisms, especially
in the transformer architecture, the attention mask usually has a shape that is
compatible with the attention logits for element-wise multiplication.</p>
<p>In the transformer model, the attention logits are often computed as a dot
product between query and key vectors, resulting in a tensor of shape
<code class="docutils literal notranslate"><span class="pre">(Batch</span> <span class="pre">size,</span> <span class="pre">Num</span> <span class="pre">heads,</span> <span class="pre">Sequence</span> <span class="pre">length,</span> <span class="pre">Sequence</span> <span class="pre">length)</span></code> or <code class="docutils literal notranslate"><span class="pre">(B,</span> <span class="pre">H,</span> <span class="pre">L,</span> <span class="pre">L)</span></code>.
Here, <code class="docutils literal notranslate"><span class="pre">B</span></code> is the batch size, <code class="docutils literal notranslate"><span class="pre">H</span></code> is the number of attention heads, and <code class="docutils literal notranslate"><span class="pre">L</span></code> is
the sequence length.</p>
<p>To make the mask tensor compatible for element-wise operations with this 4D
tensor, it needs to have a shape that can be broadcasted to <code class="docutils literal notranslate"><span class="pre">(B,</span> <span class="pre">H,</span> <span class="pre">L,</span> <span class="pre">L)</span></code>. A
mask of shape <code class="docutils literal notranslate"><span class="pre">(B,</span> <span class="pre">1,</span> <span class="pre">L,</span> <span class="pre">L)</span></code> fulfills this requirement.</p>
<p>The singleton dimension is added so that the mask can be easily broadcast to the
shape of the attention logits tensor during the computation. When a tensor with
shape <code class="docutils literal notranslate"><span class="pre">(B,</span> <span class="pre">1,</span> <span class="pre">L,</span> <span class="pre">L)</span></code> is element-wise multiplied with a tensor of shape
<code class="docutils literal notranslate"><span class="pre">(B,</span> <span class="pre">H,</span> <span class="pre">L,</span> <span class="pre">L)</span></code>, the singleton dimension (the <code class="docutils literal notranslate"><span class="pre">1</span></code>) allows the mask to be used for
each attention head without explicitly replicating the mask <code class="docutils literal notranslate"><span class="pre">H</span></code> times. This is
more memory-efficient and often faster.</p>
<p>Thus, adding a singleton dimension in masks is a preparatory step that allows
for efficient element-wise operations later in the model’s forward pass.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">target_padding_mask</span> <span class="o">=</span> <span class="n">target_padding_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">target_padding_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">future_mask</span> <span class="o">=</span> <span class="n">future_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">future_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">target_mask</span> <span class="o">=</span> <span class="n">target_padding_mask</span> <span class="o">&amp;</span> <span class="n">future_mask</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">target_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">torch.Size</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span><span style="font-weight: bold">])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">torch.Size</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span><span style="font-weight: bold">])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">torch.Size</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span><span style="font-weight: bold">])</span>
</pre>
</div></div>
</div>
</section>
<section id="why-mask-our-target-in-adder">
<h3><a class="toc-backref" href="#id20" role="doc-backlink">Why mask our target in Adder?</a><a class="headerlink" href="#why-mask-our-target-in-adder" title="Link to this heading">#</a></h3>
<p>If you see the source code of how the <code class="docutils literal notranslate"><span class="pre">AdderDataset</span></code> is constructed, you will
see that we masked out all the tokens before (and including) the equal sign.</p>
<p>For example, if our sequence is <code class="docutils literal notranslate"><span class="pre">12+97=109</span></code>, the input sequence will be
tokenized to the following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">input</span> <span class="o">=</span> <span class="p">[</span><span class="n">BOS</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">+</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="o">=</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">9</span><span class="p">]</span>
<span class="n">target</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">+</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="o">=</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="n">EOS</span><span class="p">]</span>
</pre></div>
</div>
<p>What our code below does is to mask out the tokens before the equal sign for the
target sequence.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">target</span> <span class="o">=</span> <span class="p">[</span><span class="n">MASK</span><span class="p">,</span> <span class="n">MASK</span><span class="p">,</span> <span class="n">MASK</span><span class="p">,</span> <span class="n">MASK</span><span class="p">,</span> <span class="n">MASK</span><span class="p">,</span> <span class="n">MASK</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="n">EOS</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">construct_target_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_sequence</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">:</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">input_sequence</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="n">where_equal_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">input_sequence</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">equal_token_id</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">where_equal_index</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">where_equal_index</span><span class="p">)</span>  <span class="c1"># to appease mypy lol</span>
    <span class="n">target</span><span class="p">[:</span> <span class="n">where_equal_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_token_id</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">target</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
</pre></div>
</div>
<p>Simply put, we do not care what the model predict for anything before the equal
sign. By masking out (or ignoring) the tokens before the =, we are asking the
model to “focus” on generating the correct answer after the equal sign.</p>
</section>
<section id="split-to-train-valid-test">
<h3><a class="toc-backref" href="#id21" role="doc-backlink">Split to Train-Valid-Test</a><a class="headerlink" href="#split-to-train-valid-test" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">batch_size</span>   <span class="o">=</span> <span class="mi">256</span>

<span class="n">composer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">train_loader</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_size</span>
<span class="n">composer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">valid_loader</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_size</span>
<span class="n">composer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">test_loader</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_size</span>

<span class="n">train_dataset</span><span class="p">,</span> <span class="n">valid_dataset</span><span class="p">,</span> <span class="n">test_dataset</span> <span class="o">=</span> <span class="n">split_dataset</span><span class="p">(</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">split</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">global_</span><span class="o">.</span><span class="n">seed</span>
<span class="p">)</span>

<span class="n">train_size</span><span class="p">,</span> <span class="n">valid_size</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">)</span>
<span class="n">train_size</span><span class="p">,</span> <span class="n">valid_size</span><span class="p">,</span> <span class="n">test_size</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(7000, 2000, 1000)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># max_seq_len is determined by 1+ num_digits + 1 + num_digits + 1 + num_digits + 1 + 1</span>
<span class="c1"># where the 1s represent BOS, Plus sign, Equal sign, the extra digit in the sum, EOS, respectively.</span>
<span class="n">max_seq_len</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">composer</span><span class="o">.</span><span class="n">constants</span><span class="o">.</span><span class="n">NUM_DIGITS</span> <span class="o">+</span> <span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">constants</span><span class="o">.</span><span class="n">NUM_DIGITS</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">max_seq_len</span> <span class="o">==</span> <span class="n">composer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">context_length</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="create-dataloader">
<h3><a class="toc-backref" href="#id22" role="doc-backlink">Create DataLoader</a><a class="headerlink" href="#create-dataloader" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_loader</span> <span class="o">=</span> <span class="n">create_loader</span><span class="p">(</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">loader_config</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">train_loader</span><span class="p">,</span>
    <span class="n">collate_fn_config</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">valid_loader</span> <span class="o">=</span> <span class="n">create_loader</span><span class="p">(</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">valid_dataset</span><span class="p">,</span>
    <span class="n">loader_config</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">valid_loader</span><span class="p">,</span>
    <span class="n">collate_fn_config</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">test_loader</span> <span class="o">=</span> <span class="n">create_loader</span><span class="p">(</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">test_dataset</span><span class="p">,</span>
    <span class="n">loader_config</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">test_loader</span><span class="p">,</span>
    <span class="n">collate_fn_config</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">collate_fn</span></code> defines how to combine these variable-length samples into a
batch. This usually involves padding the sequences in the batch to a common
length, which is typically the length of the longest sequence in the batch. Note
here the padding in collate is “redundant” since in our earlier code we ensured
that all sample has same number of characters by way of padding zeros in front.
For example, <code class="docutils literal notranslate"><span class="pre">23</span> <span class="pre">+</span> <span class="pre">3</span> <span class="pre">=26</span></code> will become <code class="docutils literal notranslate"><span class="pre">23</span> <span class="pre">+</span> <span class="pre">03</span> <span class="pre">=</span> <span class="pre">026</span></code>. Consequently, all samples
in the mini-batch will have same length by definition.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">global_</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>

<span class="n">batch_index</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
    <span class="c1"># Each batch is a tuple containing all elements for the batch</span>
    <span class="n">inputs_padded</span><span class="p">,</span> <span class="n">targets_padded</span><span class="p">,</span> <span class="n">padding_masks_padded_and_expanded</span><span class="p">,</span> <span class="n">future_masks_expanded</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># Print the length of each component in the batch</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Batch Size:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs_padded</span><span class="p">))</span>

    <span class="c1"># Now you can print shapes or other properties of each batch element</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Inputs Shape:&quot;</span><span class="p">,</span> <span class="n">inputs_padded</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Targets Shape:&quot;</span><span class="p">,</span> <span class="n">targets_padded</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="c1"># Decoding and other processing can be done here</span>
    <span class="c1"># For example, decoding the first sequence in the batch</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Decoded First Equation/Sample of the Batch:&quot;</span><span class="p">,</span> <span class="n">decode_equation</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">,</span> <span class="n">inputs_padded</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()))</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>

    <span class="n">batch_index</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">batch_index</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span> <span class="k">break</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Batch Size: 256
Inputs Shape: torch.Size([256, 10])
Targets Shape: torch.Size([256, 10])
Decoded First Equation/Sample of the Batch: 31+04=035
--------------------------------------------------------------------------------
Batch Size: 256
Inputs Shape: torch.Size([256, 10])
Targets Shape: torch.Size([256, 10])
Decoded First Equation/Sample of the Batch: 37+49=086
--------------------------------------------------------------------------------
Batch Size: 256
Inputs Shape: torch.Size([256, 10])
Targets Shape: torch.Size([256, 10])
Decoded First Equation/Sample of the Batch: 47+26=073
--------------------------------------------------------------------------------
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Batch Size: 256
Inputs Shape: torch.Size([256, 10])
Targets Shape: torch.Size([256, 10])
Decoded First Equation/Sample of the Batch: 53+05=058
--------------------------------------------------------------------------------
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="model">
<h2><a class="toc-backref" href="#id23" role="doc-backlink">Model</a><a class="headerlink" href="#model" title="Link to this heading">#</a></h2>
<p>We have went into extensive details on the implementation of the decoder in the
<a class="reference internal" href="implementation.html"><span class="std std-doc">implementation section</span></a>. We will not repeat the concepts
here, instead we will just compile the model with the configurations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create individual component configurations</span>
<span class="n">masked_self_attention_mha_config</span> <span class="o">=</span> <span class="n">MultiHeadedAttentionConfig</span><span class="p">(</span>
     <span class="n">attention</span><span class="o">=</span><span class="n">ScaledDotProductAttention</span><span class="p">(),</span>
    <span class="n">d_model</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">H</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span>
<span class="p">)</span>

<span class="n">feed_forward_config</span> <span class="o">=</span> <span class="n">PositionwiseFeedForwardConfig</span><span class="p">(</span>
    <span class="n">d_model</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">d_ff</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(</span><span class="n">approximate</span><span class="o">=</span><span class="s2">&quot;tanh&quot;</span><span class="p">),</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="n">add_norm_config_1</span> <span class="o">=</span> <span class="n">AddNormConfig</span><span class="p">(</span><span class="n">feature_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">add_norm_config_2</span> <span class="o">=</span> <span class="n">AddNormConfig</span><span class="p">(</span><span class="n">feature_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Create DecoderBlockConfig</span>
<span class="n">decoder_block_config</span> <span class="o">=</span> <span class="n">DecoderBlockConfig</span><span class="p">(</span>
    <span class="n">masked_self_attention_mha</span><span class="o">=</span><span class="n">masked_self_attention_mha_config</span><span class="p">,</span>
    <span class="n">feed_forward</span><span class="o">=</span><span class="n">feed_forward_config</span><span class="p">,</span>
    <span class="n">add_norm_1</span><span class="o">=</span><span class="n">add_norm_config_1</span><span class="p">,</span>
    <span class="n">add_norm_2</span><span class="o">=</span><span class="n">add_norm_config_2</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Create the overall DecoderConfig</span>
<span class="n">model_config</span> <span class="o">=</span> <span class="n">DecoderConfig</span><span class="p">(</span>
    <span class="n">d_model</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">context_length</span><span class="o">=</span><span class="n">max_seq_len</span><span class="p">,</span>
    <span class="n">num_decoder_blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">decoder_block</span><span class="o">=</span><span class="n">decoder_block_config</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">GPTDecoder</span><span class="p">(</span><span class="n">model_config</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="n">model_size</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">total_trainable_parameters</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;model_size: </span><span class="si">{</span><span class="n">model_size</span><span class="si">}</span><span class="s1">, train_set_size: </span><span class="si">{</span><span class="n">train_size</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>model_size: 270226, train_set_size: 7000
</pre></div>
</div>
</div>
</div>
</section>
<section id="training-paradigm">
<h2><a class="toc-backref" href="#id24" role="doc-backlink">Training Paradigm</a><a class="headerlink" href="#training-paradigm" title="Link to this heading">#</a></h2>
<p>Here, we would list some of the training paradigms that we would be using in
this project.</p>
<section id="optimizer">
<h3><a class="toc-backref" href="#id25" role="doc-backlink">Optimizer</a><a class="headerlink" href="#optimizer" title="Link to this heading">#</a></h3>
<p>We start off by defining the optimizer for GPT-2. A common choice used is the
<a class="reference external" href="https://arxiv.org/abs/1412.6980">Adam</a> <span id="id1">[<a class="reference internal" href="../../bibliography.html#id24" title="Diederik P. Kingma and Jimmy Ba. Adam: a method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [Submitted on 22 Dec 2014 (v1)]. URL: https://arxiv.org/abs/1412.6980.">Kingma and Ba, 2014</a>]</span> or
<a class="reference external" href="https://arxiv.org/abs/1711.05101">AdamW</a> <span id="id2">[<a class="reference internal" href="../../bibliography.html#id23" title="Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [Submitted on 14 Nov 2017 (v1)]. URL: https://arxiv.org/abs/1711.05101.">Loshchilov and Hutter, 2017</a>]</span>. We
conveniently take the configuration provided in Karpathy’s
<a class="reference external" href="https://github.com/karpathy/nanoGPT">nanoGPT</a>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\eta_{\max} &amp;= 6 \times 10^{-4} \\
\beta_1 &amp;= 0.9 \\
\beta_2 &amp;= 0.95 \\
\epsilon &amp;= 10^{-8} \\
\lambda &amp;= 10^{-1}
\end{aligned}
\end{split}\]</div>
<p>Furthermore, we briefly mention that Karpathy applies weight decay to different
parameter groups - which is quite a common practice. As we can see from the code
below, we define whitelisted and blacklisted modules that we want to apply
weight decay to. The whitelist modules are <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> and the blacklist modules
are <code class="docutils literal notranslate"><span class="pre">nn.LayerNorm</span></code>, <code class="docutils literal notranslate"><span class="pre">nn.Embedding</span></code>.</p>
<p>Weight decary, which is basically L2 regularization penalizes the square of the
weights, encouraging smaller weight values. This can lead to a “spreading out”
effect, as it discourages the model from relying too heavily on a small number
of input features, promoting a more even distribution of weight values and, by
extension, a more balanced consideration of input dimensions. This
regularization technique is particularly beneficial for layers that perform
matrix multiplication, as it helps in ensuring that the model utilizes a broader
range of input features rather than becoming overly dependent on a few dominant
ones. We can find more intuition in the discussion
<a class="reference external" href="https://stats.stackexchange.com/questions/576463/why-not-perform-weight-decay-on-layernorm-embedding">Why not perform weight decay on layernorm/embedding?</a>,
<a class="reference external" href="https://discuss.pytorch.org/t/weight-decay-in-the-optimizers-is-a-bad-idea-especially-with-batchnorm/16994">Weight decay in the optimizers is a bad idea (especially with BatchNorm)</a>
and
<a class="reference external" href="https://github.com/karpathy/minGPT/pull/24#issuecomment-679316025">Weight decay exclusions (Karpathy)</a>.</p>
<div class="highlight-md notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span>def apply_weight_decay_to_different_param_groups(
<span class="linenos"> 2</span>    model: nn.Module, weight_decay: float
<span class="linenos"> 3</span>) -&gt; List[Dict[Literal[&quot;params&quot;, &quot;weight_decay&quot;], List[torch.nn.Parameter] | float]]:
<span class="linenos"> 4</span>    decay: Set[str] = set()
<span class="linenos"> 5</span>    no_decay: Set[str] = set()
<span class="hll"><span class="linenos"> 6</span>    whitelist_weight_modules: Tuple[Type[nn.Module], ...] = (nn.Linear,)
</span><span class="hll"><span class="linenos"> 7</span>    blacklist_weight_modules: Tuple[Type[nn.Module], ...] = (nn.LayerNorm, nn.Embedding, LayerNorm)
</span><span class="linenos"> 8</span>
<span class="linenos"> 9</span>    for module_name, module in model.named_modules():
<span class="linenos">10</span>        for parameter_name, <span class="ge">_parameter in module.named_</span>parameters():
<span class="linenos">11</span>            full_parameter_name = f&quot;{module_name}.{parameter_name}&quot; if module_name else parameter_name
<span class="linenos">12</span>            if parameter_name.endswith(&quot;bias&quot;):
<span class="linenos">13</span>                # biases of all modules are not decayed
<span class="linenos">14</span>                no_decay.add(full_parameter_name)
<span class="linenos">15</span>            elif parameter_name.endswith(&quot;weight&quot;) and isinstance(module, whitelist_weight_modules):
<span class="linenos">16</span>                # weights of whitelisted modules are decayed
<span class="linenos">17</span>                decay.add(full_parameter_name)
<span class="linenos">18</span>            elif parameter_name.endswith(&quot;in_proj_weight&quot;):
<span class="linenos">19</span>                # MHA projection layer, does not exist in my implementation
<span class="linenos">20</span>                decay.add(full_parameter_name)
<span class="linenos">21</span>            elif parameter_name.endswith(&quot;weight&quot;) and isinstance(module, blacklist_weight_modules):
<span class="linenos">22</span>                # weights of blacklisted modules are not decayed
<span class="linenos">23</span>                no_decay.add(full_parameter_name)
<span class="linenos">24</span>            elif (parameter_name.endswith(&quot;gamma&quot;) or parameter_name.endswith(&quot;beta&quot;)) and isinstance(
<span class="linenos">25</span>                module, LayerNorm
<span class="linenos">26</span>            ):
<span class="linenos">27</span>                # weights of LayerNorm modules are not decayed
<span class="linenos">28</span>                # TODO: why do I need to do this is because my custom LayerNorm has gamma and beta
<span class="linenos">29</span>                # as their &quot;weight&quot; and &quot;bias&quot; attributes, respectively.
<span class="linenos">30</span>                no_decay.add(full_parameter_name)
<span class="linenos">31</span>            elif parameter_name.endswith(&quot;pos_embed&quot;):
<span class="linenos">32</span>                no_decay.add(full_parameter_name)
<span class="linenos">33</span>
<span class="linenos">34</span>    param_dict = {parameter_name: parameter for parameter_name, parameter in model.named_parameters()}  # noqa: C416
<span class="linenos">35</span>    inter_params = decay &amp; no_decay
<span class="linenos">36</span>    union_params = decay | no_decay
<span class="linenos">37</span>    assert not inter_params, f&quot;Parameters {inter_params} are in both decay and no_decay sets.&quot;
<span class="linenos">38</span>    assert not (
<span class="linenos">39</span>        param_dict.keys() - union_params
<span class="linenos">40</span>    ), f&quot;Parameters {param_dict.keys() - union_params} were not categorized.&quot;
<span class="linenos">41</span>
<span class="linenos">42</span>    optim_groups: List[Dict[Literal[&quot;params&quot;, &quot;weight_decay&quot;], List[torch.nn.Parameter] | float]] = [
<span class="linenos">43</span>        {&quot;params&quot;: [param_dict[parameter_name] for parameter_name in sorted(decay)], &quot;weight_decay&quot;: weight_decay},
<span class="linenos">44</span>        {&quot;params&quot;: [param_dict[parameter_name] for parameter_name in sorted(no_decay)], &quot;weight_decay&quot;: 0.0},
<span class="linenos">45</span>    ]
<span class="linenos">46</span>
<span class="linenos">47</span>    return optim_groups
</pre></div>
</div>
<p>We won’t go into too much technical rigour on the optimizer, but note that more
modern variations exist, for instance
<a class="reference external" href="https://docs.mosaicml.com/projects/composer/en/stable/api_reference/generated/composer.optim.DecoupledAdamW.html">DecoupledAdamW</a>,
which furthers decouple the weight decay term <span class="math notranslate nohighlight">\(\lambda\)</span> from the learning rate,
as well <a class="reference external" href="https://arxiv.org/abs/1908.03265">RAdam</a> <span id="id3">[<a class="reference internal" href="../../bibliography.html#id25" title="Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265, 2019. [Submitted on 8 Aug 2019 (v1), last revised 26 Oct 2021 (this version, v4)]. URL: https://arxiv.org/abs/1908.03265.">Liu <em>et al.</em>, 2019</a>]</span>, which
is intended to address bias correction factors leading to higher variance in the
adaptive learning rate for the initial training iterations.</p>
<p>To this end, we create the optimizer in code as follows, noting that we would
not use the exact same configuration as Karpathy, but rather use what is
deemed fit for the case at hand.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pprint</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">optimizer</span><span class="p">)</span>
<span class="n">optimizer_config_cls</span> <span class="o">=</span> <span class="n">OPTIMIZER_REGISTRY</span><span class="p">[</span><span class="n">composer</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">name</span><span class="p">]</span>
<span class="n">optimizer_pydantic_config</span> <span class="o">=</span> <span class="n">optimizer_config_cls</span><span class="p">(</span><span class="o">**</span><span class="n">composer</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">model_dump</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s2">&quot;python&quot;</span><span class="p">))</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">optimizer_pydantic_config</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">AdamConfig</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">name</span>=<span style="color: #008000; text-decoration-color: #008000">'torch.optim.Adam'</span>, <span style="color: #808000; text-decoration-color: #808000">lr</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2</span>, <span style="color: #808000; text-decoration-color: #808000">betas</span>=<span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.98</span><span style="font-weight: bold">)</span>, <span style="color: #808000; text-decoration-color: #808000">eps</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1e-09</span>, <span style="color: #808000; text-decoration-color: #808000">weight_decay</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0</span><span style="font-weight: bold">)</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">AdamConfig</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">name</span>=<span style="color: #008000; text-decoration-color: #008000">'torch.optim.Adam'</span>, <span style="color: #808000; text-decoration-color: #808000">lr</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2</span>, <span style="color: #808000; text-decoration-color: #808000">betas</span>=<span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.98</span><span style="font-weight: bold">)</span>, <span style="color: #808000; text-decoration-color: #808000">eps</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1e-09</span>, <span style="color: #808000; text-decoration-color: #808000">weight_decay</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0</span><span style="font-weight: bold">)</span>
</pre>
</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="s2">&quot;weight_decay&quot;</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer_pydantic_config</span><span class="o">.</span><span class="n">build</span><span class="p">(</span>
    <span class="n">params</span><span class="o">=</span><span class="n">apply_weight_decay_to_different_param_groups</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">weight_decay</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Adam <span style="font-weight: bold">(</span>
Parameter Group <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>amsgrad: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>betas: <span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.98</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>capturable: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>differentiable: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>eps: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1e-09</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>foreach: <span style="color: #800080; text-decoration-color: #800080; font-style: italic">None</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>fused: <span style="color: #800080; text-decoration-color: #800080; font-style: italic">None</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>lr: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>maximize: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>weight_decay: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0</span>

Parameter Group <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>amsgrad: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>betas: <span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.98</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>capturable: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>differentiable: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>eps: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1e-09</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>foreach: <span style="color: #800080; text-decoration-color: #800080; font-style: italic">None</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>fused: <span style="color: #800080; text-decoration-color: #800080; font-style: italic">None</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>lr: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>maximize: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>weight_decay: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0</span>
<span style="font-weight: bold">)</span>
</pre>
</div></div>
</div>
</section>
<section id="learning-rate-scheduler">
<h3><a class="toc-backref" href="#id26" role="doc-backlink">Learning Rate Scheduler</a><a class="headerlink" href="#learning-rate-scheduler" title="Link to this heading">#</a></h3>
<p>see common utils</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">cosine_annealing_with_warmup</span><span class="p">(</span>
    <span class="n">t</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">t_warmup</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span> <span class="n">t_max</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span> <span class="n">alpha_f</span><span class="p">:</span> <span class="nb">float</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Computes the learning rate multiplier using cosine annealing with warmup.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    t : int</span>
<span class="sd">        Current training step.</span>
<span class="sd">    t_warmup : Union[int, float]</span>
<span class="sd">        Warmup time in training steps.</span>
<span class="sd">    t_max : Union[int, float]</span>
<span class="sd">        Total duration of the scheduler in training steps.</span>
<span class="sd">    alpha_f : float</span>
<span class="sd">        Learning rate multiplier to decay to.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    alpha : float</span>
<span class="sd">        The learning rate multiplier at the given training step.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">t</span> <span class="o">&lt;</span> <span class="n">t_warmup</span><span class="p">:</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">t</span> <span class="o">/</span> <span class="n">t_warmup</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">tau_w</span> <span class="o">=</span> <span class="p">(</span><span class="n">t</span> <span class="o">-</span> <span class="n">t_warmup</span><span class="p">)</span> <span class="o">/</span> <span class="n">t_max</span>
        <span class="n">tau_w</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">tau_w</span><span class="p">)</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha_f</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha_f</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">tau_w</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="k">return</span> <span class="n">alpha</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t_warmup</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">t_max</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">alpha_f</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">steps</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">110</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>  <span class="c1"># Check at every 10 steps</span>

<span class="c1"># Simulated values for both functions</span>
<span class="n">simulated_values</span> <span class="o">=</span> <span class="p">[(</span><span class="n">step</span><span class="p">,</span> <span class="n">cosine_annealing_with_warmup</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">t_warmup</span><span class="p">,</span> <span class="n">t_max</span><span class="p">,</span> <span class="n">alpha_f</span><span class="p">))</span> <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="n">steps</span><span class="p">]</span>

<span class="n">simulated_values</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(0, 0.0),
 (10, 1.0),
 (20, 0.9755282581475768),
 (30, 0.9045084971874737),
 (40, 0.7938926261462366),
 (50, 0.6545084971874737),
 (60, 0.5),
 (70, 0.34549150281252633),
 (80, 0.2061073738537635),
 (90, 0.09549150281252633),
 (100, 0.024471741852423234)]
</pre></div>
</div>
</div>
</div>
</section>
<section id="loss">
<h3><a class="toc-backref" href="#id27" role="doc-backlink">Loss</a><a class="headerlink" href="#loss" title="Link to this heading">#</a></h3>
<p>Talk and link to bottom notes</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">warmup_steps</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
<span class="c1"># lr first increases in the warmup steps, and then decays</span>
<span class="n">lr_fn</span>        <span class="o">=</span> <span class="k">lambda</span> <span class="n">step</span><span class="p">:</span> <span class="n">model_config</span><span class="o">.</span><span class="n">d_model</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="nb">min</span><span class="p">([(</span><span class="n">step</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="n">step</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">warmup_steps</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">)])</span>
<span class="c1"># optimizer    = torch.optim.Adam(model.parameters(), lr=0.2, betas=(0.9, 0.98), eps=1e-9)</span>

<span class="c1"># optimizer_config = OptimizerConfig(name=&quot;torch.optim.Adam&quot;, lr=0.2, betas=(0.9, 0.98), eps=1e-9)</span>
<span class="c1"># optimizer   = optimizer_config.build(params=model.parameters())</span>

<span class="c1"># optimizer_config = OptimizerConfig(name=&quot;torch.optim.Adam&quot;, lr=0.2)</span>
<span class="c1"># optimizer   = optimizer_config.build(params=model.parameters(), betas=(0.9, 0.98), eps=1e-9)</span>

<span class="n">optimizer</span>   <span class="o">=</span> <span class="n">composer</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
<span class="n">scheduler</span>    <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">LambdaLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">lr_lambda</span><span class="o">=</span><span class="n">lr_fn</span><span class="p">)</span>
<span class="n">criterion</span>    <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">ignore_index</span><span class="o">=</span><span class="n">PAD</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">Metrics</span><span class="p">:</span>
    <span class="n">loss</span><span class="p">:</span> <span class="n">Loss</span>
    <span class="n">accuracy</span><span class="p">:</span> <span class="n">Accuracy</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.98)
    capturable: False
    differentiable: False
    eps: 1e-09
    foreach: None
    fused: None
    lr: 0.2
    maximize: False
    weight_decay: 0.0
)
</pre></div>
</div>
</div>
</div>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">input</span></code> is indeed <code class="docutils literal notranslate"><span class="pre">[bs,</span> <span class="pre">10]</span></code> because max len is 11, so removed last token.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">target</span></code> should be <code class="docutils literal notranslate"><span class="pre">[bs,</span> <span class="pre">10]</span></code> but left shifted of the real original input but somehow i got 11.</p></li>
<li><p>Think of vocab size to be num classes in my classification problem. But the</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create criterion</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">criterion_pydantic_config</span><span class="o">.</span><span class="n">create_instance</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">criterion</span><span class="o">.</span><span class="n">ignore_index</span> <span class="o">==</span> <span class="n">vocabulary</span><span class="o">.</span><span class="n">token_to_index</span><span class="p">[</span><span class="n">vocabulary</span><span class="o">.</span><span class="n">PAD</span><span class="p">]</span>

<span class="c1"># Create Scheduler noam</span>
<span class="c1"># TODO: this part is hardcoded in a way since we are using LambdaLR.</span>
<span class="c1"># I do not have time to make it more &quot;automated&quot; so this is anti-config-pattern.</span>
<span class="n">warmup_steps</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>

<span class="c1"># lr first increases in the warmup steps, and then decays</span>
<span class="n">noam</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">step</span><span class="p">:</span> <span class="n">noam_lr_decay</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="o">=</span><span class="n">warmup_steps</span><span class="p">)</span>  <span class="c1"># noqa: E731</span>

<span class="n">scheduler_config_cls</span> <span class="o">=</span> <span class="n">SCHEDULER_REGISTRY</span><span class="p">[</span><span class="n">cfg</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">name</span><span class="p">]</span>


<span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">scheduler_config_cls</span><span class="p">,</span> <span class="n">LambdaLRConfig</span><span class="p">):</span>
    <span class="n">scheduler_pydantic_config</span> <span class="o">=</span> <span class="n">scheduler_config_cls</span><span class="p">(</span><span class="n">lr_lambda</span><span class="o">=</span><span class="n">noam</span><span class="p">,</span> <span class="o">**</span><span class="n">cfg</span><span class="o">.</span><span class="n">scheduler</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">scheduler_pydantic_config</span> <span class="o">=</span> <span class="n">scheduler_config_cls</span><span class="p">(</span><span class="o">**</span><span class="n">cfg</span><span class="o">.</span><span class="n">scheduler</span><span class="p">)</span>  <span class="c1"># type: ignore[assignment]</span>

<span class="k">assert</span> <span class="n">composer</span><span class="o">.</span><span class="n">scheduler</span> <span class="ow">is</span> <span class="n">MISSING</span>  <span class="c1"># now it is MISSING for us to fill up.</span>
<span class="n">composer</span><span class="o">.</span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">scheduler_pydantic_config</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">scheduler_pydantic_config</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">43</span><span class="p">],</span> <span class="n">line</span> <span class="mi">2</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="c1"># Create criterion</span>
<span class="ne">----&gt; </span><span class="mi">2</span> <span class="n">criterion</span> <span class="o">=</span> <span class="n">criterion_pydantic_config</span><span class="o">.</span><span class="n">create_instance</span><span class="p">()</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="k">assert</span> <span class="n">criterion</span><span class="o">.</span><span class="n">ignore_index</span> <span class="o">==</span> <span class="n">vocabulary</span><span class="o">.</span><span class="n">token_to_index</span><span class="p">[</span><span class="n">vocabulary</span><span class="o">.</span><span class="n">PAD</span><span class="p">]</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="c1"># Create Scheduler noam</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span> <span class="c1"># TODO: this part is hardcoded in a way since we are using LambdaLR.</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span> <span class="c1"># I do not have time to make it more &quot;automated&quot; so this is anti-config-pattern.</span>

<span class="ne">NameError</span>: name &#39;criterion_pydantic_config&#39; is not defined
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">omnivault.transformer.core.state</span> <span class="kn">import</span> <span class="n">State</span>

<span class="n">state</span> <span class="o">=</span> <span class="n">State</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">criterion</span><span class="o">=</span><span class="n">criterion</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
    <span class="n">scheduler</span><span class="o">=</span><span class="n">scheduler</span><span class="p">,</span>
    <span class="n">vocabulary</span><span class="o">=</span><span class="n">vocabulary</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">state</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">State</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">model</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">GPTDecoder</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">  </span><span style="font-weight: bold">(</span>tok_embed<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Embedding</span><span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">18</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">  </span><span style="font-weight: bold">(</span>decoder_blocks<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">ModuleList</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span><span style="font-weight: bold">)</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span> x <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">GPTDecoderBlock</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│     </span><span style="font-weight: bold">(</span>masked_self_attention_mha<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">MultiHeadedAttention</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">(</span>W_Q<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Linear</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">in_features</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #808000; text-decoration-color: #808000">out_features</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #808000; text-decoration-color: #808000">bias</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">(</span>W_K<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Linear</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">in_features</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #808000; text-decoration-color: #808000">out_features</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #808000; text-decoration-color: #808000">bias</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">(</span>W_V<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Linear</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">in_features</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #808000; text-decoration-color: #808000">out_features</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #808000; text-decoration-color: #808000">bias</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">(</span>W_O<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Linear</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">in_features</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #808000; text-decoration-color: #808000">out_features</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #808000; text-decoration-color: #808000">bias</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">(</span>attention<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">ScaledDotProductAttention</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │     </span><span style="font-weight: bold">(</span>dropout<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Dropout</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">p</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0</span>, <span style="color: #808000; text-decoration-color: #808000">inplace</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">(</span>dropout<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Dropout</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">p</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1</span>, <span style="color: #808000; text-decoration-color: #808000">inplace</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│     </span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│     </span><span style="font-weight: bold">(</span>feed_forward<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">PositionwiseFeedForward</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">(</span>ffn<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">ModuleDict</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │     </span><span style="font-weight: bold">(</span>context_fc<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Linear</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">in_features</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #808000; text-decoration-color: #808000">out_features</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">256</span>, <span style="color: #808000; text-decoration-color: #808000">bias</span>=<span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │     </span><span style="font-weight: bold">(</span>activation<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">GELU</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">approximate</span>=<span style="color: #008000; text-decoration-color: #008000">'tanh'</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │     </span><span style="font-weight: bold">(</span>context_projection<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Linear</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">in_features</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">256</span>, <span style="color: #808000; text-decoration-color: #808000">out_features</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #808000; text-decoration-color: #808000">bias</span>=<span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │     </span><span style="font-weight: bold">(</span>dropout<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Dropout</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">p</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1</span>, <span style="color: #808000; text-decoration-color: #808000">inplace</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│     </span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│     </span><span style="font-weight: bold">(</span>add_norm_1<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">AddNorm</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">(</span>dropout<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Dropout</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">p</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1</span>, <span style="color: #808000; text-decoration-color: #808000">inplace</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">(</span>layer_norm<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">LayerNorm</span><span style="font-weight: bold">((</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>,<span style="font-weight: bold">)</span>, <span style="color: #808000; text-decoration-color: #808000">eps</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1e-05</span>, <span style="color: #808000; text-decoration-color: #808000">elementwise_affine</span>=<span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│     </span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│     </span><span style="font-weight: bold">(</span>add_norm_2<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">AddNorm</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">(</span>dropout<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Dropout</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">p</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1</span>, <span style="color: #808000; text-decoration-color: #808000">inplace</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">(</span>layer_norm<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">LayerNorm</span><span style="font-weight: bold">((</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>,<span style="font-weight: bold">)</span>, <span style="color: #808000; text-decoration-color: #808000">eps</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1e-05</span>, <span style="color: #808000; text-decoration-color: #808000">elementwise_affine</span>=<span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│     </span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">  </span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">  </span><span style="font-weight: bold">(</span>dropout<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Dropout</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">p</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1</span>, <span style="color: #808000; text-decoration-color: #808000">inplace</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">  </span><span style="font-weight: bold">(</span>layer_norm<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">LayerNorm</span><span style="font-weight: bold">((</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>,<span style="font-weight: bold">)</span>, <span style="color: #808000; text-decoration-color: #808000">eps</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1e-05</span>, <span style="color: #808000; text-decoration-color: #808000">elementwise_affine</span>=<span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">  </span><span style="font-weight: bold">(</span>head<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Linear</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">in_features</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #808000; text-decoration-color: #808000">out_features</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">18</span>, <span style="color: #808000; text-decoration-color: #808000">bias</span>=<span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">)</span>
<span style="font-weight: bold">)</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">criterion</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">CrossEntropyLoss</span><span style="font-weight: bold">()</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">optimizer</span>=<span style="color: #800080; text-decoration-color: #800080">Adam</span> <span style="font-weight: bold">(</span>
Parameter Group <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>amsgrad: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>betas: <span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.98</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>capturable: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>differentiable: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>eps: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1e-09</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>foreach: <span style="color: #800080; text-decoration-color: #800080; font-style: italic">None</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>fused: <span style="color: #800080; text-decoration-color: #800080; font-style: italic">None</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>initial_lr: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>lr: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.2961808030073203e-05</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>maximize: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>weight_decay: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0</span>
<span style="font-weight: bold">)</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">scheduler</span>=<span style="font-weight: bold">&lt;</span><span style="color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold">torch.optim.lr_scheduler.LambdaLR</span><span style="color: #000000; text-decoration-color: #000000"> object at </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0x1657a0730</span><span style="color: #000000; text-decoration-color: #000000">&gt;,</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">epoch_index</span><span style="color: #000000; text-decoration-color: #000000">=</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span><span style="color: #000000; text-decoration-color: #000000">,</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">train_batch_index</span><span style="color: #000000; text-decoration-color: #000000">=</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span><span style="color: #000000; text-decoration-color: #000000">,</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">step_index</span><span style="color: #000000; text-decoration-color: #000000">=</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span><span style="color: #000000; text-decoration-color: #000000">,</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">history</span><span style="color: #000000; text-decoration-color: #000000">=</span><span style="color: #000000; text-decoration-color: #000000; font-weight: bold">{}</span><span style="color: #000000; text-decoration-color: #000000">,</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">vocabulary</span><span style="color: #000000; text-decoration-color: #000000">=&lt;omnivault.transformer.core.vocabulary.AdderVocabulary object at </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0x1659237f0</span><span style="color: #000000; text-decoration-color: #000000">&gt;,</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">tokenizer</span><span style="color: #000000; text-decoration-color: #000000">=&lt;omnivault.transformer.core.tokenizer.AdderTokenizer object at </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0x1659233d0</span><span style="font-weight: bold">&gt;</span>
<span style="font-weight: bold">)</span>
</pre>
</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pprint</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">trainer</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">TrainerConfig</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">device</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">device</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">type</span>=<span style="color: #008000; text-decoration-color: #008000">'cpu'</span><span style="font-weight: bold">)</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">max_epochs</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">log_every_n_steps</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">eval_every_n_steps</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">step_scheduler_on_batch_or_epoch</span>=<span style="color: #008000; text-decoration-color: #008000">'epoch'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">use_amp</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">autocast_config</span>=<span style="font-weight: bold">{</span><span style="color: #008000; text-decoration-color: #008000">'enabled'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #008000; text-decoration-color: #008000">'dtype'</span>: torch.bfloat16, <span style="color: #008000; text-decoration-color: #008000">'cache_enabled'</span>: <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">}</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">scaler_config</span>=<span style="font-weight: bold">{</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #008000; text-decoration-color: #008000">'enabled'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #008000; text-decoration-color: #008000">'init_scale'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">65536.0</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #008000; text-decoration-color: #008000">'growth_factor'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.0</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #008000; text-decoration-color: #008000">'backoff_factor'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.5</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #008000; text-decoration-color: #008000">'growth_interval'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2000</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="font-weight: bold">}</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">gradient_accumulation_steps</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">clip_grad_norm</span>=<span style="font-weight: bold">{</span><span style="color: #008000; text-decoration-color: #008000">'max_norm'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.0</span>, <span style="color: #008000; text-decoration-color: #008000">'norm_type'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.0</span>, <span style="color: #008000; text-decoration-color: #008000">'error_if_nonfinite'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #008000; text-decoration-color: #008000">'foreach'</span>: <span style="color: #800080; text-decoration-color: #800080; font-style: italic">None</span><span style="font-weight: bold">}</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">apply_weight_decay_to_different_param_groups</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">save_dir</span>=<span style="color: #008000; text-decoration-color: #008000">'checkpoints'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">save_every_epoch</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">save_best_only</span>=<span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">monitor</span>=<span style="color: #008000; text-decoration-color: #008000">'valid_this_epoch_average_loss'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">mode</span>=<span style="color: #008000; text-decoration-color: #008000">'min'</span>
<span style="font-weight: bold">)</span>
</pre>
</div></div>
</div>
<p>talk about state fromr eadme</p>
<p>│   │   use_amp=False,
│   │   autocast_config={‘enabled’: False},
│   │   scaler_config={‘enabled’: False, ‘init_scale’: 65536.0, ‘growth_factor’: 2.0, ‘backoff_factor’: 0.5, ‘growth_interval’: 2000},</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># pprint(composer)</span>

<span class="n">composer</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">use_amp</span><span class="o">=</span><span class="kc">False</span>
<span class="n">composer</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">autocast_config</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;enabled&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="s1">&#39;cache_enabled&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>
<span class="n">composer</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">scaler_config</span><span class="p">[</span><span class="s2">&quot;enabled&quot;</span><span class="p">]</span><span class="o">=</span><span class="kc">False</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">omnivault.transformer.core.trainer</span> <span class="kn">import</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">TrainerEvent</span>
<span class="kn">from</span> <span class="nn">omnivault.transformer.core.callbacks</span> <span class="kn">import</span> <span class="n">save_state</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">state</span><span class="o">=</span><span class="n">state</span><span class="p">,</span>
    <span class="n">composer</span><span class="o">=</span><span class="n">composer</span><span class="p">,</span>
    <span class="n">logger</span><span class="o">=</span><span class="n">LOGGER</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>  <span class="c1"># type: ignore[arg-type]</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">remove_callback</span><span class="p">(</span><span class="n">event</span><span class="o">=</span><span class="n">TrainerEvent</span><span class="o">.</span><span class="n">ON_VALID_EPOCH_END</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">save_state</span><span class="p">)</span>
<span class="c1"># trainer.add_callback(</span>
<span class="c1">#     TrainerEvent.ON_VALID_EPOCH_END.value,</span>
<span class="c1">#     lambda trainer: evaluate_and_generate_on_valid_epoch_end(trainer, num_batches_to_eval=None),</span>
<span class="c1"># )</span>
<span class="n">_trained_state</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_loader</span><span class="o">=</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="o">=</span><span class="n">valid_loader</span><span class="p">,</span> <span class="n">test_loader</span><span class="o">=</span><span class="n">test_loader</span><span class="p">)</span>
<span class="n">_trained_state</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">_trained_state</span><span class="o">.</span><span class="n">history</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/gaohn/miniconda3/envs/omniverse/lib/python3.9/site-packages/torch/amp/autocast_mode.py:266: UserWarning: In CPU autocast, but the target dtype is not supported. Disabling autocast.
CPU Autocast only supports dtype of torch.bfloat16 currently.
  warnings.warn(error_message)
</pre></div>
</div>
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #7fbfbf; text-decoration-color: #7fbfbf">[2024-03-21 23:01:22] </span><span style="color: #008000; text-decoration-color: #008000">INFO    </span> Initial learning rate: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0019174124721184262</span>                         <a href="file:///Users/gaohn/omniverse/omniverse/omnivault/transformer/core/callbacks.py" target="_blank"><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">callbacks.py</span></a><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">:</span><a href="file:///Users/gaohn/omniverse/omniverse/omnivault/transformer/core/callbacks.py#63" target="_blank"><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">63</span></a>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #7fbfbf; text-decoration-color: #7fbfbf">[2024-03-21 23:01:22] </span><span style="color: #008000; text-decoration-color: #008000">INFO    </span> Initial learning rate: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0019174124721184262</span>                         <a href="file:///Users/gaohn/omniverse/omniverse/omnivault/transformer/core/callbacks.py" target="_blank"><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">callbacks.py</span></a><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">:</span><a href="file:///Users/gaohn/omniverse/omniverse/omnivault/transformer/core/callbacks.py#63" target="_blank"><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">63</span></a>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #7fbfbf; text-decoration-color: #7fbfbf">[2024-03-21 23:01:22] </span><span style="color: #008000; text-decoration-color: #008000">INFO    </span> Initial learning rate: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0019174124721184262</span>                         <a href="file:///Users/gaohn/omniverse/omniverse/omnivault/transformer/core/callbacks.py" target="_blank"><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">callbacks.py</span></a><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">:</span><a href="file:///Users/gaohn/omniverse/omniverse/omnivault/transformer/core/callbacks.py#63" target="_blank"><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">63</span></a>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #7fbfbf; text-decoration-color: #7fbfbf">                      </span><span style="color: #008000; text-decoration-color: #008000">INFO    </span> Total Parameters: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">270226</span>, Trainable Parameters: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">270226</span>              <a href="file:///Users/gaohn/omniverse/omniverse/omnivault/transformer/core/callbacks.py" target="_blank"><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">callbacks.py</span></a><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">:</span><a href="file:///Users/gaohn/omniverse/omniverse/omnivault/transformer/core/callbacks.py#120" target="_blank"><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">120</span></a>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #7fbfbf; text-decoration-color: #7fbfbf">                      </span><span style="color: #008000; text-decoration-color: #008000">INFO    </span> Total Parameters: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">270226</span>, Trainable Parameters: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">270226</span>              <a href="file:///Users/gaohn/omniverse/omniverse/omnivault/transformer/core/callbacks.py" target="_blank"><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">callbacks.py</span></a><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">:</span><a href="file:///Users/gaohn/omniverse/omniverse/omnivault/transformer/core/callbacks.py#120" target="_blank"><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">120</span></a>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #7fbfbf; text-decoration-color: #7fbfbf">                      </span><span style="color: #008000; text-decoration-color: #008000">INFO    </span> Total Parameters: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">270226</span>, Trainable Parameters: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">270226</span>              <a href="file:///Users/gaohn/omniverse/omniverse/omnivault/transformer/core/callbacks.py" target="_blank"><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">callbacks.py</span></a><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">:</span><a href="file:///Users/gaohn/omniverse/omniverse/omnivault/transformer/core/callbacks.py#120" target="_blank"><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">120</span></a>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #7fbfbf; text-decoration-color: #7fbfbf">                      </span><span style="color: #008000; text-decoration-color: #008000">INFO    </span> ====================================================== Starting      <a href="file:///Users/gaohn/omniverse/omniverse/omnivault/transformer/core/callbacks.py" target="_blank"><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">callbacks.py</span></a><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">:</span><a href="file:///Users/gaohn/omniverse/omniverse/omnivault/transformer/core/callbacks.py#68" target="_blank"><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">68</span></a>
<span style="color: #7fbfbf; text-decoration-color: #7fbfbf">                      </span>         Train Epoch: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>/<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>                                                     <span style="color: #7f7f7f; text-decoration-color: #7f7f7f">               </span>
<span style="color: #7fbfbf; text-decoration-color: #7fbfbf">                      </span>         ======================================================               <span style="color: #7f7f7f; text-decoration-color: #7f7f7f">               </span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #7fbfbf; text-decoration-color: #7fbfbf">                      </span><span style="color: #008000; text-decoration-color: #008000">INFO    </span> ====================================================== Starting      <a href="file:///Users/gaohn/omniverse/omniverse/omnivault/transformer/core/callbacks.py" target="_blank"><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">callbacks.py</span></a><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">:</span><a href="file:///Users/gaohn/omniverse/omniverse/omnivault/transformer/core/callbacks.py#68" target="_blank"><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">68</span></a>
<span style="color: #7fbfbf; text-decoration-color: #7fbfbf">                      </span>         Train Epoch: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>/<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>                                                     <span style="color: #7f7f7f; text-decoration-color: #7f7f7f">               </span>
<span style="color: #7fbfbf; text-decoration-color: #7fbfbf">                      </span>         ======================================================               <span style="color: #7f7f7f; text-decoration-color: #7f7f7f">               </span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #7fbfbf; text-decoration-color: #7fbfbf">                      </span><span style="color: #008000; text-decoration-color: #008000">INFO    </span> ====================================================== Starting      <a href="file:///Users/gaohn/omniverse/omniverse/omnivault/transformer/core/callbacks.py" target="_blank"><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">callbacks.py</span></a><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">:</span><a href="file:///Users/gaohn/omniverse/omniverse/omnivault/transformer/core/callbacks.py#68" target="_blank"><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">68</span></a>
<span style="color: #7fbfbf; text-decoration-color: #7fbfbf">                      </span>         Train Epoch: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>/<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>                                                     <span style="color: #7f7f7f; text-decoration-color: #7f7f7f">               </span>
<span style="color: #7fbfbf; text-decoration-color: #7fbfbf">                      </span>         ======================================================               <span style="color: #7f7f7f; text-decoration-color: #7f7f7f">               </span>
</pre>
</div><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                                      
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">RuntimeError</span><span class="g g-Whitespace">                              </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">155</span><span class="p">],</span> <span class="n">line</span> <span class="mi">12</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span> <span class="n">trainer</span><span class="o">.</span><span class="n">remove_callback</span><span class="p">(</span><span class="n">event</span><span class="o">=</span><span class="n">TrainerEvent</span><span class="o">.</span><span class="n">ON_VALID_EPOCH_END</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">save_state</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">8</span> <span class="c1"># trainer.add_callback(</span>
<span class="g g-Whitespace">      </span><span class="mi">9</span> <span class="c1">#     TrainerEvent.ON_VALID_EPOCH_END.value,</span>
<span class="g g-Whitespace">     </span><span class="mi">10</span> <span class="c1">#     lambda trainer: evaluate_and_generate_on_valid_epoch_end(trainer, num_batches_to_eval=None),</span>
<span class="g g-Whitespace">     </span><span class="mi">11</span> <span class="c1"># )</span>
<span class="ne">---&gt; </span><span class="mi">12</span> <span class="n">_trained_state</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_loader</span><span class="o">=</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="o">=</span><span class="n">valid_loader</span><span class="p">,</span> <span class="n">test_loader</span><span class="o">=</span><span class="n">test_loader</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">13</span> <span class="n">_trained_state</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
<span class="g g-Whitespace">     </span><span class="mi">14</span> <span class="n">history</span> <span class="o">=</span> <span class="n">_trained_state</span><span class="o">.</span><span class="n">history</span>

<span class="nn">File ~/omniverse/omniverse/omnivault/transformer/core/trainer.py:455,</span> in <span class="ni">Trainer.fit</span><span class="nt">(self, train_loader, valid_loader, test_loader)</span>
<span class="g g-Whitespace">    </span><span class="mi">453</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">454</span>     <span class="bp">self</span><span class="o">.</span><span class="n">epoch_index</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="ne">--&gt; </span><span class="mi">455</span>     <span class="bp">self</span><span class="o">.</span><span class="n">train_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_one_epoch</span><span class="p">(</span><span class="n">dataloader</span><span class="o">=</span><span class="n">train_loader</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">457</span>     <span class="k">if</span> <span class="n">valid_loader</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">458</span>         <span class="bp">self</span><span class="o">.</span><span class="n">valid_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">valid_one_epoch</span><span class="p">(</span><span class="n">dataloader</span><span class="o">=</span><span class="n">valid_loader</span><span class="p">)</span>

<span class="nn">File ~/omniverse/omniverse/omnivault/transformer/core/trainer.py:317,</span> in <span class="ni">Trainer.train_one_epoch</span><span class="nt">(self, dataloader)</span>
<span class="g g-Whitespace">    </span><span class="mi">314</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">315</span> <span class="n">total_samples</span> <span class="o">+=</span> <span class="n">batch_size</span>
<span class="ne">--&gt; </span><span class="mi">317</span> <span class="n">this_batch_average_loss</span><span class="p">,</span> <span class="n">this_batch_total_loss</span><span class="p">,</span> <span class="n">this_batch_average_perplexity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_one_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">318</span> <span class="n">this_epoch_total_running_loss</span> <span class="o">+=</span> <span class="n">this_batch_total_loss</span>
<span class="g g-Whitespace">    </span><span class="mi">320</span> <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">epoch_index</span><span class="si">}</span><span class="s2">, Step: </span><span class="si">{</span><span class="n">_batch_index</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nn">File ~/omniverse/omniverse/omnivault/transformer/core/trainer.py:222,</span> in <span class="ni">Trainer._train_one_batch</span><span class="nt">(self, batch)</span>
<span class="g g-Whitespace">    </span><span class="mi">219</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">target_padding_masks</span><span class="p">,</span> <span class="n">future_masks</span> <span class="o">=</span> <span class="n">move_to_device</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">220</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">222</span> <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">context_manager</span><span class="p">:</span>  <span class="c1"># no ops if not enabled</span>
<span class="g g-Whitespace">    </span><span class="mi">223</span>     <span class="n">logits</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">224</span>         <span class="n">inputs</span><span class="p">,</span> <span class="n">target_padding_masks</span><span class="o">=</span><span class="n">target_padding_masks</span><span class="p">,</span> <span class="n">future_masks</span><span class="o">=</span><span class="n">future_masks</span>
<span class="g g-Whitespace">    </span><span class="mi">225</span>     <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">226</span>     <span class="n">loss</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(),</span> <span class="n">targets</span><span class="o">.</span><span class="n">contiguous</span><span class="p">())</span>

<span class="nn">File ~/miniconda3/envs/omniverse/lib/python3.9/site-packages/torch/amp/autocast_mode.py:329,</span> in <span class="ni">autocast.__enter__</span><span class="nt">(self)</span>
<span class="g g-Whitespace">    </span><span class="mi">327</span>     <span class="bp">self</span><span class="o">.</span><span class="n">prev_fastdtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">get_autocast_cpu_dtype</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">328</span>     <span class="n">torch</span><span class="o">.</span><span class="n">set_autocast_cpu_enabled</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_enabled</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">329</span>     <span class="n">torch</span><span class="o">.</span><span class="n">set_autocast_cpu_dtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fast_dtype</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>
<span class="g g-Whitespace">    </span><span class="mi">330</span>     <span class="n">torch</span><span class="o">.</span><span class="n">autocast_increment_nesting</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">331</span> <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;xpu&quot;</span><span class="p">:</span>

<span class="ne">RuntimeError</span>: Currently, AutocastCPU only support Bfloat16 as the autocast_cpu_dtype
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataloader</span><span class="o">=</span><span class="n">train_loader</span><span class="p">,</span>
    <span class="n">criterion</span><span class="o">=</span><span class="n">criterion</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
    <span class="n">scheduler</span><span class="o">=</span><span class="n">scheduler</span><span class="p">,</span>
    <span class="n">grad_norm_clip</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
    <span class="n">valid_dataloader</span><span class="o">=</span><span class="n">valid_loader</span><span class="p">,</span>
    <span class="c1"># test_dataloader=test_loader,</span>
    <span class="c1"># NOTE: uncomment the above line to enable testing after each epoch</span>
    <span class="c1"># but seeding will affect.</span>
<span class="p">)</span>

<span class="k">if</span> <span class="n">DEBUG</span><span class="p">:</span>
    <span class="n">trained_model</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">max_epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># or 15</span>
    <span class="c1"># torch.save(model.state_dict(), &#39;model_debug.pt&#39;)</span>
    <span class="c1"># model_debug = torch.load(&#39;./model_debug.pt&#39;)</span>
    <span class="c1"># if are_both_models_same(model.state_dict(), model_debug):</span>
    <span class="c1">#     print(&quot;Pass&quot;)</span>
    <span class="c1"># else:</span>
    <span class="c1">#     print(&quot;Fail&quot;)</span>

<span class="k">else</span><span class="p">:</span>
    <span class="n">trained_model</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">max_epochs</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>

    <span class="c1"># torch.save(model.state_dict(), &#39;model_non_debug.pt&#39;)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">TypeError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">91</span><span class="p">],</span> <span class="n">line</span> <span class="mi">1</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span>     <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span>     <span class="n">train_dataloader</span><span class="o">=</span><span class="n">train_loader</span><span class="p">,</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span>     <span class="n">criterion</span><span class="o">=</span><span class="n">criterion</span><span class="p">,</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span>     <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span>     <span class="n">scheduler</span><span class="o">=</span><span class="n">scheduler</span><span class="p">,</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span>     <span class="n">grad_norm_clip</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
<span class="g g-Whitespace">      </span><span class="mi">8</span>     <span class="n">device</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<span class="g g-Whitespace">      </span><span class="mi">9</span>     <span class="n">valid_dataloader</span><span class="o">=</span><span class="n">valid_loader</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">10</span>     <span class="c1"># test_dataloader=test_loader,</span>
<span class="g g-Whitespace">     </span><span class="mi">11</span>     <span class="c1"># NOTE: uncomment the above line to enable testing after each epoch</span>
<span class="g g-Whitespace">     </span><span class="mi">12</span>     <span class="c1"># but seeding will affect.</span>
<span class="g g-Whitespace">     </span><span class="mi">13</span> <span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">15</span> <span class="k">if</span> <span class="n">DEBUG</span><span class="p">:</span>
<span class="g g-Whitespace">     </span><span class="mi">16</span>     <span class="n">trained_model</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">max_epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># or 15</span>

<span class="ne">TypeError</span>: __init__() got an unexpected keyword argument &#39;model&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

<span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">target_padding_masks</span><span class="p">,</span> <span class="n">future_masks</span> <span class="o">=</span> <span class="n">batch</span>


<span class="c1"># Step 2: Pass the sample through the model</span>
<span class="n">trained_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># Set the model to evaluation mode</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="c1"># Assuming your model and sample require specific formatting, adjust as necessary</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target_padding_masks</span><span class="o">=</span><span class="n">target_padding_masks</span><span class="p">,</span> <span class="n">future_masks</span><span class="o">=</span><span class="n">future_masks</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">last_decoder_block</span> <span class="o">=</span> <span class="n">trained_model</span><span class="o">.</span><span class="n">decoder_blocks</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># take last decoder block? more feature?</span>
<span class="c1"># pprint(last_decoder_block)</span>

<span class="n">masked_self_attention_mha</span> <span class="o">=</span> <span class="n">last_decoder_block</span><span class="o">.</span><span class="n">masked_self_attention_mha</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">masked_self_attention_mha</span><span class="p">)</span>

<span class="n">context_vector</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">masked_self_attention_mha</span><span class="o">.</span><span class="n">context_vector</span><span class="p">,</span> <span class="n">masked_self_attention_mha</span><span class="o">.</span><span class="n">attention_weights</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">attention_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># but has H=4 heads so do we take 1 head and check the heatmap?</span>
<span class="c1"># torch.Size([208, 4, 10, 10])</span>

<span class="n">last_batch_last_sample_first_head_attention_weights</span> <span class="o">=</span> <span class="n">attention_weights</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">last_batch_last_sample_first_head_attention_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>the xy axis is keys and queries, which is correct <code class="docutils literal notranslate"><span class="pre">Q</span> <span class="pre">&#64;</span> <span class="pre">K.T</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="c1"># Your existing setup</span>
<span class="n">last_decoder_block</span> <span class="o">=</span> <span class="n">trained_model</span><span class="o">.</span><span class="n">decoder_blocks</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">masked_self_attention_mha</span> <span class="o">=</span> <span class="n">last_decoder_block</span><span class="o">.</span><span class="n">masked_self_attention_mha</span>
<span class="n">context_vector</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">masked_self_attention_mha</span><span class="o">.</span><span class="n">context_vector</span><span class="p">,</span> <span class="n">masked_self_attention_mha</span><span class="o">.</span><span class="n">attention_weights</span>

<span class="c1"># Number of heads</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="n">attention_weights</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Labels for each character in the sequence, including BOS</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;&lt;BOS&gt;&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="s1">&#39;59+14=073&#39;</span><span class="p">)</span>

<span class="c1"># Loop over each head and plot its heatmap</span>
<span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

    <span class="c1"># Extract attention weights for the last sample in the last batch for this head</span>
    <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">attention_weights</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">head</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">attention_matrix</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Attention Weights Heatmap for &#39;&lt;BOS&gt;59+14=073&#39; - Head </span><span class="si">{</span><span class="n">head</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Keys&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Queries&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>x -&gt; tensor([[15,  9,  8, 10,  3,  5, 13]])
future_mask -&gt; 7x7
tensor([[ True, False, False, False, False, False, False],
│   │   [ True,  True, False, False, False, False, False],
│   │   [ True,  True,  True, False, False, False, False],
│   │   [ True,  True,  True,  True, False, False, False],
│   │   [ True,  True,  True,  True,  True, False, False],
│   │   [ True,  True,  True,  True,  True,  True, False],
│   │   [ True,  True,  True,  True,  True,  True,  True]])

logits--&gt; 1x7x18 because 1 sample
tensor([[[  7.8,  -0.2,  -2.3,  -1.1,  -0.1,  -3.2,  -4.4,
          -2.4,   3.7,  -0.9,  -5.1,  -4.5,  -5.6,  -2.2,
          -0.5,  -4.2,  -2.9,  -4.9],
        [  0.3,   3.7,   0.9,   1.7,   0.4,  -4.0,  -6.0,
          -2.3,   8.5,   7.3,  -6.0,  -5.1,  -6.2,  -3.0,
         -10.9,  -3.8,  -5.3,  -5.9],
        [-10.5,  -0.4,   4.3,   2.4,  -6.3,  -8.9,  -0.1,
           8.2,   8.6,   0.4,   1.2,   0.9,   0.7,   0.6,
           6.9,   0.0,   0.4,   1.2],
        [ -2.8,   9.6,   2.0,  -6.2,  -8.2,  -2.3,   5.7,
           6.6,  -0.3,  -4.7,  -0.5,  -0.9,  -0.9,   1.2,
           2.3,  -0.4,   0.1,  -1.5],
        [ -2.9,   1.6,  -1.0,  -5.8,  -0.2,   6.2,  14.1,
           8.0,  -4.0,  -9.7,  -2.1,  -3.4,  -3.2,  -1.4,
           0.0,  -1.7,   0.0,  -3.0],
        [ -9.4,   1.7,   5.4,  -1.3,  -6.6,  -4.7,   6.7,
          10.2,   1.9,  -9.6,   0.8,   0.6,   0.7,   1.2,
          10.2,   0.4,   1.3,   1.2],
        [  0.3,  16.1,   3.2,  -4.4,  -5.7,  -2.9,  -3.7,
          -6.1,  -2.1,   4.0,  -0.4,   0.1,  -0.4,   0.0,
           0.6,  -0.6,  -1.2,  -0.7]]])

logits.argmax(dim=-1) -&gt; 1x7
tensor([[0,  8,  8,  1,  6, 14,  1]])
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">logits.argmax(dim=-1)</span></code> basically compress 1x7x18 to 1x7 where for each row of the
7 rows, find the index that is maximum for example, first row 7.8 is max of all
18 elements, so index 0 is returned. <code class="docutils literal notranslate"><span class="pre">tensor([[0,</span>&#160; <span class="pre">8,</span>&#160; <span class="pre">8,</span>&#160; <span class="pre">1,</span>&#160; <span class="pre">6,</span> <span class="pre">14,</span>&#160; <span class="pre">1]])</span></code></p>
<p>There is some meaning here too, remember our input <code class="docutils literal notranslate"><span class="pre">[15,</span> <span class="pre">9,</span> <span class="pre">8,</span> <span class="pre">10,</span> <span class="pre">3,</span> <span class="pre">5,</span> <span class="pre">13]</span></code>
this is basically the BOS (15) up till the equal sign, then
<code class="docutils literal notranslate"><span class="pre">[</span> <span class="pre">0,</span> <span class="pre">8,</span> <span class="pre">8,</span> <span class="pre">1,</span> <span class="pre">6,</span> <span class="pre">14,</span> <span class="pre">1]</span></code> is basically the prediction of each token what comes
next.</p>
<ol class="arabic simple">
<li><p><strong>Input Sequence</strong>: Your input sequence is <code class="docutils literal notranslate"><span class="pre">[15,</span> <span class="pre">9,</span> <span class="pre">8,</span> <span class="pre">10,</span> <span class="pre">3,</span> <span class="pre">5,</span> <span class="pre">13]</span></code>. In
this context, <code class="docutils literal notranslate"><span class="pre">15</span></code> could be a special token like BOS (Beginning of Sentence)
or something else depending on your encoding scheme.</p></li>
<li><p><strong>Output Tensor Interpretation</strong>: The output tensor
<code class="docutils literal notranslate"><span class="pre">tensor([[</span> <span class="pre">0,</span> <span class="pre">8,</span> <span class="pre">8,</span> <span class="pre">1,</span> <span class="pre">6,</span> <span class="pre">14,</span> <span class="pre">1]])</span></code> represents the model’s sequential
predictions for each step of the input:</p>
<ul class="simple">
<li><p>The first element <code class="docutils literal notranslate"><span class="pre">0</span></code> is the prediction following the first element <code class="docutils literal notranslate"><span class="pre">15</span></code> of
the input.</p></li>
<li><p>The second element <code class="docutils literal notranslate"><span class="pre">8</span></code> is the prediction after seeing the first two
elements <code class="docutils literal notranslate"><span class="pre">15,</span> <span class="pre">9</span></code> of the input.</p></li>
<li><p>The third element <code class="docutils literal notranslate"><span class="pre">8</span></code> is predicted after seeing <code class="docutils literal notranslate"><span class="pre">15,</span> <span class="pre">9,</span> <span class="pre">8</span></code>.</p></li>
<li><p>The fourth element <code class="docutils literal notranslate"><span class="pre">1</span></code> follows after <code class="docutils literal notranslate"><span class="pre">15,</span> <span class="pre">9,</span> <span class="pre">8,</span> <span class="pre">10</span></code>.</p></li>
<li><p>The sequence continues in this manner, with each new prediction based on an
increasingly longer prefix of the input sequence.</p></li>
</ul>
</li>
<li><p><strong>Sequential Predictions</strong>: This output suggests that the model is working in
an autoregressive manner. It generates predictions one token at a time, and
each prediction is based on the sequence of tokens it has seen up to that
point.</p></li>
<li><p><strong>Specific Meanings of Output Tokens</strong>: The actual meaning of each token in
your output tensor (<code class="docutils literal notranslate"><span class="pre">0</span></code>, <code class="docutils literal notranslate"><span class="pre">8</span></code>, <code class="docutils literal notranslate"><span class="pre">1</span></code>, <code class="docutils literal notranslate"><span class="pre">6</span></code>, <code class="docutils literal notranslate"><span class="pre">14</span></code>, etc.) depends on your specific
encoding and task. In a language model, these would correspond to specific
words or characters. In a numerical context, they could represent numbers or
operations.</p></li>
</ol>
<p>In summary, the output tensor reflects the model’s predictions for what comes
next in the sequence, based on the current and all previous input tokens. Each
element in the output is the model’s guess for the next token, considering the
sequence of tokens it has seen up to that point.</p>
<blockquote>
<div><p>Then we move on to the concat operation:</p>
</div></blockquote>
<ul class="simple">
<li><p>In our model, after processing the input <code class="docutils literal notranslate"><span class="pre">[15,</span> <span class="pre">9,</span> <span class="pre">8,</span> <span class="pre">10,</span> <span class="pre">3,</span> <span class="pre">5,</span> <span class="pre">13]</span></code>, it
predicts the next token to be <code class="docutils literal notranslate"><span class="pre">1</span></code>. This prediction is based on the entire
sequence seen so far.</p></li>
<li><p>The process of extending the input sequence with this new token (<code class="docutils literal notranslate"><span class="pre">1</span></code>) and then
feeding this extended sequence back into the model for further predictions is
indeed an example of greedy decoding. The model is iteratively building a
longer sequence, one token at a time, always choosing the most likely next
token at each step.</p></li>
<li><p>This process would continue until a stopping condition is met, which might be
the prediction of an EOS (End of Sentence) token or reaching a maximum
sequence length.</p></li>
</ul>
<blockquote>
<div><p>for i in range(num_digits + 2):
now you know why loop over 4 times in total if num digits is 2.
This is because, after equal sign, we will have answer of 3 digits (xyz)
and an EOS token, our stop condition!</p>
</div></blockquote>
<p>Lastly: <code class="docutils literal notranslate"><span class="pre">tensor([[15,</span>&#160; <span class="pre">9,</span>&#160; <span class="pre">8,</span> <span class="pre">10,</span>&#160; <span class="pre">3,</span>&#160; <span class="pre">5,</span> <span class="pre">13,</span>&#160; <span class="pre">1,</span>&#160; <span class="pre">3,</span>&#160; <span class="pre">3,</span> <span class="pre">14]])</span></code> is the full predicted
after EOS is met.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">config</span><span class="o">.</span><span class="n">global_config</span><span class="o">.</span><span class="n">seed</span> <span class="o">=</span> <span class="mi">42</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">construct_future_mask</span><span class="p">(</span><span class="n">seq_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">:</span>
    <span class="n">future_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
    <span class="n">future_mask</span> <span class="o">=</span> <span class="n">future_mask</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
    <span class="n">future_mask</span> <span class="o">=</span> <span class="n">future_mask</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">(</span><span class="n">future_mask</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">construct_padding_mask</span><span class="p">(</span><span class="n">input_sequence</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">:</span>
    <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">input_sequence</span> <span class="o">!=</span> <span class="n">pad_token_id</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">(</span><span class="n">padding_mask</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">compute_sum</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
    <span class="s2">&quot;Function for computing the sum of two numbers.&quot;</span>
    <span class="c1"># x=[[15,  9,  8, 10,  3,  5, 13]]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_digits</span> <span class="o">+</span> <span class="mi">2</span><span class="p">):</span>
        <span class="c1"># pprint(x)</span>
        <span class="n">pad_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="n">PAD</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
        <span class="n">future_mask</span> <span class="o">=</span> <span class="n">construct_future_mask</span><span class="p">(</span><span class="n">seq_len</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="n">future_mask</span> <span class="o">=</span> <span class="n">future_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1">#print(pad_mask.shape, future_mask.shape)</span>
        <span class="c1">#inputs, targets, target_padding_masks, future_masks = construct_batches(x)</span>
        <span class="c1">#print(target_padding_masks.shape, future_masks.shape)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_tokens</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">target_padding_masks</span><span class="o">=</span><span class="n">pad_mask</span><span class="p">,</span> <span class="n">future_masks</span><span class="o">=</span><span class="n">future_mask</span><span class="p">)</span>
        <span class="n">pprint</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
        <span class="c1">#logits = model(inputs, target_padding_masks=target_padding_masks, future_masks=future_masks)</span>

        <span class="n">last_output</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">last_output</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
        <span class="c1"># STOPPING CONDITION!</span>
        <span class="k">if</span> <span class="n">last_output</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">==</span> <span class="n">EOS</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="c1">#return</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">num_batch</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function for evaluation the model.</span>

<span class="sd">    This function take equations, and truncate them up to the equal-sign, and feed</span>
<span class="sd">    them to the model to get the predictions, compare them with the correct answers,</span>
<span class="sd">    and output the accuracy.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">acc</span><span class="p">,</span> <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="n">num_wrong_to_display</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
        <span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">targets</span><span class="p">,</span>
            <span class="n">target_padding_masks</span><span class="p">,</span>
            <span class="n">future_masks</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">=</span> <span class="n">batch</span>  <span class="c1"># construct_batches(batch)</span>
        <span class="k">for</span> <span class="n">equation</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
            <span class="c1"># pprint(equation)</span>
            <span class="c1"># add EOS behind equation</span>
            <span class="n">equation</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">equation</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">EOS</span><span class="p">])),</span> <span class="mi">0</span><span class="p">)</span> <span class="c1"># TODO: PLEASE DO NOT DO THIS - DO NOT MODIFY LIKE THIS.</span>
            <span class="c1"># fmt: off</span>
            <span class="n">loc_equal_sign</span> <span class="o">=</span> <span class="n">equation</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">EQUAL</span><span class="p">)</span>
            <span class="n">loc_EOS</span>        <span class="o">=</span> <span class="n">equation</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">EOS</span><span class="p">)</span>
            <span class="nb">input</span>          <span class="o">=</span> <span class="n">equation</span><span class="p">[</span><span class="mi">0</span> <span class="p">:</span> <span class="n">loc_equal_sign</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
            <span class="n">ans</span>            <span class="o">=</span> <span class="n">equation</span><span class="p">[:</span> <span class="n">loc_EOS</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
            <span class="n">ans_pred</span>       <span class="o">=</span> <span class="n">compute_sum</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
            <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="c1"># fmt: on</span>

            <span class="k">if</span> <span class="n">ans</span> <span class="o">==</span> <span class="n">ans_pred</span><span class="o">.</span><span class="n">tolist</span><span class="p">():</span>
                <span class="n">acc</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">num_wrong_to_display</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s1">&#39;correct equation: </span><span class="si">{</span><span class="n">decode_equation</span><span class="p">(</span><span class="n">vocab</span><span class="o">=</span><span class="n">vocab</span><span class="p">,</span><span class="w"> </span><span class="n">equation</span><span class="o">=</span><span class="n">equation</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;&lt;PAD&gt;&quot;</span><span class="p">,</span><span class="s2">&quot;&quot;</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span>
                    <span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;wrongly predicted as:        </span><span class="si">{</span><span class="n">decode_equation</span><span class="p">(</span><span class="n">vocab</span><span class="o">=</span><span class="n">vocab</span><span class="p">,</span><span class="w"> </span><span class="n">equation</span><span class="o">=</span><span class="n">ans_pred</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="n">num_wrong_to_display</span> <span class="o">-=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">num_batch</span> <span class="ow">and</span> <span class="n">idx</span> <span class="o">&gt;</span> <span class="n">num_batch</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">acc</span> <span class="o">/</span> <span class="n">count</span>


<span class="k">def</span> <span class="nf">what_is</span><span class="p">(</span><span class="n">question</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="s2">&quot;function for computing the sum of two numbers with input in literal string format&quot;</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">compute_sum</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">encode_equation</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">num_digits</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">decode_equation</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">pred</span><span class="p">[</span><span class="n">pred</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:]</span>
    <span class="k">return</span> <span class="n">question</span> <span class="o">+</span> <span class="n">pred</span>
</pre></div>
</div>
</div>
</div>
<p>The provided code implements a form of greedy decoding for sequence generation.
Let’s break down how it aligns with the principles of greedy decoding:</p>
<ol class="arabic simple">
<li><p><strong>Greedy Decoding Principle</strong>: Greedy decoding in sequence generation models
involves choosing the most probable next token at each step of the sequence
generation. This is done iteratively until a stopping condition is met (like
reaching an EOS token or a maximum length).</p></li>
<li><p><strong>Implementation in Your Code</strong>:</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">compute_sum</span></code> function generates a sequence by repeatedly predicting
the next token and appending it to the input.</p></li>
<li><p>For each iteration in <code class="docutils literal notranslate"><span class="pre">compute_sum</span></code>:</p>
<ul>
<li><p>The model (<code class="docutils literal notranslate"><span class="pre">model(x,</span> <span class="pre">pad_mask,</span> <span class="pre">future_mask)</span></code>) generates logits for the
next token based on the current sequence (<code class="docutils literal notranslate"><span class="pre">x</span></code>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">last_output</span> <span class="pre">=</span> <span class="pre">logits.argmax(-1)[:,</span> <span class="pre">-1].view(1,</span> <span class="pre">1)</span></code> picks the most
probable next token (the token with the highest logit value) from the
logits. This is the essence of greedy decoding.</p></li>
<li><p>This token is then appended to the sequence:
<code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">torch.cat((x,</span> <span class="pre">last_output),</span> <span class="pre">1)</span></code>.</p></li>
</ul>
</li>
<li><p>The process continues until the model generates an EOS token, as indicated
by <code class="docutils literal notranslate"><span class="pre">if</span> <span class="pre">last_output.item()</span> <span class="pre">==</span> <span class="pre">EOS:</span> <span class="pre">break</span></code>.</p></li>
</ul>
</li>
<li><p><strong>Evaluation Function</strong>:</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">evaluate</span></code> function further confirms this approach by feeding truncated
sequences (up to the equal sign) from the dataloader to the <code class="docutils literal notranslate"><span class="pre">compute_sum</span></code>
function and comparing the model’s predictions to the correct answers.</p></li>
</ul>
</li>
<li><p><strong>Characteristics of Greedy Decoding</strong>:</p>
<ul class="simple">
<li><p>Greedy decoding is computationally efficient and straightforward but may
not always produce the best possible sequence. It does not reconsider past
decisions; it always picks the most likely next token at each step without
considering the global context of the sequence.</p></li>
</ul>
</li>
</ol>
<p>In summary, the provided code, especially the <code class="docutils literal notranslate"><span class="pre">compute_sum</span></code> function, implements
a typical greedy decoding approach. It iteratively generates a sequence by
choosing the most probable next token at each step, which is characteristic of
greedy decoding in sequence generation tasks.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;training set examples the model gives an incorrect result:&#39;</span><span class="p">)</span>
<span class="c1"># rng = torch.Generator().manual_seed(config.global_config.seed)</span>
<span class="n">seed_all</span><span class="p">(</span><span class="mi">1992</span><span class="p">,</span> <span class="n">seed_torch</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">train_acc</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">train_acc</span><span class="p">)</span> <span class="c1">#</span>
<span class="c1"># print(&#39;validataion set examples the model gives an incorrect result:&#39;)</span>
<span class="n">val_acc</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">val_acc</span><span class="p">)</span>
<span class="c1"># print(&#39;test set examples the model gives an incorrect result:&#39;)</span>
<span class="n">test_acc</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">test_acc</span><span class="p">)</span>
<span class="c1"># result = f&#39;&#39;&#39;train_size: {train_size}, test_acc: {test_acc}, val_acc: {val_acc}, train_acc: {train_acc}</span>
<span class="c1">#                 &#39;&#39;&#39;</span>
<span class="c1"># print(result)</span>
</pre></div>
</div>
</div>
</div>
<p>QUESTION:</p>
<p>another not so smart question of the day: For an input sequence x1,x2,…,x_L, when it forward pass all the way through the decoder model, up till before the pre-logits/head/linear layer, and assuming for simplicity that we squeeze out the first batch dimension (only 1 sample), the the shape of the pre-logits is [L, D] where L is seq len and D the hidden embedding dimension. Am I right to say that the last row of [L, D] being the last token’s representation, holds info of the full context of all previous tokens.</p>
<ol class="arabic simple">
<li><p>This means the last token in the input sequence (the last row in [L, D]) is a function of all previous tokens, so it is not surprising why the tutorial will just use the last row/token’s corresponding prediction as the next predicted token/word, given all previous tokens.</p></li>
</ol>
<blockquote>
<div><p>Important to know the last token or last row of [L, D] is actually a function of all previous tokens, here it is unmasked already.
So if confused, just remember the pre logits last row, corresponding to the last token in the input sequence, is a function of all previous tokens.
It just means that row holds all information, context, of all previous tokens so we can say its conditioned on all previous tokens.</p>
</div></blockquote>
<p>train acc: 0.021484375 , 0.0185546875</p>
<p>non debug</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">correct</span> <span class="n">equation</span><span class="p">:</span> <span class="mi">24</span><span class="o">+</span><span class="mi">86</span><span class="o">=</span><span class="mi">110</span>
<span class="n">predicted</span><span class="p">:</span>        <span class="mi">24</span><span class="o">+</span><span class="mi">86</span><span class="o">=</span><span class="mi">100</span>
<span class="n">correct</span> <span class="n">equation</span><span class="p">:</span> <span class="mi">84</span><span class="o">+</span><span class="mi">26</span><span class="o">=</span><span class="mi">110</span>
<span class="n">predicted</span><span class="p">:</span>        <span class="mi">84</span><span class="o">+</span><span class="mi">26</span><span class="o">=</span><span class="mi">100</span>
<span class="n">validataion</span> <span class="nb">set</span> <span class="n">examples</span> <span class="n">the</span> <span class="n">model</span> <span class="n">gives</span> <span class="n">an</span> <span class="n">incorrect</span> <span class="n">result</span><span class="p">:</span>
<span class="n">test</span> <span class="nb">set</span> <span class="n">examples</span> <span class="n">the</span> <span class="n">model</span> <span class="n">gives</span> <span class="n">an</span> <span class="n">incorrect</span> <span class="n">result</span><span class="p">:</span>
<span class="n">train_size</span><span class="p">:</span> <span class="mi">7000</span><span class="p">,</span> <span class="n">train_loss</span><span class="p">:</span> <span class="mf">0.013642309483007662</span><span class="p">,</span>
                <span class="n">val_loss</span><span class="p">:</span> <span class="mf">0.0008140208410623018</span><span class="p">,</span> <span class="n">test_loss</span><span class="p">:</span> <span class="mf">0.00040599027124699205</span><span class="p">,</span>
                <span class="n">test_acc</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">val_acc</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">train_acc</span><span class="p">:</span> <span class="mf">0.9996448863636364</span>
</pre></div>
</div>
</section>
<section id="multiheadattention">
<h3><a id='toc1_10_5_'></a><a class="reference internal" href="#toc0_"><span class="xref myst">MultiHeadAttention</span></a><a class="headerlink" href="#multiheadattention" title="Link to this heading">#</a></h3>
<p>We start off by understanding the rationale of the following block:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_Q</span><span class="p">(</span><span class="n">query</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span> <span class="c1"># Z @ W_Q -&gt; BxLxD @ DxD = BxLxD</span>
<span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_K</span><span class="p">(</span><span class="n">key</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>   <span class="c1"># Z @ W_K</span>
<span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_V</span><span class="p">(</span><span class="n">value</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span> <span class="c1"># Z @ W_V</span>
</pre></div>
</div>
<section id="a-primer">
<h4><a id='toc1_10_5_1_'></a><a class="reference internal" href="#toc0_"><span class="xref myst">A Primer</span></a><a class="headerlink" href="#a-primer" title="Link to this heading">#</a></h4>
<p>In the context of the Transformer architecture and self-attention mechanism, the
matrices <span class="math notranslate nohighlight">\(\mathbf{W}^{Q}, \mathbf{W}^{K},\)</span> and <span class="math notranslate nohighlight">\(\mathbf{W}^{V}\)</span> are learnable
parameters designed to project the input embeddings <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> into distinct
subspaces tailored for attention calculations. Let’s explore their purpose and
their resulting transformations:</p>
<ol class="arabic">
<li><p><strong>The Role of Weights</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}^{Q}\)</span>: Projects input embeddings into a query subspace,
determining the type of information each token seeks from others.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}^{K}\)</span>: Positions the embeddings in a key subspace, highlighting
the token features that others would search for.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}^{V}\)</span>: Transforms embeddings into a value subspace, showcasing
the actual token content to be aggregated by the attention scores.</p></li>
</ul>
</li>
<li><p><strong>Intuitive &amp; Mathematical Interpretations</strong>:</p>
<ul class="simple">
<li><p><strong>Query Transformation</strong> (<span class="math notranslate nohighlight">\(\mathbf{Z} \mathbf{W}^{Q}\)</span>): Intuitively, it
tailors the raw embeddings to optimally question the rest of the sequence.
Mathematically, it’s a linear transformation of the embedding space into
the query space, akin to a high-dimensional rotation and scaling,
emphasizing aspects relevant to querying.</p></li>
<li><p><strong>Key Transformation</strong> (<span class="math notranslate nohighlight">\(\mathbf{Z} \mathbf{W}^{K}\)</span>): Intuitively, it
accentuates token features that other tokens might seek. Mathematically,
it’s another linear transformation emphasizing aspects that make tokens
searchable.</p></li>
<li><p><strong>Value Transformation</strong> (<span class="math notranslate nohighlight">\(\mathbf{Z} \mathbf{W}^{V}\)</span>): Intuitively, it
prepares tokens to share their intrinsic content when beckoned by the
attention mechanism. Mathematically, it’s a linear transformation
accentuating token content aspects.</p></li>
</ul>
</li>
<li><p><strong>Creating Q, K, V</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{Q} = \mathbf{Z} \mathbf{W}^{Q}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{K} = \mathbf{Z} \mathbf{W}^{K}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{V} = \mathbf{Z} \mathbf{W}^{V}\)</span></p></li>
</ul>
<p>These operations recast the embedded tokens into roles for the attention
mechanism:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{Q}\)</span>: Information seekers. The queries are seeking information, and
the computation <span class="math notranslate nohighlight">\(Q &#64; K^T\)</span> finds how much each part of the input (holder)
should be attended to.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{K}\)</span>: Information gatekeepers. The keys hold the information being
sought, and their arrangement in space defines the subspace that the
queries are projected onto to find these relevance scores.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{V}\)</span>: Information providers. The values contain the content that
needs to be retrieved, and once we have the attention weights, we know how
much of each value to retrieve and combine to form the output.</p></li>
</ul>
<p>Mathematically, the resulting matrices (<span class="math notranslate nohighlight">\(\mathbf{Q}, \mathbf{K}, \mathbf{V}\)</span>)
have rows that represent different aspects (querying, key, value) of the
original tokens.</p>
</li>
<li><p><strong>Relevance to Self-Attention</strong>:</p>
<p>The transformations set the stage for attention score calculations. In this
step, each query vector in <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span> computes its similarity (via dot
product) against all key vectors in <span class="math notranslate nohighlight">\(\mathbf{K}\)</span>. This score matrix reveals
the attention weightage for each token regarding every other token in the
sequence.</p>
<p>Specifically, <span class="math notranslate nohighlight">\(\mathbf{Q} &#64; \mathbf{K}^T\)</span> calculates how each token (query)
aligns with every other token (key). It’s akin to measuring the relevance of
each word to every other word in the sequence.</p>
<p>After normalizing these scores (typically with softmax), we get the attention
weights. These weights guide how the value vectors in <span class="math notranslate nohighlight">\(\mathbf{V}\)</span> are
aggregated. The outcome is a new matrix where each row aggregates
contextually relevant information from the entire sequence. This enriched
output feeds into subsequent transformer layers for further processing.</p>
</li>
</ol>
<p>Overall, by using the <span class="math notranslate nohighlight">\(\mathbf{W}^{Q}, \mathbf{W}^{K},\)</span> and <span class="math notranslate nohighlight">\(\mathbf{W}^{V}\)</span>
matrices, the transformer fine-tunes its focus on inter-token relationships,
enabling the model to capture intricate contextual nuances within a given
sequence.</p>
</section>
</section>
<section id="how-loss-is-computed">
<h3><a id='toc1_10_7_'></a><a class="reference internal" href="#toc0_"><span class="xref myst">How Loss is Computed?</span></a><a class="headerlink" href="#how-loss-is-computed" title="Link to this heading">#</a></h3>
<p>The unreduced loss for the Cross Entropy calculation is given by:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\mathcal{X}, \mathcal{Y}) = \{l_1, \ldots, l_N\}^\top, \quad l_n = -\mathcal{W}_{\mathcal{Y}_n} \cdot \log \left( \frac{\exp(\mathcal{X}_{n, \mathcal{Y}_n})}{\sum_{c=1}^\mathcal{C} \exp(\mathcal{X}_{n, c})} \right) \cdot \mathbb{1}\{\mathcal{Y}_n \neq \text{ignore\_index}\}
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{X}\)</span> is the input tensor of logits, with shape ([B, d_1, \ldots,
d_K, \mathcal{C}]) where <span class="math notranslate nohighlight">\(\mathcal{C}\)</span> is the number of classes and
<span class="math notranslate nohighlight">\([d_1, \ldots, d_K]\)</span> represent any additional dimensions.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> is the target tensor of class indices, with shape ([B, d_1,
\ldots, d_K]).</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{W}\)</span> is a tensor of weights corresponding to class indices.</p></li>
<li><p><span class="math notranslate nohighlight">\(N\)</span> is the product of the batch size and any additional dimensions, i.e.,
<span class="math notranslate nohighlight">\(N = B \times d_1 \times \ldots \times d_K\)</span>. It spans all elements in the
batch and across the additional dimensions, effectively flattening these into
a single dimension for the loss calculation.</p></li>
</ul>
<p>For the reduced loss, the calculation depends on the reduction method (‘mean’ or
‘sum’). The mean reduction averages the loss over all <span class="math notranslate nohighlight">\(N\)</span> elements, while the
sum reduction simply sums over them:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathcal{L}(\mathcal{X}, \mathcal{Y}) =
\begin{cases}
\sum_{n=1}^N \left( \frac{l_n}{\sum_{n=1}^N \mathcal{W}_{\mathcal{Y}_n} \cdot \mathbb{1}\{\mathcal{Y}_n \neq \text{ignore\_index}\}} \right), &amp; \text{if reduction = 'mean'}\\
\sum_{n=1}^N l_n, &amp; \text{if reduction = 'sum'}
\end{cases}
\end{split}\]</div>
<p>This formulation emphasizes that the loss is computed element-wise for each
class index in the target tensor <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>, and then either summed or
averaged depending on the chosen reduction method. The indicator function
<span class="math notranslate nohighlight">\(\mathbb{1}\{\}\)</span> ensures that the ignore_index is not considered in the loss
computation.</p>
<ol class="arabic simple">
<li><p><strong>Define the Loss Function</strong>: The <code class="docutils literal notranslate"><span class="pre">nn.CrossEntropyLoss</span></code> function:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">nn.CrossEntropyLoss</span></code> in PyTorch expects the input logits to be of shape
<code class="docutils literal notranslate"><span class="pre">[N,</span> <span class="pre">C,</span> <span class="pre">d1,</span> <span class="pre">d2,</span> <span class="pre">...,</span> <span class="pre">dK]</span></code> (where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the batch size, <code class="docutils literal notranslate"><span class="pre">C</span></code> is the number of
classes, and <code class="docutils literal notranslate"><span class="pre">d1</span></code> to <code class="docutils literal notranslate"><span class="pre">dK</span></code> are optional additional dimensions) and the target
to be of shape <code class="docutils literal notranslate"><span class="pre">[N,</span> <span class="pre">d1,</span> <span class="pre">d2,</span> <span class="pre">...,</span> <span class="pre">dK]</span></code>.</p></li>
<li><p>Let’s look a simplified example in image classification. The target is a
single integer representing the class label, and the input logits are a
vector of length <code class="docutils literal notranslate"><span class="pre">C</span></code> (the number of classes).</p></li>
</ul>
</li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rng</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">global_config</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span> <span class="c1"># indicating sample 1 is class 1 and sample 2 is class 0</span>
<span class="n">logits</span>  <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">]])</span>
<span class="n">loss</span>   <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here things are simple, because the target is a single integer representing the class label, and the input logits are a vector of length <code class="docutils literal notranslate"><span class="pre">C</span></code> (the number of classes).</p>
<p>The confusion arises when the target is a sequence of integers, as in the case of sequence-to-sequence prediction. In this case, the target is a sequence of integers representing the class labels, and the input logits are a sequence of vectors of length <code class="docutils literal notranslate"><span class="pre">C</span></code> (the number of classes).</p>
<p>Let’s walk through an example for concrete understanding.</p>
<p>Consider the following example:</p>
<ul class="simple">
<li><p>Batch size: 2</p></li>
<li><p>Sequence length: 3</p></li>
<li><p>Number of classes/Vocab size: 4</p></li>
<li><p>Targets is of shape: <code class="docutils literal notranslate"><span class="pre">[B,</span> <span class="pre">L]</span> <span class="pre">=</span> <span class="pre">[2,</span> <span class="pre">3]</span></code></p></li>
<li><p>Logits is of shape: <code class="docutils literal notranslate"><span class="pre">[B,</span> <span class="pre">L,</span> <span class="pre">V]</span> <span class="pre">=</span> <span class="pre">[2,</span> <span class="pre">3,</span> <span class="pre">4]</span></code> where <code class="docutils literal notranslate"><span class="pre">V</span></code> is <code class="docutils literal notranslate"><span class="pre">C</span></code> in the above definition.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># fmt: off</span>
<span class="n">rng</span>        <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">global_config</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>

<span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">V</span>    <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span>                                                   <span class="c1"># Assuming we have B = batch size, L = sequence length, V = vocab size</span>

<span class="n">logits</span>     <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>                       <span class="c1"># logits from the head</span>
<span class="n">targets</span>    <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">V</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>  <span class="c1"># targets are the labels</span>
<span class="c1"># fmt: on</span>

<span class="n">pprint</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># logits for the first sequence [L=10, V=18]</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">targets</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># target for the first sequence [L=10]</span>
</pre></div>
</div>
</div>
</div>
<p>We establish some conceptual understanding first:</p>
<ul class="simple">
<li><p>Each sample in the batch has the following characteristics:</p>
<ul>
<li><p>Denote <code class="docutils literal notranslate"><span class="pre">target</span></code> and <code class="docutils literal notranslate"><span class="pre">logit</span></code> as the target and logits for a particular sample in the batch.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">target</span></code> is of shape <code class="docutils literal notranslate"><span class="pre">[L]</span> <span class="pre">=</span> <span class="pre">[3]</span></code> and each element is the class/vocab label for each token in the sequence.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">logit</span></code> is of shape <code class="docutils literal notranslate"><span class="pre">[L,</span> <span class="pre">V]</span> <span class="pre">=</span> <span class="pre">[3,</span> <span class="pre">4]</span></code> and each row is the logits for each token in the sequence.</p></li>
<li><p>Therefore, we want to compare each row in <code class="docutils literal notranslate"><span class="pre">logit</span></code> with each element in <code class="docutils literal notranslate"><span class="pre">target</span></code> to compute the loss.</p></li>
<li><p>We can think of each row in <code class="docutils literal notranslate"><span class="pre">logit</span></code> as the prediction for each token in the sequence, and each element in <code class="docutils literal notranslate"><span class="pre">target</span></code> as the ground truth for each token in the sequence.</p></li>
<li><p>Intuitively this means that within each sample, there are many “sub-samples” where each sub-sample is a token in the sequence. If you can visualize this, then there should be no confusion.</p></li>
</ul>
</li>
<li><p>In code, we can do so with the following manner:</p>
<ul>
<li><p>Calculate loss for each token in each sample individually and then sum them up.</p></li>
<li><p>Reduction by mean will mean we need to divide our <code class="docutils literal notranslate"><span class="pre">total_loss</span></code> by the total number
of samples in the batch. But remember that even though technically we have
2 samples in the batch, we are actually treating each token in each sample
as a sub-sample, so the total samples is <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">*</span> <span class="pre">L</span></code> where <code class="docutils literal notranslate"><span class="pre">B</span></code> is the batch size
and <code class="docutils literal notranslate"><span class="pre">L</span></code> is the sequence length.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">criterion</span>  <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)</span>

<span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">L</span><span class="p">):</span>
        <span class="n">logit</span>      <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">target</span>     <span class="o">=</span> <span class="n">targets</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">logit</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="n">pprint</span><span class="p">(</span><span class="n">total_loss</span><span class="p">)</span>
<span class="n">total_loss</span>  <span class="o">=</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="p">(</span><span class="n">B</span> <span class="o">*</span> <span class="n">L</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">total_loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>In PyTorch however, if you have a logits tensor of shape <code class="docutils literal notranslate"><span class="pre">[B,</span> <span class="pre">S,</span> <span class="pre">V]</span></code>, you need to permute it to
<code class="docutils literal notranslate"><span class="pre">[B,</span> <span class="pre">V,</span> <span class="pre">S]</span></code> to align with the format that <code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code> wants, so that <code class="docutils literal notranslate"><span class="pre">V</span></code> (vocab size) is
treated as <code class="docutils literal notranslate"><span class="pre">C</span></code> (number of classes), and <code class="docutils literal notranslate"><span class="pre">S</span></code> (sequence length) is treated as
one of the additional dimensions <code class="docutils literal notranslate"><span class="pre">d1,</span> <span class="pre">d2,</span> <span class="pre">...,</span> <span class="pre">dK</span></code>.</p>
<p>But all in all, if you understood the previous loop to calculate the loss for each token in each sample individually and then sum them up, then dividing to fulfill reduction of mean, then you should be fine.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Permute logits to shape [B, V, S]</span>
<span class="n">logits_permuted</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Instantiate the CrossEntropyLoss</span>
<span class="c1"># By default, it reduces by averaging the losses over each observation in the input</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">logits_permuted</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="masking-and-ignore-index">
<h4><a class="toc-backref" href="#id31" role="doc-backlink">Masking and Ignore Index</a><a class="headerlink" href="#masking-and-ignore-index" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># fmt: off</span>
<span class="n">rng</span>        <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">global_config</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>

<span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">V</span>    <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span>                                                   <span class="c1"># Assuming we have B = batch size, L = sequence length, V = vocab size</span>

<span class="n">logits</span>     <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>                       <span class="c1"># logits from the head</span>
<span class="n">targets</span>    <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">V</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>  <span class="c1"># targets are the labels</span>
<span class="c1"># fmt: on</span>

<span class="n">pprint</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># logits for the first sequence [L=10, V=18]</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">targets</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># target for the first sequence [L=10]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">targets</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">123</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">targets</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">PAD_</span> <span class="o">=</span> <span class="o">-</span><span class="mi">123</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">criterion</span>  <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="n">PAD_</span><span class="p">)</span>

<span class="n">NON_IGNORE_COUNT</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">L</span><span class="p">):</span>
        <span class="n">logit</span>      <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">target</span>     <span class="o">=</span> <span class="n">targets</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">target</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">PAD_</span><span class="p">]):</span>
            <span class="k">continue</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">logit</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">NON_IGNORE_COUNT</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="n">pprint</span><span class="p">(</span><span class="n">total_loss</span><span class="p">)</span>
<span class="n">total_loss</span>  <span class="o">=</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="n">NON_IGNORE_COUNT</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">total_loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>NOTE: <code class="docutils literal notranslate"><span class="pre">NON_IGNORE_COUNT</span></code> is used instead of <code class="docutils literal notranslate"><span class="pre">BxL</span></code>, why? Cause we are averaging over
all non-ignored guys!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Permute logits to shape [B, V, S]</span>
<span class="n">logits_permuted</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Instantiate the CrossEntropyLoss</span>
<span class="c1"># By default, it reduces by averaging the losses over each observation in the input</span>
<span class="n">criterion</span>  <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="n">PAD_</span><span class="p">)</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">logits_permuted</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="training-with-gpt-like-model">
<h2><a id='toc1_12_'></a><a class="reference internal" href="#toc0_"><span class="xref myst">Training with GPT-like Model</span></a><a class="headerlink" href="#training-with-gpt-like-model" title="Link to this heading">#</a></h2>
<p>If you’re working with a GPT-like model, which is a decoder-only architecture, the training mechanics differ slightly compared to the encoder-decoder models like seq2seq. In a GPT-style model, the entire sequence (input and output) is provided to the model at once, and each token is predicted based on the tokens that came before it. The model is still autoregressive, but there’s no separate encoder to produce an intermediate representation; the “encoding” is effectively built into the ongoing autoregressive decoding process.</p>
<p>In your case, if the equations are like <code class="docutils literal notranslate"><span class="pre">90+38=128</span></code>, during training you’d provide <code class="docutils literal notranslate"><span class="pre">90+38=</span></code> as the input and then use the remaining part <code class="docutils literal notranslate"><span class="pre">128</span></code> as the expected output, potentially along with special tokens to demarcate sequence boundaries or to flag the equation/result parts. However, unlike an encoder-decoder model where the decoder gets to “peek” at the correct output during training (also known as “teacher forcing”), here every token in the output is predicted one by one, based solely on the preceding tokens.</p>
<p>In such a setup, you can definitely feed the entire equation to the model and try to predict each subsequent token based on the preceding tokens. For example, given <code class="docutils literal notranslate"><span class="pre">90+38=</span></code>, the model should predict <code class="docutils literal notranslate"><span class="pre">1</span></code>, <code class="docutils literal notranslate"><span class="pre">2</span></code>, <code class="docutils literal notranslate"><span class="pre">8</span></code> in succession.</p>
<section id="loss-computation">
<h3><a id='toc1_12_1_'></a><a class="reference internal" href="#toc0_"><span class="xref myst">Loss Computation</span></a><a class="headerlink" href="#loss-computation" title="Link to this heading">#</a></h3>
<p>For training a GPT-like model, you’d usually use a standard loss function like cross-entropy loss for each token’s prediction. You’d compare the token predicted by the model to the actual token in the target sequence to compute the loss. This is calculated for each token and then averaged over the sequence or batch, depending on your implementation.</p>
</section>
<section id="example">
<h3><a id='toc1_12_2_'></a><a class="reference internal" href="#toc0_"><span class="xref myst">Example</span></a><a class="headerlink" href="#example" title="Link to this heading">#</a></h3>
<p>In a GPT-like model, each token in the sequence is used to predict the next token. The model takes a sequence of tokens and produces a new sequence of the same length where each new token is predicted based on all the preceding tokens in the input sequence. The loss is then computed between the predicted sequence and the target sequence.</p>
<p>Let’s take a closer look at an example:</p>
<ul class="simple">
<li><p>The original tensor: <code class="docutils literal notranslate"><span class="pre">[15,</span> <span class="pre">9,</span> <span class="pre">0,</span> <span class="pre">10,</span> <span class="pre">3,</span> <span class="pre">8,</span> <span class="pre">13,</span> <span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">8,</span> <span class="pre">14]</span></code> which corresponds to <code class="docutils literal notranslate"><span class="pre">&lt;SOS&gt;90+38=128&lt;EOS&gt;</span></code></p></li>
<li><p>Input tensor:  <code class="docutils literal notranslate"><span class="pre">[15,</span> <span class="pre">9,</span>&#160; <span class="pre">0,</span>&#160; <span class="pre">10,</span> <span class="pre">3,</span>&#160; <span class="pre">8,</span>&#160; <span class="pre">13,</span> <span class="pre">1,</span>&#160; <span class="pre">2,</span> <span class="pre">8]</span></code>, which corresponds to <code class="docutils literal notranslate"><span class="pre">&lt;SOS&gt;90+38=128</span></code> without <code class="docutils literal notranslate"><span class="pre">EOS</span></code></p></li>
<li><p>Target tensor:     <code class="docutils literal notranslate"><span class="pre">[9,</span>&#160; <span class="pre">0,</span>&#160; <span class="pre">10,</span> <span class="pre">3,</span>&#160; <span class="pre">8,</span>&#160; <span class="pre">13,</span> <span class="pre">1,</span>&#160; <span class="pre">2,</span>&#160; <span class="pre">8,</span> <span class="pre">14]</span></code>
<code class="docutils literal notranslate"><span class="pre">[16,</span> <span class="pre">16,</span> <span class="pre">16,</span> <span class="pre">16,</span> <span class="pre">16,</span> <span class="pre">16,</span> <span class="pre">1,</span>&#160; <span class="pre">2,</span>&#160; <span class="pre">8,</span> <span class="pre">14]</span></code></p></li>
</ul>
<p>During training:</p>
<ol class="arabic simple">
<li><p><strong>First Timestep</strong>: The model takes <code class="docutils literal notranslate"><span class="pre">[15]</span></code> (or <code class="docutils literal notranslate"><span class="pre">[&lt;BOS&gt;]</span></code> if 15 is your BOS token) and tries to predict the next token. Ideally, it should predict <code class="docutils literal notranslate"><span class="pre">9</span></code>. But here, your target sequence starts with masked tokens (<code class="docutils literal notranslate"><span class="pre">16</span></code>, if 16 is your masking token). So the loss is computed between the predicted token and the masked token <code class="docutils literal notranslate"><span class="pre">16</span></code>. But since <code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code> has an <code class="docutils literal notranslate"><span class="pre">ignore_index</span></code> (now you know what they are right!), you can set it to say <code class="docutils literal notranslate"><span class="pre">16</span></code> or (default <code class="docutils literal notranslate"><span class="pre">-1</span></code> but you would need to change padding number) and tell the model that whenever the ground truth is <code class="docutils literal notranslate"><span class="pre">16</span></code>, the loss
is zeroed out so it is not counted? This allows the model to focus on learning from the relevant parts of the sequence while ignoring the masked portions.</p></li>
<li><p><strong>Second Timestep</strong>: The model takes <code class="docutils literal notranslate"><span class="pre">[15,</span> <span class="pre">9]</span></code> and predicts the next token, which should be <code class="docutils literal notranslate"><span class="pre">0</span></code>. Again, the target is a masked token <code class="docutils literal notranslate"><span class="pre">16</span></code>.</p></li>
<li><p><strong>…</strong></p></li>
<li><p><strong>Eighth Timestep</strong>: The model takes <code class="docutils literal notranslate"><span class="pre">[15,</span> <span class="pre">9,</span>&#160; <span class="pre">0,</span>&#160; <span class="pre">10,</span> <span class="pre">3,</span>&#160; <span class="pre">8,</span>&#160; <span class="pre">13]</span></code> (which is <code class="docutils literal notranslate"><span class="pre">90+38=</span></code>) and predicts the next token. Now the target is <code class="docutils literal notranslate"><span class="pre">1</span></code>, so the loss is computed between the predicted token and <code class="docutils literal notranslate"><span class="pre">1</span></code>. There is no mask anymore here, so the loss will be computed.</p></li>
<li><p><strong>Ninth Timestep</strong>: The model takes <code class="docutils literal notranslate"><span class="pre">[15,</span> <span class="pre">9,</span>&#160; <span class="pre">0,</span>&#160; <span class="pre">10,</span> <span class="pre">3,</span>&#160; <span class="pre">8,</span>&#160; <span class="pre">13,</span> <span class="pre">1]</span></code> (which is <code class="docutils literal notranslate"><span class="pre">90+38=1</span></code>) and predicts the next token. Now the target is <code class="docutils literal notranslate"><span class="pre">2</span></code>, so the loss is computed between the predicted token and <code class="docutils literal notranslate"><span class="pre">2</span></code>.</p>
<ol class="arabic simple">
<li><p>Here’s an important thing for beginners (me), In a typical GPT-like architecture used for sequence-to-sequence tasks like this one, the model doesn’t use its own predictions as input during training. Instead, it uses the original, ground-truth input sequence. This is known as “teacher forcing.” In teacher forcing, even if the model predicts a wrong token at some timestep, it doesn’t affect the input sequence for subsequent timesteps. The model continues to get the original input sequence for the entire training epoch.</p></li>
<li><p>So if model predicts a <code class="docutils literal notranslate"><span class="pre">3</span></code> during the eighth timestep, where the ground trut is <code class="docutils literal notranslate"><span class="pre">1</span></code>, the model would simply incur a higher loss for that prediction. However, the input for the ninth timestep would still be the ground truth sequence up to that point, regardless of what the model predicted at the eighth timestep.</p></li>
<li><p>But it is noted that this behaviour is still autoregressive.</p></li>
</ol>
</li>
<li><p><strong>Tenth</strong>: The model takes <code class="docutils literal notranslate"><span class="pre">[15,</span> <span class="pre">9,</span>&#160; <span class="pre">0,</span>&#160; <span class="pre">10,</span> <span class="pre">3,</span>&#160; <span class="pre">8,</span>&#160; <span class="pre">13,</span> <span class="pre">1,</span> <span class="pre">2]</span></code> and predicts the next token which is <code class="docutils literal notranslate"><span class="pre">8</span></code>.</p></li>
<li><p><strong>Last</strong>: The model takes <code class="docutils literal notranslate"><span class="pre">[15,</span> <span class="pre">9,</span>&#160; <span class="pre">0,</span>&#160; <span class="pre">10,</span> <span class="pre">3,</span>&#160; <span class="pre">8,</span>&#160; <span class="pre">13,</span> <span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">8]</span></code> and predicts the next token which is <code class="docutils literal notranslate"><span class="pre">14</span></code> the <code class="docutils literal notranslate"><span class="pre">EOS</span></code>.</p>
<ol class="arabic simple">
<li><p>The reason you need to predict <code class="docutils literal notranslate"><span class="pre">EOS</span></code> is simple intuitively, consider the case where there’s no need for <code class="docutils literal notranslate"><span class="pre">EOS</span></code>, then the model will not know when to stop.</p></li>
</ol>
</li>
</ol>
<p>This goes on until the entire sequence is processed. Note that the model never actually “sees” the target tokens during the prediction. It is solely relying on the tokens that came before the current token in the input sequence. After the model makes its prediction, then the predicted tokens are compared to the target tokens to compute the loss, which is then backpropagated to update the model weights.</p>
</section>
<section id="confusion-training-versus-inference">
<h3><a id='toc1_12_3_'></a><a class="reference internal" href="#toc0_"><span class="xref myst">Confusion: Training versus Inference</span></a><a class="headerlink" href="#confusion-training-versus-inference" title="Link to this heading">#</a></h3>
<p>The statement “it generates one token at a time and uses its own previously generated tokens as context for generating subsequent tokens” is generally true for GPT-like models during the inference stage, not during training. During inference (or generation), the model does indeed use its own previously generated tokens to produce the next token, since there is no ground truth sequence to rely on. In that case, if the model makes an incorrect prediction at a certain timestep, that incorrect token is used as part of the context for the following timestep.</p>
<p>During training, however, the model typically uses the ground truth tokens for the preceding sequence as context for predicting each next token, as described in your example. This resembles teacher forcing, in that the ground truth, rather than the model’s own predictions, is used to guide training.</p>
<p>So there’s no contradiction, but the behavior is context-dependent:</p>
<ul class="simple">
<li><p>During training, the ground truth sequence is used for context.</p></li>
<li><p>During inference, the model’s own previously generated tokens are used for context.</p></li>
</ul>
<p>Both approaches are consistent with the autoregressive nature of the model: in both cases, the token at each position is generated based on the tokens at all previous positions. The difference lies in whether those preceding tokens come from the ground truth (during training) or from the model’s own previous outputs (during inference).</p>
</section>
<section id="training-vs-inference">
<h3><a id='toc1_12_4_'></a><a class="reference internal" href="#toc0_"><span class="xref myst">Training vs Inference</span></a><a class="headerlink" href="#training-vs-inference" title="Link to this heading">#</a></h3>
<p>In an autoregressive model like a Transformer decoder, the concept of “learning
the representation of the sequence as it goes” does not refer to the model
processing one token at a time during actual forward passes. Instead, it refers
to the model’s ability to generate or predict one token at a time during
inference, while training on a full sequence in a batched manner.</p>
<p>During training:</p>
<ul class="simple">
<li><p>All tokens are processed in parallel for efficiency. This is possible because
the entire sequence is known beforehand (it’s the training data).</p></li>
<li><p>The “autoregressive” property is enforced by using masks in the self-attention
mechanism. This masking ensures that the prediction for each token can only
depend on previously generated tokens, not on future tokens which the model
has no access to during inference. This is how the model learns the
conditional probability distribution of each token given the previous tokens,
despite the parallel processing of tokens.</p></li>
</ul>
<p>During inference:</p>
<ul class="simple">
<li><p>The model starts with an initial token (such as a start-of-sequence token) and
generates the next token based on this single input.</p></li>
<li><p>Then, the model uses both the initial token and the newly generated token to
predict the third token, and so on.</p></li>
<li><p>This process is sequential and each new token is predicted based on the
previously generated tokens, creating a sequence one token at a time.</p></li>
</ul>
<p>So, when we say that the model learns the representation of the sequence as it
goes, we mean that the model is trained to handle sequences in such a way that
it can generate them one piece at a time, respecting the causal order inherent
to the task (e.g., language modeling). The parallel processing during training
does not contradict the autoregressive nature of the model; it is simply a
computational efficiency that is enabled by knowing the full sequence in
advance.</p>
</section>
</section>
<section id="questions">
<h2><a class="toc-backref" href="#id37" role="doc-backlink">Questions</a><a class="headerlink" href="#questions" title="Link to this heading">#</a></h2>
<section id="why-masked-0-in-some">
<h3><a class="toc-backref" href="#id38" role="doc-backlink">Why Masked == 0 in some?</a><a class="headerlink" href="#why-masked-0-in-some" title="Link to this heading">#</a></h3>
<p>The use of <code class="docutils literal notranslate"><span class="pre">mask</span> <span class="pre">==</span> <span class="pre">0</span></code> in the <code class="docutils literal notranslate"><span class="pre">masked_fill</span></code> operation is a result of how the
mask is constructed. Essentially, different implementations may represent masks
differently:</p>
<ol class="arabic">
<li><p><strong>Boolean Masking with True/False</strong>: In some implementations, the mask might
be a Boolean tensor where <code class="docutils literal notranslate"><span class="pre">True</span></code> denotes the positions to mask (set to
negative infinity) and <code class="docutils literal notranslate"><span class="pre">False</span></code> for the positions to keep. In such cases, you
can directly use the mask in <code class="docutils literal notranslate"><span class="pre">masked_fill</span></code> as in your provided code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">attention_scores</span> <span class="o">=</span> <span class="n">attention_scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">))</span>
</pre></div>
</div>
<p>Here, if <code class="docutils literal notranslate"><span class="pre">mask[i][j]</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, <code class="docutils literal notranslate"><span class="pre">attention_scores[i][j]</span></code> would be set to
<code class="docutils literal notranslate"><span class="pre">-inf</span></code>.</p>
</li>
<li><p><strong>Integer Masking with 1/0</strong>: In other implementations, the mask might be an
integer tensor where <code class="docutils literal notranslate"><span class="pre">1</span></code> denotes the positions to keep and <code class="docutils literal notranslate"><span class="pre">0</span></code> denotes the
positions to mask. In such cases, you’ll often find the mask is inverted
(<code class="docutils literal notranslate"><span class="pre">mask</span> <span class="pre">==</span> <span class="pre">0</span></code>) before using <code class="docutils literal notranslate"><span class="pre">masked_fill</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">attention_scores</span> <span class="o">=</span> <span class="n">attention_scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">))</span>
</pre></div>
</div>
<p>Here, if <code class="docutils literal notranslate"><span class="pre">mask[i][j]</span></code> is <code class="docutils literal notranslate"><span class="pre">0</span></code>, <code class="docutils literal notranslate"><span class="pre">attention_scores[i][j]</span></code> would be set to
<code class="docutils literal notranslate"><span class="pre">-inf</span></code>.</p>
</li>
</ol>
<p>The core functionality—masking certain positions in the attention scores—is the
same in both cases. The difference lies in how the mask tensor is constructed
and interpreted. So, if you find an implementation using <code class="docutils literal notranslate"><span class="pre">mask</span> <span class="pre">==</span> <span class="pre">0</span></code>, it’s
likely using an integer mask where <code class="docutils literal notranslate"><span class="pre">0</span></code> signifies positions to mask, whereas if
it’s directly using <code class="docutils literal notranslate"><span class="pre">mask</span></code>, it’s probably a Boolean mask where <code class="docutils literal notranslate"><span class="pre">True</span></code> signifies
positions to mask.</p>
</section>
<section id="what-is-the-reason-of-setting-the-attention-scores-s-mask-indexes-to-negative-infinity">
<h3><a class="toc-backref" href="#id39" role="doc-backlink">what is the reason of setting the attention scores’s mask indexes to negative infinity</a><a class="headerlink" href="#what-is-the-reason-of-setting-the-attention-scores-s-mask-indexes-to-negative-infinity" title="Link to this heading">#</a></h3>
<p>In the attention mechanism, particularly in the Scaled Dot-Product Attention,
attention scores are computed for each query-key pair and then passed through a
softmax function to obtain attention weights. These weights are used to take a
weighted sum of the value vectors, resulting in the final output or the context
vectors. The purpose of the mask is to prevent certain tokens (like padding
tokens) from being attended to.</p>
<p>The reason for setting masked attention scores to negative infinity (<code class="docutils literal notranslate"><span class="pre">-inf</span></code>)
lies in the properties of the softmax function:</p>
<ol class="arabic simple">
<li><p><strong>Softmax Behavior</strong>: The softmax function transforms its input (the
attention scores in this case) into a probability distribution.
Mathematically, the softmax function for a given vector <span class="math notranslate nohighlight">\(x\)</span> is defined as:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\text{Softmax}(x)_i = \frac{e^{x_i}}{\sum_{j=1}^{N} e^{x_j}}
\]</div>
<ol class="arabic simple" start="2">
<li><p><strong>Impact of Negative Infinity</strong>: When you pass negative infinity through the
softmax function, <span class="math notranslate nohighlight">\(e^{-\infty}\)</span> approaches zero. As a result, the masked
positions get a near-zero weight in the attention mechanism.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\text{Softmax}(-\infty) = \frac{e^{-\infty}}{\sum_{j=1}^{N} e^{x_j}} \approx 0
\]</div>
<ol class="arabic simple" start="3">
<li><p><strong>Avoiding Unwanted Attention</strong>: The point of setting these specific
positions to <code class="docutils literal notranslate"><span class="pre">-inf</span></code> is to ensure that when softmax is applied, these
positions get zero attention weights. This is a way of making sure that the
model does not attend to the positions we’ve masked (like padding tokens or
future tokens in the sequence, depending on the mask).</p></li>
</ol>
<p>In summary, setting the masked attention scores to <code class="docutils literal notranslate"><span class="pre">-inf</span></code> and then passing them
through a softmax effectively nullifies the contribution of the masked positions
in the resulting attention-weighted sum of the value vectors. This is a commonly
used trick to impose a certain structure (like masking out future information in
the decoder) or to handle variable-length sequences with padding.</p>
</section>
<section id="why-do-we-need-both-ignore-index-in-loss-and-also-negative-infinity-mask">
<h3><a id='toc1_13_3_'></a><a class="reference internal" href="#toc0_"><span class="xref myst">Why do we need both ignore index in Loss and also negative infinity mask</span></a><a class="headerlink" href="#why-do-we-need-both-ignore-index-in-loss-and-also-negative-infinity-mask" title="Link to this heading">#</a></h3>
<p>Using an “ignore index” in the <code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code> function in PyTorch can ignore
the effect of certain tokens (like padding tokens) during the loss computation.
However, the purpose of the mask in the attention mechanism and the “ignore
index” in the loss function serve different roles in the model, and they operate
at different stages of the computational graph.</p>
<ol class="arabic simple">
<li><p><strong>Ignore Index in Loss Function</strong>: The “ignore index” in the loss function
ensures that the model’s output at certain positions (typically corresponding
to padding tokens) does not contribute to the loss. This happens at the very
end of the forward pass, just before backpropagation begins.</p></li>
<li><p><strong>Mask in Attention Mechanism</strong>: The mask in the attention mechanism, on the
other hand, operates during the forward pass at the time when attention
scores are computed. This is a more “internal” operation and ensures that
certain positions do not contribute to the output at all, not just during the
loss computation but actually in the intermediate representations (i.e.,
context vectors) that the model computes.</p></li>
</ol>
<p>To put it another way, even if you’re ignoring certain tokens in your loss
calculation, those tokens can still influence the model’s output unless they’re
masked out in the attention mechanism itself.</p>
<p>For example, consider a decoder in a sequence-to-sequence model:</p>
<ul class="simple">
<li><p>If you don’t use a mask in the attention mechanism, future tokens could
influence the output at the current timestep, which is not desirable.</p></li>
<li><p>Even if you use an “ignore index” in your loss function, it doesn’t prevent
the model from “cheating” by peeking at the future tokens if they are not
masked in the attention mechanism.</p></li>
</ul>
<p>So in summary, using an “ignore index” in <code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code> is not a
replacement for using attention masks. Both have specific roles in the model,
and they are often used together to ensure both that the model attends to the
right tokens and that it is trained properly.</p>
</section>
<section id="target-and-preds-logits-shape">
<h3><a id='toc1_13_4_'></a><a class="reference internal" href="#toc0_"><span class="xref myst">Target and Preds/Logits Shape</span></a><a class="headerlink" href="#target-and-preds-logits-shape" title="Link to this heading">#</a></h3>
<p>The target tensor for the cross-entropy loss function should typically have a
shape of <code class="docutils literal notranslate"><span class="pre">[batch_size,</span> <span class="pre">sequence_length]</span></code> where each entry in the tensor is an
integer representing the index of the true class (i.e., the actual word/token
from the vocabulary) for that position in the sequence. Here <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> refers
to the number of sequences in each batch, and <code class="docutils literal notranslate"><span class="pre">sequence_length</span></code> is the length of
each sequence.</p>
<p>Let’s break it down step-by-step:</p>
<ol class="arabic simple">
<li><p><strong>Last Linear Layer of Decoder</strong>: When you say that the last linear layer of
your decoder has shape <code class="docutils literal notranslate"><span class="pre">[bs,</span> <span class="pre">vocab_size]</span></code>, it means that for each example in
the batch, you’re outputting a distribution over the vocabulary. The values
can be logit scores that represent the likelihood of each word in your
vocabulary being the next word in the sequence.</p></li>
<li><p><strong>Target Shape</strong>: In comparison, your target tensor should contain the actual
words (as integers) that appear at each position in your sequence for each
example in the batch. The target tensor does not need to have a <code class="docutils literal notranslate"><span class="pre">vocab_size</span></code>
dimension because it is not a distribution; it contains the indices of the
actual next words. Thus, it should have a shape <code class="docutils literal notranslate"><span class="pre">[bs,</span> <span class="pre">sequence_length]</span></code>.</p></li>
<li><p><strong>Cross-Entropy Loss</strong>: When using the cross-entropy loss, the logits (i.e.,
the output from your linear layer) should have a shape
<code class="docutils literal notranslate"><span class="pre">[bs,</span> <span class="pre">sequence_length,</span> <span class="pre">vocab_size]</span></code>, while the target should have a shape
<code class="docutils literal notranslate"><span class="pre">[bs,</span> <span class="pre">sequence_length]</span></code>. The cross-entropy loss function will internally
apply a softmax to the logits, and then compute the log-likelihood between
the predicted distribution and the target class.</p></li>
</ol>
<p>To sum up, if your decoder’s last linear layer has shape <code class="docutils literal notranslate"><span class="pre">[bs,</span> <span class="pre">vocab_size]</span></code> for
each time step, make sure that your target tensor has the shape
<code class="docutils literal notranslate"><span class="pre">[bs,</span> <span class="pre">sequence_length]</span></code>, and your logits should be
<code class="docutils literal notranslate"><span class="pre">[bs,</span> <span class="pre">sequence_length,</span> <span class="pre">vocab_size]</span></code> when you feed them into the cross-entropy
loss function.</p>
</section>
<section id="why-do-we-flatten-prediction-and-target-logits">
<h3><a id='toc1_13_5_'></a><a class="reference internal" href="#toc0_"><span class="xref myst">Why do we flatten prediction and target (logits)?</span></a><a class="headerlink" href="#why-do-we-flatten-prediction-and-target-logits" title="Link to this heading">#</a></h3>
<p>Flattening both the predicted logits and the target labels serves a specific
purpose when using the cross-entropy loss function for sequence data. Let’s dig
into each component to understand why this is done:</p>
<section id="background">
<h4><a id='toc1_13_5_1_'></a><a class="reference internal" href="#toc0_"><span class="xref myst">Background</span></a><a class="headerlink" href="#background" title="Link to this heading">#</a></h4>
<ol class="arabic">
<li><p><strong>Logits Tensor</strong>: In a sequence-to-sequence model, you usually generate a
sequence of logits for each item in your batch. The logits for each position
in the sequence form a vector of size <code class="docutils literal notranslate"><span class="pre">vocab_size</span></code>, which gives you a
probability distribution across all possible tokens.</p>
<p>Shape: <code class="docutils literal notranslate"><span class="pre">[batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">vocab_size]</span></code></p>
</li>
<li><p><strong>Targets Tensor</strong>: Your ground truth data, the <code class="docutils literal notranslate"><span class="pre">targets</span></code>, are integers
representing the correct class labels (or tokens) at each sequence position.</p>
<p>Shape: <code class="docutils literal notranslate"><span class="pre">[batch_size,</span> <span class="pre">sequence_length]</span></code></p>
</li>
</ol>
</section>
<section id="traditional-loss-computation">
<h4><a id='toc1_13_5_2_'></a><a class="reference internal" href="#toc0_"><span class="xref myst">Traditional Loss Computation</span></a><a class="headerlink" href="#traditional-loss-computation" title="Link to this heading">#</a></h4>
<p>Typically, the cross-entropy loss between predicted probabilities and target
labels for one data point is computed, and then you average over all data
points. In sequence-to-sequence models, you can think of each position in the
sequence as a separate data point.</p>
</section>
<section id="why-flatten">
<h4><a id='toc1_13_5_3_'></a><a class="reference internal" href="#toc0_"><span class="xref myst">Why Flatten?</span></a><a class="headerlink" href="#why-flatten" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><strong>Batch and Sequence Unification</strong>: The idea of flattening both logits and
targets is to treat each <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">sequence_position)</span></code> pair as an independent
data point. Instead of having a batch of sequences, you have a “flattened”
batch of tokens. This simplifies the application of the loss function by
converting the 3D logits tensor and 2D targets tensor into 2D and 1D tensors,
respectively.</p></li>
<li><p><strong>Efficiency</strong>: Loss computations often benefit from vectorization for
computational efficiency. By flattening the tensors, you enable a more
efficient matrix operation, which is generally faster than using nested loops
over each sequence and batch.</p></li>
<li><p><strong>Alignment</strong>: The key is to ensure that each row in the flattened logits
corresponds to the same position in the flattened targets. This alignment is
crucial for the correct computation of the loss.</p></li>
</ol>
</section>
<section id="step-by-step-flattening">
<h4><a id='toc1_13_5_4_'></a><a class="reference internal" href="#toc0_"><span class="xref myst">Step-by-step Flattening</span></a><a class="headerlink" href="#step-by-step-flattening" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><strong>Logits Flattening</strong>: <code class="docutils literal notranslate"><span class="pre">logits.view(-1,</span> <span class="pre">logits.size(-1))</span></code> will take the 3D
tensor <code class="docutils literal notranslate"><span class="pre">[batch_size,</span> <span class="pre">seq_length,</span> <span class="pre">vocab_size]</span></code> and reshape it into a 2D tensor
of shape <code class="docutils literal notranslate"><span class="pre">[batch_size</span> <span class="pre">*</span> <span class="pre">seq_length,</span> <span class="pre">vocab_size]</span></code>.</p></li>
<li><p><strong>Targets Flattening</strong>: <code class="docutils literal notranslate"><span class="pre">targets.view(-1)</span></code> will take the 2D tensor
<code class="docutils literal notranslate"><span class="pre">[batch_size,</span> <span class="pre">seq_length]</span></code> and convert it into a 1D tensor of shape
<code class="docutils literal notranslate"><span class="pre">[batch_size</span> <span class="pre">*</span> <span class="pre">seq_length]</span></code>.</p></li>
<li><p><strong>Loss Calculation</strong>: Both flattened tensors are then used in the
cross-entropy loss function. The loss between each row in the flattened
logits and the corresponding element in the flattened targets is computed.</p></li>
</ol>
<p>By flattening the tensors this way, you maintain the correspondence between each
logit and its corresponding target, enabling you to correctly compute the loss
for each token across all sequences and batches.</p>
</section>
</section>
<section id="why-sometimes-unsqueeze-masks">
<h3><a id='toc1_13_6_'></a><a class="reference internal" href="#toc0_"><span class="xref myst">Why sometimes unsqueeze masks?</span></a><a class="headerlink" href="#why-sometimes-unsqueeze-masks" title="Link to this heading">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">unsqueeze</span></code> operation is used to add an additional dimension to the tensor.
In attention mechanisms, particularly the scaled dot-product attention used in
models like the Transformer, the masks usually need to have the same number of
dimensions as the attention logits for proper broadcasting.</p>
<p>For instance, let’s say your source tensor (<code class="docutils literal notranslate"><span class="pre">src</span></code>) has a shape of <span class="math notranslate nohighlight">\(B \times L\)</span>
where <span class="math notranslate nohighlight">\(B\)</span> is the batch size and <span class="math notranslate nohighlight">\(L\)</span> is the sequence length. The attention logit
tensor resulting from the query-key dot product would then have shape
<span class="math notranslate nohighlight">\(B \times N \times L \times L\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> is the number of attention heads.</p>
<p>The mask needs to align with the <span class="math notranslate nohighlight">\(L \times L\)</span> dimensions of this 4D tensor. In
order to accomplish that, you add singleton dimensions to make it compatible
with the attention logit tensor. By unsqueezing the mask tensor from
<span class="math notranslate nohighlight">\(B \times L\)</span> to <span class="math notranslate nohighlight">\(B \times 1 \times 1 \times L\)</span>, you enable broadcasting such
that the mask effectively gets expanded to <span class="math notranslate nohighlight">\(B \times N \times L \times L\)</span> during
the attention calculation, perfectly aligning with the attention logits.</p>
<p>That’s why the line:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">src_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">src</span> <span class="o">!=</span> <span class="n">pad</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>adds a singleton dimension, converting the shape from <span class="math notranslate nohighlight">\(B \times L\)</span> to
<span class="math notranslate nohighlight">\(B \times 1 \times 1 \times L\)</span> for proper broadcasting during the attention
computations.</p>
</section>
<section id="why-does-sequence-length-differ-for-source-and-target-usually-i-thought-it-is-just-all-l-same">
<h3><a id='toc1_13_7_'></a><a class="reference internal" href="#toc0_"><span class="xref myst">Why does sequence length differ for source and target, usually I thought it is just all L, same.</span></a><a class="headerlink" href="#why-does-sequence-length-differ-for-source-and-target-usually-i-thought-it-is-just-all-l-same" title="Link to this heading">#</a></h3>
<p>…</p>
</section>
<section id="am-i-right-to-assume-that-the-core-idea-of-autoregressive-model-like-decoder-only-gpt-like-is-that-for-a-given-sample-there-will-eventually-be-l-rows-where-l-is-the-seq-length-and-therefore-i-can-intuitively-view-it-as-1-sample-having-l-samples-since-for-each-row-we-will-compute-the-loss-am-i-right-in-my-understanding-do-not-hesistate-to-correct-me">
<h3><a id='toc1_13_8_'></a><a class="reference internal" href="#toc0_"><span class="xref myst">Am i right to assume that the core idea of autoregressive model like decoder only (GPT like) is that for a given sample, there will eventually be L rows where L is the seq length, and therefore I can intuitively view it as 1 sample having L samples, since for each row, we will compute the loss. Am I right in my understanding? Do not hesistate to correct me.</span></a><a class="headerlink" href="#am-i-right-to-assume-that-the-core-idea-of-autoregressive-model-like-decoder-only-gpt-like-is-that-for-a-given-sample-there-will-eventually-be-l-rows-where-l-is-the-seq-length-and-therefore-i-can-intuitively-view-it-as-1-sample-having-l-samples-since-for-each-row-we-will-compute-the-loss-am-i-right-in-my-understanding-do-not-hesistate-to-correct-me" title="Link to this heading">#</a></h3>
<p>Your description captures an important aspect of autoregressive models like
decoder-only Transformers (e.g., GPT). Specifically, you’re talking about how
the model treats a sequence during training. Let’s break down your understanding
step by step.</p>
<ol class="arabic simple">
<li><p><strong>Sequence Length</strong>: When you mention “L rows,” where “L” is the sequence
length, you’re essentially pointing out that each sequence is divided into
“L” time steps (or tokens). Each time step becomes an input-output pair for
training the model.</p></li>
<li><p><strong>One Sequence as Multiple Samples</strong>: You’re correct to intuit that a single
sequence of length “L” can be treated like “L” samples, at least in the
context of loss calculation. This is because, during training, the model
computes the loss at each time step by comparing the predicted token with the
actual next token in the sequence.</p></li>
<li><p><strong>Loss Computation</strong>: The loss is often computed at each position and then
averaged over the sequence length or summed up, depending on the specific
loss function or training regime.</p></li>
</ol>
<p>However, it’s crucial to clarify that although a single sequence may contribute
“L” terms to the loss function, this is not equivalent to having “L” independent
samples. The key difference lies in the autoregressive property: the prediction
at each time step is conditioned on the preceding tokens. This introduces a
temporal dependency across the “L” positions, making them not entirely
independent samples.</p>
<p>In other words, while it’s accurate to say that a single sequence contributes
multiple terms to the loss function, these terms are correlated because they
come from the same sequence and are generated in an autoregressive manner.</p>
<p>To summarize, you’re mostly correct in your understanding that a single sequence
is broken down into multiple steps for the purpose of loss computation, but it’s
important to remember that these steps are not independent samples due to the
autoregressive nature of the model.</p>
</section>
</section>
<section id="some-implementation-details">
<h2><a class="toc-backref" href="#id50" role="doc-backlink">Some Implementation Details</a><a class="headerlink" href="#some-implementation-details" title="Link to this heading">#</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Performs</span> <span class="n">one</span> <span class="n">decoder</span> <span class="n">forward</span> <span class="k">pass</span> <span class="n">given</span> <span class="n">encoder</span> <span class="n">hidden</span> <span class="n">states</span><span class="p">,</span> <span class="n">the</span> <span class="n">decoder</span> <span class="nb">input</span> <span class="n">tokens</span> <span class="ow">and</span> <span class="n">attention</span> <span class="n">masks</span><span class="o">.</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">batch</span> <span class="n">size</span>
<span class="n">S</span> <span class="o">=</span> <span class="n">source</span> <span class="n">sequence</span> <span class="n">length</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">target</span> <span class="n">sequence</span> <span class="n">length</span>
<span class="n">E</span> <span class="o">=</span> <span class="n">embedding</span> <span class="n">dimensionality</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">vocabulary</span> <span class="n">size</span>
</pre></div>
</div>
<section id="input">
<h3><a class="toc-backref" href="#id51" role="doc-backlink">Input</a><a class="headerlink" href="#input" title="Link to this heading">#</a></h3>
<p>Let’s view input’s first two samples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>tensor([[15,  4,  9, 10,  1,  3, 13,  0,  6,  2],
│   │   [15,  3,  5, 10,  4,  6, 13,  0,  8,  1]])
</pre></div>
</div>
<p>which is</p>
<ul class="simple">
<li><p>shape is <code class="docutils literal notranslate"><span class="pre">[2,</span> <span class="pre">10]</span></code> which is <code class="docutils literal notranslate"><span class="pre">BxL</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">49+13=62</span></code> but no <code class="docutils literal notranslate"><span class="pre">EOS</span></code> as we truncated last token.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">35+46=81</span></code> but no <code class="docutils literal notranslate"><span class="pre">EOS</span></code> as we truncated last token.</p></li>
</ul>
</section>
<section id="positional-encodings">
<h3><a class="toc-backref" href="#id52" role="doc-backlink">Positional Encodings</a><a class="headerlink" href="#positional-encodings" title="Link to this heading">#</a></h3>
<section id="why-do-we-hardcode-batch-size-of-1-when-creating-p">
<h4><a class="toc-backref" href="#id53" role="doc-backlink">Why do we hardcode batch size of 1 when creating P?</a><a class="headerlink" href="#why-do-we-hardcode-batch-size-of-1-when-creating-p" title="Link to this heading">#</a></h4>
<p>The tensor <span class="math notranslate nohighlight">\(P\)</span> for positional encoding is initialized with a batch size of 1.
This makes it easy to add to the actual input sequences later, during the
forward pass. Positional encodings are not dependent on the specific input
sequence but are a function of the position within the sequence. Therefore, they
can be precomputed and stored. When you look at the forward pass:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Z</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_add_positional_encoding</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
</pre></div>
</div>
<p>and the <code class="docutils literal notranslate"><span class="pre">_add_positional_encoding</span></code> method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_add_positional_encoding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Z</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add the positional encoding tensor to the input tensor.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">Z</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">P</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">Z</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">:]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">Z</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>You’ll see that <span class="math notranslate nohighlight">\(P\)</span> is sliced to match the sequence length of <span class="math notranslate nohighlight">\(Z\)</span> and then added
to <span class="math notranslate nohighlight">\(Z\)</span>. Because of broadcasting rules in PyTorch, <span class="math notranslate nohighlight">\(P\)</span> will automatically be
broadcasted to the batch size of <span class="math notranslate nohighlight">\(Z\)</span> during this addition. This is why <span class="math notranslate nohighlight">\(P\)</span> is
initialized with a batch size of 1; it keeps the implementation flexible while
making the broadcasting implicit.</p>
</section>
<section id="why-do-we-register-p-as-a-buffer-in-pytorch">
<h4><a class="toc-backref" href="#id54" role="doc-backlink">Why do we register P as a buffer in PyTorch?</a><a class="headerlink" href="#why-do-we-register-p-as-a-buffer-in-pytorch" title="Link to this heading">#</a></h4>
<p>In your <code class="docutils literal notranslate"><span class="pre">PositionalEncoding</span></code> class, the tensor <code class="docutils literal notranslate"><span class="pre">self.P</span></code> holds the pre-computed
positional encodings. If you intend for this tensor to be automatically moved to
the correct device when the module is moved, and if it should not be a learnable
parameter, then registering it as a buffer would be a good idea. This ensures
that <code class="docutils literal notranslate"><span class="pre">self.P</span></code> is part of the module’s state but is not updated during
backpropagation.</p>
<p>You could register <code class="docutils literal notranslate"><span class="pre">self.P</span></code> as a buffer right after you initialize it in the
<code class="docutils literal notranslate"><span class="pre">_init_positional_encoding</span></code> method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_init_positional_encoding</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the positional encoding tensor.&quot;&quot;&quot;</span>
    <span class="n">P</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">))</span>
    <span class="n">position</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_position_vector</span><span class="p">()</span>
    <span class="n">div_term</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_div_term_vector</span><span class="p">()</span>
    <span class="n">P</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">/</span> <span class="n">div_term</span><span class="p">)</span>
    <span class="n">P</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">/</span> <span class="n">div_term</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;P&quot;</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">P</span>
</pre></div>
</div>
<p>Using <code class="docutils literal notranslate"><span class="pre">register_buffer</span></code> ensures that:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">self.P</span></code> is automatically moved to the device the model is moved to (e.g.,
from CPU to GPU).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.P</span></code> is saved when you save the model using <code class="docutils literal notranslate"><span class="pre">torch.save</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.load</span></code>.</p></li>
</ol>
<p>The <code class="docutils literal notranslate"><span class="pre">persistent=False</span></code> argument indicates that the buffer should not be part of
the model’s <code class="docutils literal notranslate"><span class="pre">state_dict</span></code>, meaning it won’t be saved or loaded with the model. If
you do want it to be part of the <code class="docutils literal notranslate"><span class="pre">state_dict</span></code>, you can simply omit this
argument.</p>
</section>
</section>
<section id="attention">
<h3><a class="toc-backref" href="#id55" role="doc-backlink">Attention</a><a class="headerlink" href="#attention" title="Link to this heading">#</a></h3>
<section id="why-do-we-call-contiguous-on-q-k-and-v">
<h4><a class="toc-backref" href="#id56" role="doc-backlink">Why do we call contiguous on Q, K and V?</a><a class="headerlink" href="#why-do-we-call-contiguous-on-q-k-and-v" title="Link to this heading">#</a></h4>
<p>D2L’s code uses <code class="docutils literal notranslate"><span class="pre">reshape</span></code> to reshape the <code class="docutils literal notranslate"><span class="pre">Q</span></code>, <code class="docutils literal notranslate"><span class="pre">K</span></code> and <code class="docutils literal notranslate"><span class="pre">V</span></code>, where other code such
as from the Annotated Transformer uses <code class="docutils literal notranslate"><span class="pre">view</span></code>. When you use <code class="docutils literal notranslate"><span class="pre">view</span></code>, this assumes
the tensor is <code class="docutils literal notranslate"><span class="pre">contiguous</span></code>, so it is better to call <code class="docutils literal notranslate"><span class="pre">contiguous</span></code> first.</p>
</section>
<section id="why-do-we-want-to-transpose-q-k-and-v">
<h4><a class="toc-backref" href="#id57" role="doc-backlink">Why do we want to transpose Q, K, and V?</a><a class="headerlink" href="#why-do-we-want-to-transpose-q-k-and-v" title="Link to this heading">#</a></h4>
<p>The transposition of <span class="math notranslate nohighlight">\(Q\)</span>, <span class="math notranslate nohighlight">\(K\)</span>, and <span class="math notranslate nohighlight">\(V\)</span> in multi-head attention serves a specific
purpose: to allow for parallel computation across multiple attention heads. In
the original shape, the “heads” dimension does not exist; the tensor is simply
<span class="math notranslate nohighlight">\(B \times L \times D\)</span>, where <span class="math notranslate nohighlight">\(B\)</span> is the batch size, <span class="math notranslate nohighlight">\(L\)</span> is the sequence length,
and <span class="math notranslate nohighlight">\(D\)</span> is the model dimension. By transposing, we create a new shape
<span class="math notranslate nohighlight">\(B \times H \times L \times (D/H)\)</span>, where <span class="math notranslate nohighlight">\(H\)</span> is the number of heads. This
enables the following:</p>
<ol class="arabic simple">
<li><p><strong>Parallelization</strong>: Each head can now be computed in parallel since each
head operates independently of the others.</p></li>
<li><p><strong>Optimization</strong>: Modern hardware accelerators like GPUs are optimized for
certain tensor operations, and having a shape that aligns well with these
optimizations can result in faster computation.</p></li>
<li><p><strong>Readability and Maintainability</strong>: It’s easier to understand and debug the
operations for each head when they’re isolated like this.</p></li>
</ol>
</section>
<section id="why-do-we-want-to-reverse-transpose-q-k-and-v">
<h4><a class="toc-backref" href="#id58" role="doc-backlink">Why do we want to reverse transpose Q, K, and V?</a><a class="headerlink" href="#why-do-we-want-to-reverse-transpose-q-k-and-v" title="Link to this heading">#</a></h4>
<p>After the attention scores are computed and used to weight <span class="math notranslate nohighlight">\(V\)</span>, we get a new
tensor for each head. However, these tensors are still in the transposed shape
<span class="math notranslate nohighlight">\(B \times H \times L \times (D/H)\)</span>, and they need to be concatenated and
linearly transformed to continue through the network. The reverse transposition
essentially does the following:</p>
<ol class="arabic simple">
<li><p><strong>Concatenation</strong>: Converts the multiple heads back into a single tensor.
This is required because subsequent layers (like feed-forward neural
networks) expect input in the original <span class="math notranslate nohighlight">\(D\)</span>-dimensional space.</p></li>
<li><p><strong>Compatibility</strong>: The rest of the neural network architecture often expects
input tensors to have a specific shape (usually <span class="math notranslate nohighlight">\(B \times L \times D\)</span>).
Reverse transposing ensures that the output of the multi-head attention block
can be fed into subsequent layers without issue.</p></li>
<li><p><strong>Resource Efficiency</strong>: By reducing the tensor back to its original
dimensions, we can save memory and computational resources, which is
beneficial when you’re training large models or operating under hardware
constraints.</p></li>
</ol>
<p>In summary, the initial transposition is done to facilitate parallel computation
across heads, and the reverse transposition is done to concatenate these heads
and prepare the tensor for subsequent layers.</p>
</section>
</section>
</section>
<section id="why-we-need-positional-vector">
<h2><a class="toc-backref" href="#id59" role="doc-backlink">Why we need Positional Vector</a><a class="headerlink" href="#why-we-need-positional-vector" title="Link to this heading">#</a></h2>
<p>Positional encoding is critical cause the cat ate the mouse is the same as the
mouse ate the cat without it</p>
<p>Without it the attention Q and K matmul would result in a permutation invariant
matrix. So adding position info makes the last token in the attention matrix
(say mouse from the cat ate the mouse) would allow the word mouse to hold info
for every other word in the sentence as well as knowing every other token
position (including knowing it’s the last token)</p>
</section>
<section id="todo">
<h2><a class="toc-backref" href="#id60" role="doc-backlink">TODO</a><a class="headerlink" href="#todo" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Add Positional Encoding</p></li>
<li><p>Add LR Scheduler</p></li>
<li><p>Check why need to use <code class="docutils literal notranslate"><span class="pre">torch.nn.utils.clip_grad_norm_</span></code> to clip gradients</p></li>
<li><p>Why unsqueeze mask?</p></li>
<li><p>Can you init weights inside Encoder instead of outside?</p></li>
<li><p>Add Epoch and Batch State see my old code.</p></li>
<li><p>Important use <code class="docutils literal notranslate"><span class="pre">Vocab</span></code> class like in <a class="github reference external" href="https://github.com/jsbaan/transformer-from-scratch/blob/main/vocabulary.py">jsbaan/transformer-from-scratch</a>.</p></li>
</ol>
<p><a class="reference external" href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html">https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html</a></p>
</section>
<section id="references-and-further-readings">
<h2><a id='toc1_15_'></a><a class="reference internal" href="#toc0_"><span class="xref myst">References and Further Readings</span></a><a class="headerlink" href="#references-and-further-readings" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://slds-lmu.github.io/seminar_nlp_ss20/">https://slds-lmu.github.io/seminar_nlp_ss20/</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./transformer/decoder"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="implementation.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">The Implementation of Generative Pre-trained Transformers (GPT)</p>
      </div>
    </a>
    <a class="right-next"
       href="../../playbook/how_to_calculate_flops_in_gpt2.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">How to Calculate the Number of FLOPs in GPT-2</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">Motivation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#config">Config</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reproducibility">Reproducibility</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vocabulary">Vocabulary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenization">Tokenization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-dataset">Create Dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#encoding-strategy-overview">Encoding Strategy Overview</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#construct-batches-collate-function-and-dataloader">Construct Batches, Collate Function and DataLoader</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-and-target">Input and Target</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#target-padding-mask">Target Padding Mask</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#future-mask">Future Mask</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#merge-padding-and-future-masks">Merge Padding and Future Masks</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#first-sample-first-token">First Sample First Token</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#first-sample-fourth-token">First Sample Fourth Token</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#further-add-a-singleton-dimension-in-target-masks">Further Add a Singleton Dimension in Target Masks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-mask-our-target-in-adder">Why mask our target in Adder?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#split-to-train-valid-test">Split to Train-Valid-Test</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-dataloader">Create DataLoader</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model">Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-paradigm">Training Paradigm</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizer">Optimizer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-scheduler">Learning Rate Scheduler</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss">Loss</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multiheadattention"><a id="toc1_10_5_"></a><span class="xref myst">MultiHeadAttention</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-primer"><a id="toc1_10_5_1_"></a><span class="xref myst">A Primer</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-loss-is-computed"><a id="toc1_10_7_"></a><span class="xref myst">How Loss is Computed?</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#masking-and-ignore-index">Masking and Ignore Index</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-with-gpt-like-model"><a id="toc1_12_"></a><span class="xref myst">Training with GPT-like Model</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-computation"><a id="toc1_12_1_"></a><span class="xref myst">Loss Computation</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example"><a id="toc1_12_2_"></a><span class="xref myst">Example</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-training-versus-inference"><a id="toc1_12_3_"></a><span class="xref myst">Confusion: Training versus Inference</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-vs-inference"><a id="toc1_12_4_"></a><span class="xref myst">Training vs Inference</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#questions">Questions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-masked-0-in-some">Why Masked == 0 in some?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-reason-of-setting-the-attention-scores-s-mask-indexes-to-negative-infinity">what is the reason of setting the attention scores’s mask indexes to negative infinity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-need-both-ignore-index-in-loss-and-also-negative-infinity-mask"><a id="toc1_13_3_"></a><span class="xref myst">Why do we need both ignore index in Loss and also negative infinity mask</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#target-and-preds-logits-shape"><a id="toc1_13_4_"></a><span class="xref myst">Target and Preds/Logits Shape</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-flatten-prediction-and-target-logits"><a id="toc1_13_5_"></a><span class="xref myst">Why do we flatten prediction and target (logits)?</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#background"><a id="toc1_13_5_1_"></a><span class="xref myst">Background</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#traditional-loss-computation"><a id="toc1_13_5_2_"></a><span class="xref myst">Traditional Loss Computation</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#why-flatten"><a id="toc1_13_5_3_"></a><span class="xref myst">Why Flatten?</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-by-step-flattening"><a id="toc1_13_5_4_"></a><span class="xref myst">Step-by-step Flattening</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-sometimes-unsqueeze-masks"><a id="toc1_13_6_"></a><span class="xref myst">Why sometimes unsqueeze masks?</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-does-sequence-length-differ-for-source-and-target-usually-i-thought-it-is-just-all-l-same"><a id="toc1_13_7_"></a><span class="xref myst">Why does sequence length differ for source and target, usually I thought it is just all L, same.</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#am-i-right-to-assume-that-the-core-idea-of-autoregressive-model-like-decoder-only-gpt-like-is-that-for-a-given-sample-there-will-eventually-be-l-rows-where-l-is-the-seq-length-and-therefore-i-can-intuitively-view-it-as-1-sample-having-l-samples-since-for-each-row-we-will-compute-the-loss-am-i-right-in-my-understanding-do-not-hesistate-to-correct-me"><a id="toc1_13_8_"></a><span class="xref myst">Am i right to assume that the core idea of autoregressive model like decoder only (GPT like) is that for a given sample, there will eventually be L rows where L is the seq length, and therefore I can intuitively view it as 1 sample having L samples, since for each row, we will compute the loss. Am I right in my understanding? Do not hesistate to correct me.</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#some-implementation-details">Some Implementation Details</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input">Input</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-encodings">Positional Encodings</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-hardcode-batch-size-of-1-when-creating-p">Why do we hardcode batch size of 1 when creating P?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-register-p-as-a-buffer-in-pytorch">Why do we register P as a buffer in PyTorch?</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attention">Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-call-contiguous-on-q-k-and-v">Why do we call contiguous on Q, K and V?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-want-to-transpose-q-k-and-v">Why do we want to transpose Q, K, and V?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-want-to-reverse-transpose-q-k-and-v">Why do we want to reverse transpose Q, K, and V?</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-we-need-positional-vector">Why we need Positional Vector</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#todo">TODO</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references-and-further-readings"><a id="toc1_15_"></a><span class="xref myst">References and Further Readings</span></a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gao Hongnan
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>