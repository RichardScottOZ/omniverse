
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Training a Mini-GPT to Learn Two-Digit Addition &#8212; Omniverse</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=bb35926c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=888ff710"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script src="../../_static/tabs.js?v=3ee01567"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=36754332"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-MYW5YKC2WF"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MYW5YKC2WF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MYW5YKC2WF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'transformer/decoder/adder';</script>
    <link rel="canonical" href="https://www.gaohongnan.com/transformer/decoder/adder.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="How to Calculate the Number of FLOPs in GPT-2" href="../../playbook/how_to_calculate_flops_in_gpt2.html" />
    <link rel="prev" title="The Implementation of Generative Pre-trained Transformers (GPT)" href="implementation.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Omniverse - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Omniverse - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    🌌 Omniverse: A Journey Through Knowledge
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Notations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../notations/machine_learning.html">Machine Learning Notations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Generative Pre-trained Transformer</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="intro.html">Generative Pre-trained Transformers</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="notations.html">Notations</a></li>
<li class="toctree-l2"><a class="reference internal" href="concept.html">The Concept of Generative Pre-trained Transformers (GPT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="implementation.html">The Implementation of Generative Pre-trained Transformers (GPT)</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Training a Mini-GPT to Learn Two-Digit Addition</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Playbook</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../playbook/how_to_calculate_flops_in_gpt2.html">How to Calculate the Number of FLOPs in GPT-2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/how_to_inspect_function_and_class_signatures.html">How to Inspect Function and Class Signatures in Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/why_cosine_annealing_warmup_stabilize_training.html">Why Does Cosine Annealing With Warmup Stabilize Training?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/why_softmax_preserves_order_translation_invariant_not_invariant_scaling.html">Softmax Preserves Order, Is Translation Invariant But Not Invariant Under Scaling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/how_to_finetune_gpt2.html">How To Fine-Tune GPT-2 To Classify Text</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../deep_learning/training_chronicles/intro.html">Training Chronicles</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../deep_learning/training_chronicles/loss.html">The Loss Landscape</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Software Engineering</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/devops/continuous-integration/concept.html">Continuous Integration (CI) Workflow</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/devops/continuous-integration/styling.html">Styling, Formatting, and Linting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/devops/continuous-integration/testing.html">Testing</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../software_engineering/design_patterns/dependency-inversion-principle.html">Dependency Inversion Principle</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/concurrency_parallelism_asynchronous/intro.html">Concurrency, Parallelism and Asynchronous Programming</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/concurrency_parallelism_asynchronous/generator_yield.html">A Rudimentary Introduction to Generator and Yield in Python</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/serving/restful_api/intro.html">RESTful API</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/serving/restful_api/application_banking.html">Application: Designing a RESTful Banking API with FastAPI and SQLAlchemy</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Computer Science</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../computer_science/type_theory/intro.html">Type Theory, A Very Rudimentary Introduction</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/01-subtypes.html">Subtypes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/02-type-safety.html">Type Safety</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/03-subsumption.html">Subsumption</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/04-generics.html">Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/05-typevar-bound-constraints.html">Bound and Constraint in Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/06-invariance-covariance-contravariance.html">Invariance, Covariance and Contravariance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/07-pep-3124-overloading.html">Function Overloading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/08-pep-661-sentinel-values.html">Sentinel Types</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data Structures and Algorithms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/complexity_analysis/intro.html">Complexity Analysis</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/complexity_analysis/master_theorem.html">Master Theorem </a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/stack/intro.html">Stack</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/stack/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/searching_algorithms/linear_search/intro.html">Linear Search</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/linear_search/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/intro.html">Binary Search</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/problems/875-koko-eating-bananas.html">Koko Eating Bananas</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Linear Algebra</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../linear_algebra/01_preliminaries/intro.html">Preliminaries</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/01_preliminaries/01-fields.html">Fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/01_preliminaries/02-systems-of-linear-equations.html">Systems of Linear Equations</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../linear_algebra/02_vectors/intro.html">Vectors</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/01-vector-definition.html">Vector and Its Definition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/02-vector-operation.html">Vector and Its Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/03-vector-norm.html">Vector Norm and Distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/04-vector-products.html">A First Look at Vector Products</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References, Resources and Roadmap</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../bibliography.html">Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../citations.html">IEEE (Style) Citations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../api/reproducibility.html">API Reference</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/gao-hongnan/omniverse/blob/main/omniverse/transformer/decoder/adder.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/gao-hongnan/omniverse" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/gao-hongnan/omniverse/issues/new?title=Issue%20on%20page%20%2Ftransformer/decoder/adder.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/transformer/decoder/adder.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Training a Mini-GPT to Learn Two-Digit Addition</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">Motivation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#config">Config</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reproducibility">Reproducibility</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vocabulary">Vocabulary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenization">Tokenization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">Dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-dataset">Create Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#encoding-strategy-overview">Encoding Strategy Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#constructing-pytorch-dataset">Constructing PyTorch Dataset</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#construct-batches-collate-function-and-dataloader">Construct Batches, Collate Function and DataLoader</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-and-target">Input and Target</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#target-padding-mask">Target Padding Mask</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#future-mask">Future Mask</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#merge-padding-and-future-masks">Merge Padding and Future Masks</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#first-sample-first-token">First Sample First Token</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#first-sample-fourth-token">First Sample Fourth Token</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#further-add-a-singleton-dimension-in-target-masks">Further Add a Singleton Dimension in Target Masks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-mask-our-target-in-adder">Why mask our target in Adder?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#split-to-train-valid-test">Split to Train-Valid-Test</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-dataloader">Create DataLoader</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model">Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-paradigm">Training Paradigm</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizer">Optimizer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-scheduler">Learning Rate Scheduler</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Motivation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#warmup">Warmup</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#definition">Definition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation">Implementation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#criterion">Criterion</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#inputs-and-targets">Inputs and Targets</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-computation">Loss Computation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#reduction">Reduction</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-binary-classification-example">Simple Binary Classification Example</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-example">GPT Example</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-smaller-example-for-illustration">A Smaller Example for Illustration</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#masking-and-ignore-index">Masking and Ignore Index</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#initializing-criterion-with-composer">Initializing Criterion With Composer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#state">State</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#trainer">Trainer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#trained-attention-heatmaps">Trained Attention Heatmaps</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generation">Generation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references-and-further-readings">References and Further Readings</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="training-a-mini-gpt-to-learn-two-digit-addition">
<h1>Training a Mini-GPT to Learn Two-Digit Addition<a class="headerlink" href="#training-a-mini-gpt-to-learn-two-digit-addition" title="Link to this heading">#</a></h1>
<p><a class="reference external" href="https://twitter.com/gaohongnan"><img alt="Twitter Handle" src="https://img.shields.io/badge/Twitter-&#64;gaohongnan-blue?style=social&amp;logo=twitter" /></a>
<a class="reference external" href="https://linkedin.com/in/gao-hongnan"><img alt="LinkedIn Profile" src="https://img.shields.io/badge/&#64;gaohongnan-blue?style=social&amp;logo=linkedin" /></a>
<a class="reference external" href="https://github.com/gao-hongnan"><img alt="GitHub Profile" src="https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&amp;logo=github" /></a>
<img alt="Tag" src="https://img.shields.io/badge/Tag-Organized_Chaos-orange" />
<a class="reference external" href="https://github.com/gao-hongnan/omniverse/tree/5221d5d8b9bd845568b2e323d908be282c6e8434/omnivault/transformer/projects/adder"><img alt="Code" src="https://img.shields.io/badge/View-Code-blue?style=flat-square&amp;logo=github" /></a></p>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#motivation" id="id9">Motivation</a></p></li>
<li><p><a class="reference internal" href="#config" id="id10">Config</a></p></li>
<li><p><a class="reference internal" href="#reproducibility" id="id11">Reproducibility</a></p></li>
<li><p><a class="reference internal" href="#vocabulary" id="id12">Vocabulary</a></p></li>
<li><p><a class="reference internal" href="#tokenization" id="id13">Tokenization</a></p></li>
<li><p><a class="reference internal" href="#dataset" id="id14">Dataset</a></p>
<ul>
<li><p><a class="reference internal" href="#create-dataset" id="id15">Create Dataset</a></p></li>
<li><p><a class="reference internal" href="#encoding-strategy-overview" id="id16">Encoding Strategy Overview</a></p></li>
<li><p><a class="reference internal" href="#constructing-pytorch-dataset" id="id17">Constructing PyTorch Dataset</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#construct-batches-collate-function-and-dataloader" id="id18">Construct Batches, Collate Function and DataLoader</a></p>
<ul>
<li><p><a class="reference internal" href="#input-and-target" id="id19">Input and Target</a></p></li>
<li><p><a class="reference internal" href="#target-padding-mask" id="id20">Target Padding Mask</a></p></li>
<li><p><a class="reference internal" href="#future-mask" id="id21">Future Mask</a></p></li>
<li><p><a class="reference internal" href="#merge-padding-and-future-masks" id="id22">Merge Padding and Future Masks</a></p>
<ul>
<li><p><a class="reference internal" href="#first-sample-first-token" id="id23">First Sample First Token</a></p></li>
<li><p><a class="reference internal" href="#first-sample-fourth-token" id="id24">First Sample Fourth Token</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#further-add-a-singleton-dimension-in-target-masks" id="id25">Further Add a Singleton Dimension in Target Masks</a></p></li>
<li><p><a class="reference internal" href="#why-mask-our-target-in-adder" id="id26">Why mask our target in Adder?</a></p></li>
<li><p><a class="reference internal" href="#split-to-train-valid-test" id="id27">Split to Train-Valid-Test</a></p></li>
<li><p><a class="reference internal" href="#create-dataloader" id="id28">Create DataLoader</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#model" id="id29">Model</a></p></li>
<li><p><a class="reference internal" href="#training-paradigm" id="id30">Training Paradigm</a></p>
<ul>
<li><p><a class="reference internal" href="#optimizer" id="id31">Optimizer</a></p></li>
<li><p><a class="reference internal" href="#learning-rate-scheduler" id="id32">Learning Rate Scheduler</a></p>
<ul>
<li><p><a class="reference internal" href="#id4" id="id33">Motivation</a></p></li>
<li><p><a class="reference internal" href="#warmup" id="id34">Warmup</a></p></li>
<li><p><a class="reference internal" href="#definition" id="id35">Definition</a></p></li>
<li><p><a class="reference internal" href="#implementation" id="id36">Implementation</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#criterion" id="id37">Criterion</a></p>
<ul>
<li><p><a class="reference internal" href="#inputs-and-targets" id="id38">Inputs and Targets</a></p></li>
<li><p><a class="reference internal" href="#loss-computation" id="id39">Loss Computation</a></p></li>
<li><p><a class="reference internal" href="#reduction" id="id40">Reduction</a></p></li>
<li><p><a class="reference internal" href="#simple-binary-classification-example" id="id41">Simple Binary Classification Example</a></p></li>
<li><p><a class="reference internal" href="#gpt-example" id="id42">GPT Example</a></p></li>
<li><p><a class="reference internal" href="#a-smaller-example-for-illustration" id="id43">A Smaller Example for Illustration</a></p></li>
<li><p><a class="reference internal" href="#masking-and-ignore-index" id="id44">Masking and Ignore Index</a></p></li>
<li><p><a class="reference internal" href="#initializing-criterion-with-composer" id="id45">Initializing Criterion With Composer</a></p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference internal" href="#state" id="id46">State</a></p></li>
<li><p><a class="reference internal" href="#trainer" id="id47">Trainer</a></p></li>
<li><p><a class="reference internal" href="#trained-attention-heatmaps" id="id48">Trained Attention Heatmaps</a></p></li>
<li><p><a class="reference internal" href="#generation" id="id49">Generation</a></p></li>
<li><p><a class="reference internal" href="#references-and-further-readings" id="id50">References and Further Readings</a></p></li>
</ul>
</nav>
<section id="motivation">
<h2><a class="toc-backref" href="#id9" role="doc-backlink">Motivation</a><a class="headerlink" href="#motivation" title="Link to this heading">#</a></h2>
<p>Generative Pre-trained Transformer (GPT) are well known to perform bad on
arithmetic tasks such as addition. This should not come as a surprise since GPT
is a <em>language</em> model and not a <em>math</em> model. It is designed to train on a large
corpus of text and learn the patterns and structure of natural language. While
we do encounter many arithmetic operations in corpus, the encoding of these
operations are often in a form that is in the text sense, not in the
mathematical sense. After all, what GPT does best is to predict the next token
over the entire <strong>vocabulary</strong> distribution.</p>
<p>In one of the examples provided from the repository
<a class="reference external" href="https://github.com/karpathy/minGPT/tree/master">minGPT</a>, Karpathy demonstrates
training a GPT model to learn the addition of two numbers presented as strings.
This is a simple task designed to illustrate how a decoder-only model can be
trained to learn “addition”. Thus, the input is a sequence of characters
representing an addition operation (like “12 + 35”) and the output is the
sequence of characters representing the result of the addition (like “47”).</p>
<p>To this end, we replicate his example, which serves as a proof-of-concept to
show that decoder only models, which are often used for language-related tasks,
can learn other patterns or “languages,” such as the “language” of arithmetic.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">rich</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">omegaconf</span> <span class="kn">import</span> <span class="n">OmegaConf</span> <span class="k">as</span> <span class="n">om</span>
<span class="kn">from</span> <span class="nn">rich.pretty</span> <span class="kn">import</span> <span class="n">pprint</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Optimizer</span>
<span class="kn">from</span> <span class="nn">torch.optim.lr_scheduler</span> <span class="kn">import</span> <span class="n">LRScheduler</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">Subset</span><span class="p">,</span> <span class="n">random_split</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">find_root_dir</span><span class="p">(</span><span class="n">current_path</span><span class="p">:</span> <span class="n">Path</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">marker</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;.git&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Path</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Find the root directory by searching for a directory or file that serves as a</span>
<span class="sd">    marker.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    current_path : Path | None</span>
<span class="sd">        The starting path to search from. If None, the current working directory</span>
<span class="sd">        `Path.cwd()` is used.</span>
<span class="sd">    marker : str</span>
<span class="sd">        The name of the file or directory that signifies the root.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    Path | None</span>
<span class="sd">        The path to the root directory. Returns None if the marker is not found.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">current_path</span><span class="p">:</span>
        <span class="n">current_path</span> <span class="o">=</span> <span class="n">Path</span><span class="o">.</span><span class="n">cwd</span><span class="p">()</span>
    <span class="n">current_path</span> <span class="o">=</span> <span class="n">current_path</span><span class="o">.</span><span class="n">resolve</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">parent</span> <span class="ow">in</span> <span class="p">[</span><span class="n">current_path</span><span class="p">,</span> <span class="o">*</span><span class="n">current_path</span><span class="o">.</span><span class="n">parents</span><span class="p">]:</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">parent</span> <span class="o">/</span> <span class="n">marker</span><span class="p">)</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
            <span class="k">return</span> <span class="n">parent</span>
    <span class="k">return</span> <span class="kc">None</span>

<span class="n">current_file_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">())</span>
<span class="n">root_dir</span>          <span class="o">=</span> <span class="n">find_root_dir</span><span class="p">(</span><span class="n">current_file_path</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;omnivault&#39;</span><span class="p">)</span>

<span class="k">if</span> <span class="n">root_dir</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">root_dir</span><span class="p">))</span>
    <span class="kn">from</span> <span class="nn">omnivault._types._alias</span> <span class="kn">import</span> <span class="n">Accuracy</span><span class="p">,</span> <span class="n">Loss</span>
    <span class="kn">from</span> <span class="nn">omnivault.core.logger</span> <span class="kn">import</span> <span class="n">RichLogger</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.config.composer</span> <span class="kn">import</span> <span class="n">Composer</span><span class="p">,</span> <span class="n">DataConfig</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.config.constants</span> <span class="kn">import</span> <span class="n">MaybeConstant</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.config.decoder</span> <span class="kn">import</span> <span class="p">(</span>
        <span class="n">AddNormConfig</span><span class="p">,</span>
        <span class="n">DecoderBlockConfig</span><span class="p">,</span>
        <span class="n">DecoderConfig</span><span class="p">,</span>
        <span class="n">MultiHeadedAttentionConfig</span><span class="p">,</span>
        <span class="n">PositionwiseFeedForwardConfig</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.config.generator</span> <span class="kn">import</span> <span class="n">GeneratorConfig</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.config.global_</span> <span class="kn">import</span> <span class="n">MaybeGlobal</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.config.optim</span> <span class="kn">import</span> <span class="n">OPTIMIZER_REGISTRY</span><span class="p">,</span> <span class="n">AdamConfig</span><span class="p">,</span> <span class="n">OptimizerConfig</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.config.scheduler</span> <span class="kn">import</span> <span class="n">SCHEDULER_REGISTRY</span><span class="p">,</span> <span class="n">LambdaLRConfig</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.config.trainer</span> <span class="kn">import</span> <span class="n">TrainerConfig</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.core.callbacks</span> <span class="kn">import</span> <span class="n">save_state</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.core.dataset</span> <span class="kn">import</span> <span class="p">(</span>
        <span class="n">AdderDataset</span><span class="p">,</span>
        <span class="n">construct_dummy_batch_future_masks</span><span class="p">,</span>
        <span class="n">construct_dummy_batch_target_padding_masks</span><span class="p">,</span>
        <span class="n">create_loader</span><span class="p">,</span>
        <span class="n">split_dataset</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.core.optim</span> <span class="kn">import</span> <span class="n">apply_weight_decay_to_different_param_groups</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.core.tokenizer</span> <span class="kn">import</span> <span class="n">AdderTokenizer</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.core.trainer</span> <span class="kn">import</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">TrainerEvent</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.core.vocabulary</span> <span class="kn">import</span> <span class="n">AdderVocabulary</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.decoder.core</span> <span class="kn">import</span> <span class="n">GPTDecoder</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.modules.attention.core</span> <span class="kn">import</span> <span class="n">ScaledDotProductAttention</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.projects.adder.main</span> <span class="kn">import</span> <span class="n">evaluate_and_generate_on_valid_epoch_end</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.utils.general_utils</span> <span class="kn">import</span> <span class="n">create_directory</span><span class="p">,</span> <span class="n">download_file</span>
    <span class="kn">from</span> <span class="nn">omnivault.transformer.utils.visualization</span> <span class="kn">import</span> <span class="n">show_attention_heatmaps</span>
    <span class="kn">from</span> <span class="nn">omnivault.utils.config_management.omegaconf</span> <span class="kn">import</span> <span class="n">load_yaml_config</span><span class="p">,</span> <span class="n">merge_configs</span>
    <span class="kn">from</span> <span class="nn">omnivault.utils.inspector.core</span> <span class="kn">import</span> <span class="n">get_field_annotations</span>
    <span class="kn">from</span> <span class="nn">omnivault.utils.reproducibility.seed</span> <span class="kn">import</span> <span class="n">seed_all</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;Root directory not found.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="config">
<h2><a class="toc-backref" href="#id10" role="doc-backlink">Config</a><a class="headerlink" href="#config" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yaml_cfg</span> <span class="o">=</span> <span class="n">load_yaml_config</span><span class="p">(</span><span class="n">yaml_path</span><span class="o">=</span><span class="n">root_dir</span> <span class="o">/</span> <span class="s2">&quot;omnivault/transformer/projects/adder/config.yaml&quot;</span><span class="p">)</span>
<span class="n">cfg</span> <span class="o">=</span> <span class="n">merge_configs</span><span class="p">(</span><span class="n">yaml_cfg</span><span class="p">,</span> <span class="n">args_list</span><span class="o">=</span><span class="p">[])</span>
<span class="n">om</span><span class="o">.</span><span class="n">resolve</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>  <span class="c1"># inplace ops</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">constants</span><span class="p">:</span> <span class="n">MaybeConstant</span> <span class="o">=</span> <span class="n">MaybeConstant</span><span class="p">(</span>
    <span class="n">NUM_DIGITS</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">TOKENS</span><span class="o">=</span><span class="p">[</span>
        <span class="s2">&quot;0&quot;</span><span class="p">,</span>
        <span class="s2">&quot;1&quot;</span><span class="p">,</span>
        <span class="s2">&quot;2&quot;</span><span class="p">,</span>
        <span class="s2">&quot;3&quot;</span><span class="p">,</span>
        <span class="s2">&quot;4&quot;</span><span class="p">,</span>
        <span class="s2">&quot;5&quot;</span><span class="p">,</span>
        <span class="s2">&quot;6&quot;</span><span class="p">,</span>
        <span class="s2">&quot;7&quot;</span><span class="p">,</span>
        <span class="s2">&quot;8&quot;</span><span class="p">,</span>
        <span class="s2">&quot;9&quot;</span><span class="p">,</span>
        <span class="s2">&quot;+&quot;</span><span class="p">,</span>
        <span class="s2">&quot;*&quot;</span><span class="p">,</span>
        <span class="s2">&quot;-&quot;</span><span class="p">,</span>
        <span class="s2">&quot;=&quot;</span><span class="p">,</span>
        <span class="s2">&quot;&lt;BOS&gt;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;&lt;EOS&gt;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;&lt;PAD&gt;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;&lt;UNK&gt;&quot;</span><span class="p">,</span>
    <span class="p">],</span>
<span class="p">)</span>
<span class="n">global_config</span><span class="p">:</span> <span class="n">MaybeGlobal</span> <span class="o">=</span> <span class="n">MaybeGlobal</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">debug_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">data_config</span><span class="p">:</span> <span class="n">DataConfig</span> <span class="o">=</span> <span class="n">DataConfig</span><span class="p">(</span><span class="o">**</span><span class="n">cfg</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">optimizer_config</span> <span class="o">=</span> <span class="n">AdamConfig</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;torch.optim.Adam&quot;</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.98</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-9</span><span class="p">)</span>
<span class="n">cfg</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span>
<span class="n">cfg</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">max_epochs</span> <span class="o">=</span> <span class="mi">9</span>

<span class="n">trainer_config</span> <span class="o">=</span> <span class="n">TrainerConfig</span><span class="p">(</span><span class="o">**</span><span class="n">cfg</span><span class="o">.</span><span class="n">trainer</span><span class="p">)</span>
<span class="n">generate_config</span> <span class="o">=</span> <span class="n">GeneratorConfig</span><span class="p">(</span><span class="o">**</span><span class="n">cfg</span><span class="o">.</span><span class="n">generator</span><span class="p">)</span>
<span class="n">composer</span> <span class="o">=</span> <span class="n">Composer</span><span class="p">(</span>
    <span class="n">constants</span><span class="o">=</span><span class="n">constants</span><span class="p">,</span>
    <span class="n">global_</span><span class="o">=</span><span class="n">global_config</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="n">data_config</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer_config</span><span class="p">,</span>
    <span class="n">trainer</span><span class="o">=</span><span class="n">trainer_config</span><span class="p">,</span>
    <span class="n">generator</span><span class="o">=</span><span class="n">generate_config</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">composer</span><span class="p">)</span>

<span class="n">LOGGER</span> <span class="o">=</span> <span class="n">RichLogger</span><span class="p">(</span><span class="o">**</span><span class="n">composer</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">model_dump</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s2">&quot;python&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">logger</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Composer</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">constants</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">MaybeConstant</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">NUM_DIGITS</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">TOKENS</span>=<span style="font-weight: bold">[</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'0'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'1'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'2'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'3'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'4'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'5'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'6'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'7'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'8'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'9'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'+'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'*'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'-'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'='</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'&lt;BOS&gt;'</span><span style="color: #000000; text-decoration-color: #000000">,</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'&lt;EOS&gt;'</span><span style="color: #000000; text-decoration-color: #000000">,</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'&lt;PAD&gt;'</span><span style="color: #000000; text-decoration-color: #000000">,</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'&lt;UNK&gt;'</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">]</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="font-weight: bold">)</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">logger</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">LoggerConfig</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">log_file</span>=<span style="color: #800080; text-decoration-color: #800080; font-style: italic">None</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">module_name</span>=<span style="color: #800080; text-decoration-color: #800080; font-style: italic">None</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">propagate</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">log_root_dir</span>=<span style="color: #800080; text-decoration-color: #800080; font-style: italic">None</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">rich_handler_config</span>=<span style="font-weight: bold">{</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'level'</span>: <span style="color: #008000; text-decoration-color: #008000">'INFO'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'console'</span>: MISSING,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'show_level'</span>: <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'show_path'</span>: <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'show_time'</span>: <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'rich_tracebacks'</span>: <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'markup'</span>: <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'log_time_format'</span>: <span style="color: #008000; text-decoration-color: #008000">'[%Y-%m-%d %H:%M:%S]'</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">}</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="font-weight: bold">)</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">global_</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">MaybeGlobal</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">seed</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">42</span>, <span style="color: #808000; text-decoration-color: #808000">debug</span>=<span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #808000; text-decoration-color: #808000">debug_samples</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">100</span><span style="font-weight: bold">)</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">data</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">DataConfig</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">context_length</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">dataset_name</span>=<span style="color: #008000; text-decoration-color: #008000">'adder_dataset'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">dataset_size</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10000</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">dataset_path</span>=<span style="color: #008000; text-decoration-color: #008000">'./data/adder/adder_dataset.txt'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">dataset_dir</span>=<span style="color: #008000; text-decoration-color: #008000">'./data/adder'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">dataset_url</span>=<span style="color: #008000; text-decoration-color: #008000">'https://raw.githubusercontent.com/gao-hongnan/omniverse/dev/omnivault/transformer/projects/adder/assets/adder_dataset.txt'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">split</span>=<span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.7</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">collate_fn</span>=<span style="font-weight: bold">{</span><span style="color: #008000; text-decoration-color: #008000">'batch_first'</span>: <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #008000; text-decoration-color: #008000">'pad_token_id'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span><span style="font-weight: bold">}</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">train_loader</span>=<span style="font-weight: bold">{</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'batch_size'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'shuffle'</span>: <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'num_workers'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'pin_memory'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'drop_last'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">}</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">valid_loader</span>=<span style="font-weight: bold">{</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'batch_size'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'shuffle'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'num_workers'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'pin_memory'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'drop_last'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">}</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">test_loader</span>=<span style="font-weight: bold">{</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'batch_size'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'shuffle'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'num_workers'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'pin_memory'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'drop_last'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">}</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="font-weight: bold">)</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">model</span>=<span style="color: #800080; text-decoration-color: #800080">MISSING</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">optimizer</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">AdamConfig</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">name</span>=<span style="color: #008000; text-decoration-color: #008000">'torch.optim.Adam'</span>, <span style="color: #808000; text-decoration-color: #808000">lr</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2</span>, <span style="color: #808000; text-decoration-color: #808000">betas</span>=<span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.98</span><span style="font-weight: bold">)</span>, <span style="color: #808000; text-decoration-color: #808000">eps</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1e-09</span>, <span style="color: #808000; text-decoration-color: #808000">weight_decay</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0</span><span style="font-weight: bold">)</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">criterion</span>=<span style="color: #800080; text-decoration-color: #800080">MISSING</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">scheduler</span>=<span style="color: #800080; text-decoration-color: #800080">MISSING</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">trainer</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">TrainerConfig</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">device</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">device</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">type</span>=<span style="color: #008000; text-decoration-color: #008000">'cpu'</span><span style="font-weight: bold">)</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">max_epochs</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">9</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">log_every_n_steps</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">100</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">eval_every_n_steps</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">step_scheduler_on_batch_or_epoch</span>=<span style="color: #008000; text-decoration-color: #008000">'epoch'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">use_amp</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">autocast_config</span>=<span style="font-weight: bold">{</span><span style="color: #008000; text-decoration-color: #008000">'enabled'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #008000; text-decoration-color: #008000">'dtype'</span>: <span style="color: #800080; text-decoration-color: #800080; font-style: italic">None</span>, <span style="color: #008000; text-decoration-color: #008000">'cache_enabled'</span>: <span style="color: #800080; text-decoration-color: #800080; font-style: italic">None</span><span style="font-weight: bold">}</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">scaler_config</span>=<span style="font-weight: bold">{</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'enabled'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'init_scale'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">65536.0</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'growth_factor'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.0</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'backoff_factor'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.5</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #008000; text-decoration-color: #008000">'growth_interval'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2000</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">}</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">gradient_accumulation_steps</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">clip_grad_norm</span>=<span style="font-weight: bold">{</span><span style="color: #008000; text-decoration-color: #008000">'max_norm'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.0</span>, <span style="color: #008000; text-decoration-color: #008000">'norm_type'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.0</span>, <span style="color: #008000; text-decoration-color: #008000">'error_if_nonfinite'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #008000; text-decoration-color: #008000">'foreach'</span>: <span style="color: #800080; text-decoration-color: #800080; font-style: italic">None</span><span style="font-weight: bold">}</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">apply_weight_decay_to_different_param_groups</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">save_dir</span>=<span style="color: #008000; text-decoration-color: #008000">'./data/adder/checkpoints/2024-04-14_02-19-26'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">save_every_epoch</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">save_best_only</span>=<span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">monitor</span>=<span style="color: #008000; text-decoration-color: #008000">'valid_this_epoch_average_loss'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">mode</span>=<span style="color: #008000; text-decoration-color: #008000">'min'</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="font-weight: bold">)</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">generator</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">GeneratorConfig</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">max_tokens</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #808000; text-decoration-color: #808000">temperature</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.0</span>, <span style="color: #808000; text-decoration-color: #808000">greedy</span>=<span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #808000; text-decoration-color: #808000">top_k</span>=<span style="color: #800080; text-decoration-color: #800080; font-style: italic">None</span>, <span style="color: #808000; text-decoration-color: #808000">top_p</span>=<span style="color: #800080; text-decoration-color: #800080; font-style: italic">None</span><span style="font-weight: bold">)</span>
<span style="font-weight: bold">)</span>
</pre>
</div></div>
</div>
</section>
<section id="reproducibility">
<h2><a class="toc-backref" href="#id11" role="doc-backlink">Reproducibility</a><a class="headerlink" href="#reproducibility" title="Link to this heading">#</a></h2>
<p>Reproducibility in deep learning ensures that experiments can be repeated with
identical results, critical for verifying research findings and deploying
reliable models. Distributed training introduces complexity because it involves
multiple computation units which may not synchronize their random states
perfectly. If training is paused and resumed, ensuring each unit starts with the
correct seed to reproduce the exact computational path becomes challenging. To
address this, one can find more sophisticated examples in libraries like
Composer, where the whole library’s core is built around training deep neural
nets in any environment (distributed or not) with reproducibility in mind.</p>
<div class="seealso admonition">
<p class="admonition-title">References</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/mosaicml/composer/blob/dev/composer/utils/reproducibility.py">Composer</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/notes/randomness.html">PyTorch Reproducibility</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/notes/randomness.html#dataloader">PyTorch Worker</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html">PyTorch deterministic algorithms</a></p></li>
<li><p><a class="reference external" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility">CUBLAS reproducibility</a></p></li>
</ul>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">get_field_annotations</span><span class="p">(</span><span class="n">func_or_method</span> <span class="o">=</span> <span class="n">seed_all</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">inspect</span><span class="o">.</span><span class="n">getdoc</span><span class="p">(</span><span class="n">seed_all</span><span class="p">))</span>

<span class="n">seed_all</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">global_</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span> <span class="n">seed_torch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">set_torch_deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;seed&#39;, &lt;class &#39;int&#39;&gt;, 1992), (&#39;seed_torch&#39;, &lt;class &#39;bool&#39;&gt;, True), (&#39;set_torch_deterministic&#39;, &lt;class &#39;bool&#39;&gt;, True)]


Seeds all relevant random number generators to ensure reproducible
outcomes. Optionally seeds PyTorch and activates deterministic
behavior in PyTorch based on the flags provided.

Parameters
----------
seed : int, default=1992
    The seed number for reproducibility.
seed_torch : bool, default=True
    If True, seeds PyTorch&#39;s RNGs.
set_torch_deterministic : bool, default=True
    If True, activates deterministic mode in PyTorch.

Returns
-------
seed : int
    The seed number used for reproducibility.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>42
</pre></div>
</div>
</div>
</div>
</section>
<section id="vocabulary">
<h2><a class="toc-backref" href="#id12" role="doc-backlink">Vocabulary</a><a class="headerlink" href="#vocabulary" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vocabulary</span> <span class="o">=</span> <span class="n">AdderVocabulary</span><span class="o">.</span><span class="n">from_tokens</span><span class="p">(</span><span class="n">tokens</span><span class="o">=</span><span class="n">constants</span><span class="o">.</span><span class="n">TOKENS</span><span class="p">,</span> <span class="n">num_digits</span><span class="o">=</span><span class="n">constants</span><span class="o">.</span><span class="n">NUM_DIGITS</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>
<span class="n">token_to_index</span> <span class="o">=</span> <span class="n">vocabulary</span><span class="o">.</span><span class="n">token_to_index</span>
<span class="n">index_to_token</span> <span class="o">=</span> <span class="n">vocabulary</span><span class="o">.</span><span class="n">index_to_token</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocabulary</span><span class="o">.</span><span class="n">vocab_size</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">token_to_index</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">index_to_token</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">{</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'0'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'1'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'2'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'3'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'4'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'5'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'6'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'7'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">7</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'8'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'9'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">9</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'+'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'*'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'-'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">12</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'='</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">13</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'&lt;BOS&gt;'</span><span style="color: #000000; text-decoration-color: #000000">: </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">14</span><span style="color: #000000; text-decoration-color: #000000">,</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'&lt;EOS&gt;'</span><span style="color: #000000; text-decoration-color: #000000">: </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">15</span><span style="color: #000000; text-decoration-color: #000000">,</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'&lt;PAD&gt;'</span><span style="color: #000000; text-decoration-color: #000000">: </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span><span style="color: #000000; text-decoration-color: #000000">,</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008000; text-decoration-color: #008000">'&lt;UNK&gt;'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">17</span>
<span style="font-weight: bold">}</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">{</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>: <span style="color: #008000; text-decoration-color: #008000">'0'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>: <span style="color: #008000; text-decoration-color: #008000">'1'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>: <span style="color: #008000; text-decoration-color: #008000">'2'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>: <span style="color: #008000; text-decoration-color: #008000">'3'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>: <span style="color: #008000; text-decoration-color: #008000">'4'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>: <span style="color: #008000; text-decoration-color: #008000">'5'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6</span>: <span style="color: #008000; text-decoration-color: #008000">'6'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">7</span>: <span style="color: #008000; text-decoration-color: #008000">'7'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>: <span style="color: #008000; text-decoration-color: #008000">'8'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">9</span>: <span style="color: #008000; text-decoration-color: #008000">'9'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10</span>: <span style="color: #008000; text-decoration-color: #008000">'+'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11</span>: <span style="color: #008000; text-decoration-color: #008000">'*'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">12</span>: <span style="color: #008000; text-decoration-color: #008000">'-'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">13</span>: <span style="color: #008000; text-decoration-color: #008000">'='</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">14</span>: <span style="color: #008000; text-decoration-color: #008000">'&lt;BOS&gt;'</span><span style="color: #000000; text-decoration-color: #000000">,</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">15</span><span style="color: #000000; text-decoration-color: #000000">: </span><span style="color: #008000; text-decoration-color: #008000">'&lt;EOS&gt;'</span><span style="color: #000000; text-decoration-color: #000000">,</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span><span style="color: #000000; text-decoration-color: #000000">: </span><span style="color: #008000; text-decoration-color: #008000">'&lt;PAD&gt;'</span><span style="color: #000000; text-decoration-color: #000000">,</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">17</span><span style="color: #000000; text-decoration-color: #000000">: </span><span style="color: #008000; text-decoration-color: #008000">'&lt;UNK&gt;'</span>
<span style="font-weight: bold">}</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">18</span>
</pre>
</div></div>
</div>
<p>Assign <code class="docutils literal notranslate"><span class="pre">vocab_size</span></code> to <code class="docutils literal notranslate"><span class="pre">composer.model</span></code> because we don’t want to hardcode
<code class="docutils literal notranslate"><span class="pre">vocab_size</span></code> beforehand, and want to derive concrete values from the
<code class="docutils literal notranslate"><span class="pre">Vocabulary</span></code> object.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="n">composer</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
<span class="k">except</span> <span class="ne">AttributeError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
    <span class="n">LOGGER</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #7fbfbf; text-decoration-color: #7fbfbf">[2024-04-14 02:19:26] </span><span style="color: #800000; text-decoration-color: #800000">ERROR   </span> _Missing instances are immutable                                     <a href="file:///tmp/ipykernel_2631/2890644827.py" target="_blank"><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">2890644827.py</span></a><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">:</span><a href="file:///tmp/ipykernel_2631/2890644827.py#4" target="_blank"><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">4</span></a>
</pre>
</div></div>
</div>
<p>Ah okay haha, this is the price of writing overly complex and useless code to
look fancy and you end up a mess. Anyways, we will handle this later on where
we can explicitly instantiate the model config class.</p>
</section>
<section id="tokenization">
<h2><a class="toc-backref" href="#id13" role="doc-backlink">Tokenization</a><a class="headerlink" href="#tokenization" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AdderTokenizer</span><span class="p">(</span><span class="n">vocabulary</span><span class="o">=</span><span class="n">vocabulary</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">vocabulary</span><span class="o">.</span><span class="n">token_to_index</span> <span class="o">==</span> <span class="n">token_to_index</span>
<span class="k">assert</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">vocabulary</span><span class="o">.</span><span class="n">index_to_token</span> <span class="o">==</span> <span class="n">index_to_token</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pprint</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;1&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">14</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">15</span><span style="font-weight: bold">]</span>
</pre>
</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sequence</span> <span class="o">=</span> <span class="s2">&quot;15+57=072&quot;</span>
<span class="n">sequences</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;15+57=072&quot;</span><span class="p">,</span> <span class="s2">&quot;01+02=003&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">encoded_sentence</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Encoded sentence: </span><span class="si">{</span><span class="n">encoded_sentence</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">decoded_sentence</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">encoded_sentence</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Decoded sentence: </span><span class="si">{</span><span class="n">decoded_sentence</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Encoded sentence: [14, 1, 5, 10, 5, 7, 13, 0, 7, 2, 15]
Decoded sentence: 15+57=072
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">encoded_sentences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode_batch</span><span class="p">(</span><span class="n">sequences</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Encoded sentences: </span><span class="si">{</span><span class="n">encoded_sentences</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">decoded_sentences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode_batch</span><span class="p">(</span><span class="n">encoded_sentences</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Decoded sentences: </span><span class="si">{</span><span class="n">decoded_sentences</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Encoded sentences: [[14, 1, 5, 10, 5, 7, 13, 0, 7, 2, 15], [14, 0, 1, 10, 0, 2, 13, 0, 0, 3, 15]]
Decoded sentences: [&#39;15+57=072&#39;, &#39;01+02=003&#39;]
</pre></div>
</div>
</div>
</div>
</section>
<section id="dataset">
<h2><a class="toc-backref" href="#id14" role="doc-backlink">Dataset</a><a class="headerlink" href="#dataset" title="Link to this heading">#</a></h2>
<section id="create-dataset">
<h3><a class="toc-backref" href="#id15" role="doc-backlink">Create Dataset</a><a class="headerlink" href="#create-dataset" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">pad_number</span><span class="p">(</span><span class="n">num</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">length</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pad numbers with zeros in front so that they have uniform length.</span>

<span class="sd">    Note, if a + b = c and num digits allowed to add is 2, then for</span>
<span class="sd">    a and b we always pad to length 2, but for c we always pad to length 3.</span>

<span class="sd">    Example</span>
<span class="sd">    -------</span>
<span class="sd">    6 + 90 = 96 -&gt; 06 + 90 = 096</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    num : int</span>
<span class="sd">        Number to be padded.</span>
<span class="sd">    num_digits : int</span>
<span class="sd">        Length of the resulting padded number string.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    str</span>
<span class="sd">        Padded number string.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="n">num</span><span class="p">)</span><span class="o">.</span><span class="n">zfill</span><span class="p">(</span><span class="n">length</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">equation_to_string</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_digits</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Formats the addition equation as a string.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    a : int</span>
<span class="sd">        First addend.</span>
<span class="sd">    b : int</span>
<span class="sd">        Second addend.</span>
<span class="sd">    c : int</span>
<span class="sd">        Sum of a and b.</span>
<span class="sd">    num_digits : int</span>
<span class="sd">        Number of digits each number in the equation should have.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    str</span>
<span class="sd">        Formatted equation string.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">padded_a</span> <span class="o">=</span> <span class="n">pad_number</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">num_digits</span><span class="p">)</span>
    <span class="n">padded_b</span> <span class="o">=</span> <span class="n">pad_number</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">num_digits</span><span class="p">)</span>
    <span class="n">padded_c</span> <span class="o">=</span> <span class="n">pad_number</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">num_digits</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># note the padding here!</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">padded_a</span><span class="si">}</span><span class="s2">+</span><span class="si">{</span><span class="n">padded_b</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="n">padded_c</span><span class="si">}</span><span class="s2">&quot;</span>

<span class="k">def</span> <span class="nf">decode_equation</span><span class="p">(</span><span class="n">vocab</span><span class="p">:</span> <span class="n">AdderVocabulary</span><span class="p">,</span> <span class="n">equation</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">show_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Convert an equation in list format to string format.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    equation : List[int]</span>
<span class="sd">        The equation in list format.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    str</span>
<span class="sd">        The equation in string format.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">equation</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">equation</span> <span class="o">=</span> <span class="n">equation</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="n">UNK</span> <span class="o">=</span> <span class="n">vocab</span><span class="o">.</span><span class="n">token_to_index</span><span class="p">[</span><span class="n">vocab</span><span class="o">.</span><span class="n">UNK</span><span class="p">]</span>
    <span class="n">decoded_equation</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="n">index_to_token</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">UNK</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">equation</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">show_special_tokens</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">decoded_equation</span>
    <span class="k">return</span> <span class="n">decoded_equation</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;&lt;BOS&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;&lt;EOS&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;&lt;PAD&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;&lt;UNK&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">batch_decode_equation</span><span class="p">(</span><span class="n">vocab</span><span class="p">:</span> <span class="n">AdderVocabulary</span><span class="p">,</span> <span class="n">equations</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="n">decoded_equations</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">equation</span> <span class="ow">in</span> <span class="n">equations</span><span class="p">:</span>
        <span class="n">decoded_equation</span> <span class="o">=</span> <span class="n">decode_equation</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">equation</span><span class="p">)</span>
        <span class="n">decoded_equations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">decoded_equation</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">decoded_equations</span>

<span class="k">def</span> <span class="nf">encode_equation</span><span class="p">(</span><span class="n">vocab</span><span class="p">:</span> <span class="n">AdderVocabulary</span><span class="p">,</span> <span class="n">equation</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">num_digits</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Convert an equation (up to the equal sign in it) in string format to a list.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    equation : str</span>
<span class="sd">        The equation in string format.</span>
<span class="sd">    num_digits : int</span>
<span class="sd">        Number of digits each number in the equation should have.</span>
<span class="sd">    device : torch.device</span>
<span class="sd">        The device to which the tensor should be sent.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    torch.Tensor</span>
<span class="sd">        The equation in list format as a tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">plus_idx</span> <span class="o">=</span> <span class="n">equation</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">vocab</span><span class="o">.</span><span class="n">ADD</span><span class="p">)</span>
    <span class="n">equal_idx</span> <span class="o">=</span> <span class="n">equation</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">vocab</span><span class="o">.</span><span class="n">EQUAL</span><span class="p">)</span>

    <span class="n">BOS</span> <span class="o">=</span> <span class="n">vocab</span><span class="o">.</span><span class="n">token_to_index</span><span class="p">[</span><span class="n">vocab</span><span class="o">.</span><span class="n">BOS</span><span class="p">]</span>
    <span class="n">UNK</span> <span class="o">=</span> <span class="n">vocab</span><span class="o">.</span><span class="n">token_to_index</span><span class="p">[</span><span class="n">vocab</span><span class="o">.</span><span class="n">UNK</span><span class="p">]</span>

    <span class="n">a</span> <span class="o">=</span> <span class="n">pad_number</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">equation</span><span class="p">[:</span><span class="n">plus_idx</span><span class="p">]),</span> <span class="n">num_digits</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">pad_number</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">equation</span><span class="p">[</span><span class="n">plus_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span><span class="n">equal_idx</span><span class="p">]),</span> <span class="n">num_digits</span><span class="p">)</span>

    <span class="n">new_equation</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s2">+</span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s2">=&quot;</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
        <span class="p">[</span><span class="n">BOS</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">token_to_index</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">UNK</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">new_equation</span><span class="p">],</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_add_dataset</span><span class="p">(</span>
    <span class="n">vocab</span><span class="p">:</span> <span class="n">AdderVocabulary</span><span class="p">,</span> <span class="n">num_digits</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dataset_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">rng_seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1337</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
    <span class="n">BOS</span> <span class="o">=</span> <span class="n">vocab</span><span class="o">.</span><span class="n">token_to_index</span><span class="p">[</span><span class="n">vocab</span><span class="o">.</span><span class="n">BOS</span><span class="p">]</span>
    <span class="n">EOS</span> <span class="o">=</span> <span class="n">vocab</span><span class="o">.</span><span class="n">token_to_index</span><span class="p">[</span><span class="n">vocab</span><span class="o">.</span><span class="n">EOS</span><span class="p">]</span>
    <span class="n">UNK</span> <span class="o">=</span> <span class="n">vocab</span><span class="o">.</span><span class="n">token_to_index</span><span class="p">[</span><span class="n">vocab</span><span class="o">.</span><span class="n">UNK</span><span class="p">]</span>

    <span class="n">rng</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span>
    <span class="n">rng</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">rng_seed</span><span class="p">)</span>

    <span class="n">max_num</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="n">num_digits</span> <span class="o">-</span> <span class="mi">1</span>

    <span class="n">dataset_str</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dataset_size</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">max_num</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">generator</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">max_num</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">generator</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>

        <span class="n">equation</span> <span class="o">=</span> <span class="n">equation_to_string</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">num_digits</span><span class="p">)</span>

        <span class="n">dataset_str</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">equation</span><span class="p">)</span>

    <span class="n">dataset_tensor</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">BOS</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">token_to_index</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">UNK</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">EOS</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">dataset_str</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="n">dataset_tensor</span><span class="p">,</span> <span class="n">dataset_str</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataset_tensor</span><span class="p">,</span> <span class="n">dataset_str</span> <span class="o">=</span> <span class="n">create_add_dataset</span><span class="p">(</span><span class="n">vocab</span><span class="o">=</span><span class="n">vocabulary</span><span class="p">,</span> <span class="n">num_digits</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dataset_size</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">dataset_tensor</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">dataset_str</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">[</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">14</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">7</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">13</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">7</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">15</span><span style="font-weight: bold">])</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">14</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">9</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">13</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">9</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">15</span><span style="font-weight: bold">])</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">14</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">9</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">13</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">15</span><span style="font-weight: bold">])</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">14</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">13</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">15</span><span style="font-weight: bold">])</span>
<span style="font-weight: bold">]</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">[</span><span style="color: #008000; text-decoration-color: #008000">'15+57=072'</span>, <span style="color: #008000; text-decoration-color: #008000">'92+00=092'</span>, <span style="color: #008000; text-decoration-color: #008000">'95+53=148'</span>, <span style="color: #008000; text-decoration-color: #008000">'15+10=025'</span><span style="font-weight: bold">]</span>
</pre>
</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Decoded equation: </span><span class="si">{</span><span class="n">decode_equation</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">,</span><span class="w"> </span><span class="n">dataset_tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span>
    <span class="n">decode_equation</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">,</span> <span class="n">dataset_tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="o">==</span> <span class="n">dataset_str</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="o">==</span> <span class="n">decode_equation</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">,</span> <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">14</span><span class="p">])</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Decoded equation: 15+57=072
</pre></div>
</div>
</div>
</div>
<p>if we encode equation, we can encode up to equal sign like below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Encoded equation: </span><span class="si">{</span><span class="n">encode_equation</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">,</span><span class="w"> </span><span class="n">dataset_str</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="n">num_digits</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="n">device</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_close</span><span class="p">(</span>
    <span class="n">encode_equation</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">,</span> <span class="n">dataset_str</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">num_digits</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">14</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">13</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Encoded equation: tensor([14,  1,  5, 10,  5,  7, 13], dtype=torch.int32)
</pre></div>
</div>
</div>
</div>
<p>Uncomment the below code to generate the dataset into a text file and yes, I am
lazy to add a config variable for whether to generate the dataset or not.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># dataset, dataset_str = create_add_dataset(vocab, self.num_digits, self.dataset_size)</span>

<span class="c1"># write dataset_str to a file</span>
<span class="c1"># with open(&quot;dataset_str.txt&quot;, &quot;w&quot;) as f:</span>
<span class="c1">#     for item in dataset_str:</span>
<span class="c1">#         f.write(&quot;%s\n&quot; % item)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="encoding-strategy-overview">
<h3><a class="toc-backref" href="#id16" role="doc-backlink">Encoding Strategy Overview</a><a class="headerlink" href="#encoding-strategy-overview" title="Link to this heading">#</a></h3>
<p>Our strategy for encoding arithmetic expressions is pretty self-explanatory,
where given a string <code class="docutils literal notranslate"><span class="pre">D1</span> <span class="pre">+</span> <span class="pre">D2</span> <span class="pre">=</span> <span class="pre">D3</span></code>, we encode it as <code class="docutils literal notranslate"><span class="pre">&lt;BOS&gt;D1+D2=0D3&lt;EOS&gt;</span></code>.
However, this is verbose for clarity sake. In fact, Karpathy’s encoding strategy
simplifies arithmetic expressions by concatenating the digits of operands and
the result into a single string without explicit symbols for operations or
equality. This method relies on a fixed number of digits (<code class="docutils literal notranslate"><span class="pre">num_digits</span></code>) for
operands, which streamlines the model’s interpretation of the sequence. For
example, if <code class="docutils literal notranslate"><span class="pre">num_digits</span></code> is set to 2, every encoded expression is structured to
follow a predictable pattern: the first two digits represent the first operand,
the next two digits represent the second operand, and the final digits are
encoded as 3 digits because the max sum of two 2-digit numbers is 199, which is
3 digits. The digits of the result are encoded in reverse order. This
counterintuitive approach is designed to align with the GPT model’s learning
algorithm, facilitating easier learning of the addition operation by mimicking
the traditional right-to-left calculation process in addition.</p>
<p>To illustrate, let’s examine the encoding of arithmetic expressions with
<code class="docutils literal notranslate"><span class="pre">num_digits=2</span></code>:</p>
<p>For the expression <code class="docutils literal notranslate"><span class="pre">6</span> <span class="pre">+</span> <span class="pre">39</span> <span class="pre">=</span> <span class="pre">45</span></code>, we have the following:</p>
<ul class="simple">
<li><p>The first two digits <code class="docutils literal notranslate"><span class="pre">06</span></code> represent the number 6, zero-padded to adhere to
the <code class="docutils literal notranslate"><span class="pre">num_digits=2</span></code> requirement.</p></li>
<li><p>The next two digits <code class="docutils literal notranslate"><span class="pre">39</span></code> represent the number 39, already fitting the digit
requirement.</p></li>
<li><p>The final part <code class="docutils literal notranslate"><span class="pre">054</span></code> represents the result 45, reversed to <code class="docutils literal notranslate"><span class="pre">54</span></code> and preceded
by a zero to maintain the total length of <span class="math notranslate nohighlight">\(2n + (n + 1) = 7 \)</span> digits for
<code class="docutils literal notranslate"><span class="pre">num_digits=2</span></code>.</p></li>
</ul>
</section>
<section id="constructing-pytorch-dataset">
<h3><a class="toc-backref" href="#id17" role="doc-backlink">Constructing PyTorch Dataset</a><a class="headerlink" href="#constructing-pytorch-dataset" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">create_directory</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dataset_dir</span><span class="p">)</span>
<span class="n">download_file</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dataset_url</span><span class="p">,</span> <span class="n">output_path</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dataset_path</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100   97k  100   97k    0     0   931k      0 --:--:-- --:--:-- --:--:--  939k
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dataset_path</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
    <span class="n">sequences</span> <span class="o">=</span> <span class="p">[</span><span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">file</span><span class="p">]</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">AdderDataset</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">sequences</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>

<span class="n">pprint</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataset</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">14</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">7</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">13</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">7</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span><span style="font-weight: bold">])</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">7</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">15</span><span style="font-weight: bold">])</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([</span><span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">])</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">]])</span>
<span style="font-weight: bold">)</span>
</pre>
</div></div>
</div>
</section>
</section>
<section id="construct-batches-collate-function-and-dataloader">
<h2><a class="toc-backref" href="#id18" role="doc-backlink">Construct Batches, Collate Function and DataLoader</a><a class="headerlink" href="#construct-batches-collate-function-and-dataloader" title="Link to this heading">#</a></h2>
<p>We first reverse engineer what our dataset is returning. The disclaimer here is
that for decoder only models like GPT, many people often omit the padding mask
since all the samples <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> are chunked to sequence/context length of
window size <span class="math notranslate nohighlight">\(T\)</span>, and future masks are usually handled within the <code class="docutils literal notranslate"><span class="pre">Attention</span></code>
class since we will never attend to the future tokens. However, for the sake of
clarity, we will include the padding and future mask in the dataset (i.e.
actually it is for the sake of my own understanding when I started to implement
decoder from scratch).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">target_padding_mask</span><span class="p">,</span> <span class="n">future_mask</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<section id="input-and-target">
<h3><a class="toc-backref" href="#id19" role="doc-backlink">Input and Target</a><a class="headerlink" href="#input-and-target" title="Link to this heading">#</a></h3>
<p>I think if you’ve read my
<a class="reference external" href="https://www.gaohongnan.com/transformer/decoder/implementation.html#construction-of-input-and-target-sequences">section here</a>,
then we would easily see that given an input sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, the target
sequence <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> is simply the input sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> shifted by one
time step to the left.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input : </span><span class="si">{</span><span class="nb">input</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Target: </span><span class="si">{</span><span class="n">target</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Input : tensor([14,  1,  5, 10,  5,  7, 13,  0,  7,  2])
Target: tensor([16, 16, 16, 16, 16, 16,  0,  7,  2, 15])
</pre></div>
</div>
</div>
</div>
</section>
<section id="target-padding-mask">
<h3><a class="toc-backref" href="#id20" role="doc-backlink">Target Padding Mask</a><a class="headerlink" href="#target-padding-mask" title="Link to this heading">#</a></h3>
<p>When you’re dealing with sequences of different lengths, you pad the shorter
sequences with a special token <code class="docutils literal notranslate"><span class="pre">PAD</span></code> (usually <span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(-100\)</span>) to make them the
same length as the longest one in the batch. These paddings should not
contribute to the model’s learning, so you need to mask them out. In practice,
you’ll often see a mask argument in <code class="docutils literal notranslate"><span class="pre">Attention</span></code> layers in PyTorch where if
<code class="docutils literal notranslate"><span class="pre">True</span></code>, the attention scores are set to <code class="docutils literal notranslate"><span class="pre">-inf</span></code> for the padded positions so that
these positions become zero after the softmax operation, thereby not
contributing to the weighted sum of the input sequence.</p>
<p>In a decoder-only model like GPT, the input sequence is essentially the target.
The model aims to generate tokens that come after the given input, treating it
as the “history” or “context” for the task of text generation. Unlike
encoder-decoder models like the original Transformer, where the encoder
processes a source sequence and the decoder generates a target sequence, a
decoder-only model works solely with what would traditionally be considered the
target sequence.</p>
<p>Consequently, although the terminology “target padding mask” might seem more
intuitive in the context of encoder-decoder models, where the distinction
between source (input) and target (output) sequences is clear. The distinction
is blurred in decoder-only models like GPT as the model processes input to
predict the next token in a sequence. Here, the source is essentially the target
at different stages of processing: the model uses previous tokens (source) to
predict the next token (target). However, during my implementation, I was mainly
referring to transformer models that use encoder-decoder architecture, and the
terminology therefore stemmed from that context.</p>
<p>The definition of a target padding mask is a binary mark that ignores pad-tokens
in the source input (in decoder only model, the source is the target). And the
shape is <span class="math notranslate nohighlight">\((\mathcal{B}, T)\)</span>.</p>
<p>Let’s illustrate the target padding mask with an example. Suppose we have a
batch of sequences with different lengths:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">target_batch</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">17</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
<span class="p">]</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">target_batch</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">[[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">7</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">9</span><span style="font-weight: bold">]</span>, <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6</span><span style="font-weight: bold">]</span>, <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">12</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">17</span><span style="font-weight: bold">]</span>, <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span><span style="font-weight: bold">]]</span>
</pre>
</div></div>
</div>
<p>If we try to “batch” these sequences, PyTorch would throw an error indicating
that you need all sequences to have the same length.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="n">target_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">target_batch</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">ValueError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
    <span class="n">LOGGER</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #7fbfbf; text-decoration-color: #7fbfbf">                      </span><span style="color: #800000; text-decoration-color: #800000">ERROR   </span> expected sequence of length <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span> at dim <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span> <span style="font-weight: bold">(</span>got <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span><span style="font-weight: bold">)</span>                       <a href="file:///tmp/ipykernel_2631/1205213247.py" target="_blank"><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">1205213247.py</span></a><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">:</span><a href="file:///tmp/ipykernel_2631/1205213247.py#4" target="_blank"><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">4</span></a>
</pre>
</div></div>
</div>
<p>To address this issue, we could pad the sequences to the same length and create a mask to indicate
which positions are padded.  We pad the shorter sequences with a special token <code class="docutils literal notranslate"><span class="pre">PAD</span></code>
to make them the same length as the longest one in the batch.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">PAD</span> <span class="o">=</span> <span class="n">vocabulary</span><span class="o">.</span><span class="n">token_to_index</span><span class="p">[</span><span class="n">vocabulary</span><span class="o">.</span><span class="n">PAD</span><span class="p">]</span>

<span class="n">max_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span> <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">target_batch</span><span class="p">)</span>
<span class="n">target_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">seq</span> <span class="o">+</span> <span class="p">[</span><span class="n">PAD</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_len</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">seq</span><span class="p">))</span> <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">target_batch</span><span class="p">]</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">target_batch</span><span class="p">)</span>

<span class="n">target_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">target_batch</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">target_batch</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">[[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">7</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">9</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span><span style="font-weight: bold">]</span>, <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span><span style="font-weight: bold">]</span>, <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">12</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">17</span><span style="font-weight: bold">]</span>, <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span><span style="font-weight: bold">]]</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">7</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">9</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">12</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">17</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span><span style="font-weight: bold">]])</span>
</pre>
</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span> <span class="o">=</span> <span class="n">target_batch</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

<span class="n">target_padding_mask</span> <span class="o">=</span> <span class="n">target_batch</span> <span class="o">!=</span> <span class="n">PAD</span>

<span class="n">pprint</span><span class="p">(</span><span class="n">target_padding_mask</span><span class="p">)</span>

<span class="k">assert</span> <span class="n">target_padding_mask</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]])</span>
</pre>
</div></div>
</div>
<p>Of course, we would need a <em>batch</em> of these masks, so we would have a shape of
<span class="math notranslate nohighlight">\((\mathcal{B}, T)\)</span> like mentioned above. As we will see later, we will still
need to broadcast the shape to <span class="math notranslate nohighlight">\((\mathcal{B}, 1, T, T)\)</span> to match the shape of
the attention scores.</p>
<p>Theoretically speaking, it is possible for the sequence length <span class="math notranslate nohighlight">\(T\)</span> to vary
across samples <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. However, we usually have the same length for all
samples in GPT, and in this particular case, we do know that each sample
necessarily have the same length by <em>design</em>. However, for the sake of
explanation, we note that in our <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>, it will only generate 1 single
sample data point and do not worry about different sequence length across other
samples in the dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>, but in deep learning we train in
mini-batches <span class="math notranslate nohighlight">\(\mathcal{B}\)</span>, and with different batch sizes we may encounter
issues (i.e. matrix multiplication may not work).</p>
</section>
<section id="future-mask">
<h3><a class="toc-backref" href="#id21" role="doc-backlink">Future Mask</a><a class="headerlink" href="#future-mask" title="Link to this heading">#</a></h3>
<p>In the decoder, each position can only attend to positions that come before it
in the sequence to maintain the auto-regressive property. This is different from
the encoder, where all positions can attend to all other positions.</p>
<p>The definition of future mask is basically a look-ahead mask to ensure that each
position only attends to positions before it in the sequence where we mask out
future positions (i.e., positions that come after the current position) so that
they don’t contribute to the current attention scores. Before the softmax
operation, we’ll mark these positions as <code class="docutils literal notranslate"><span class="pre">-inf</span></code> so that they become zero after
the softmax operation - effectively zeroing out the attention scores for future
positions. What does zeroing out these masked logits actually does? Basically,
the attention mechanism can be thought of as a weighted average of all the
tokens in the input sequence. Each token is assigned a weight, with higher
weights indicating more relevance to the token under consideration. If a certain
token should not be considered at all (e.g., it’s a future token that should not
be visible to the current decoder step, or it’s a padding token), its weight
should be zero.</p>
<p>The shape of the future mask is <span class="math notranslate nohighlight">\((T, T)\)</span> for a target sequence/sample
<span class="math notranslate nohighlight">\(\mathbf{x}\)</span> of length <span class="math notranslate nohighlight">\(T\)</span>. Let’s see a concrete example to illustrate the
future mask.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">seq_len</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">future_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">future_mask</span> <span class="o">=</span> <span class="n">future_mask</span> <span class="o">==</span> <span class="mi">0</span>

<span class="n">pprint</span><span class="p">(</span><span class="n">future_mask</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">future_mask</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">]])</span>
</pre>
</div></div>
</div>
</section>
<section id="merge-padding-and-future-masks">
<h3><a class="toc-backref" href="#id22" role="doc-backlink">Merge Padding and Future Masks</a><a class="headerlink" href="#merge-padding-and-future-masks" title="Link to this heading">#</a></h3>
<p>We see from our <code class="docutils literal notranslate"><span class="pre">decoder</span></code> implementation below, that one of the method is
creating the target masks. In other words, we are creating the target padding
masks and future masks, and merging them together.</p>
<div class="highlight-md notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span>def create_target_masks(
<span class="linenos"> 2</span>    self,
<span class="linenos"> 3</span>    batch_size: int,
<span class="linenos"> 4</span>    seq_len: int,
<span class="linenos"> 5</span>    target_padding_masks: torch.BoolTensor | NotGiven = NOT_GIVEN,
<span class="linenos"> 6</span>    future_masks: torch.BoolTensor | NotGiven = NOT_GIVEN,
<span class="linenos"> 7</span>) -&gt; torch.BoolTensor:
<span class="linenos"> 8</span>    target_masks_shape = (batch_size, 1, seq_len, seq_len)
<span class="linenos"> 9</span>    if target_padding_masks is NOT_GIVEN and future_masks is NOT_GIVEN:
<span class="linenos">10</span>        target_padding_masks = cast(
<span class="linenos">11</span>            torch.BoolTensor, construct_dummy_batch_target_padding_masks(batch_size, seq_len)
<span class="linenos">12</span>        )
<span class="linenos">13</span>        future_masks = cast(torch.BoolTensor, construct_dummy_batch_future_masks(batch_size, seq_len))
<span class="linenos">14</span>
<span class="linenos">15</span>    if target_padding_masks is NOT_GIVEN:
<span class="linenos">16</span>        target_padding_masks = cast(
<span class="linenos">17</span>            torch.BoolTensor, construct_dummy_batch_target_padding_masks(batch_size, seq_len)
<span class="linenos">18</span>        )
<span class="linenos">19</span>
<span class="linenos">20</span>    if future_masks is NOT_GIVEN:
<span class="linenos">21</span>        future_masks = cast(torch.BoolTensor, construct_dummy_batch_future_masks(batch_size, seq_len))
<span class="linenos">22</span>
<span class="linenos">23</span>    assert target_padding_masks.shape == future_masks.shape == target_masks_shape  # type: ignore[union-attr]
<span class="linenos">24</span>
<span class="linenos">25</span>    return cast(
<span class="linenos">26</span>        torch.BoolTensor,
<span class="hll"><span class="linenos">27</span>        torch.logical_and(cast(torch.Tensor, target_padding_masks), cast(torch.Tensor, future_masks)).bool(),
</span><span class="linenos">28</span>    )
</pre></div>
</div>
<p>The purpose of applying <code class="docutils literal notranslate"><span class="pre">logical_and</span></code> between <code class="docutils literal notranslate"><span class="pre">target_padding_mask</span></code> and
<code class="docutils literal notranslate"><span class="pre">future_mask</span></code> is to combine the constraints from both masks when calculating
self-attention scores in the transformer’s decoder. The <code class="docutils literal notranslate"><span class="pre">target_padding_mask</span></code> is
designed to mask out the padding tokens in the input sequence, while the
<code class="docutils literal notranslate"><span class="pre">future_mask</span></code> ensures that a given position cannot attend to future positions in
the sequence. By combining these masks, you can perform the necessary masking
for both padding and future tokens in a single step.</p>
<p>Here’s how it works:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">target_padding_mask</span></code>: Masks out the padding tokens so that they don’t
contribute to the attention calculations. True values mean “attend to this
token,” and False values mean “ignore this token.”</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">future_mask</span></code>: The future mask is created as a lower triangular matrix, where
the lower triangle, including the diagonal, is filled with ones, and the
upper triangle is filled with zeros. Masks out future tokens in a sequence so
that a token at a given position can only attend to positions that come
before it (and itself). True values mean “attend to this token,” and False
values mean “ignore this token.”</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">logical_and(target_padding_mask,</span> <span class="pre">future_mask)</span></code>: Combines the two masks. A
True in the resulting mask means that the condition for both padding and
future attention is satisfied.</p></li>
</ol>
<p>By combining these two masks, the decoder obeys the autoregressive property,
ensuring it doesn’t see future tokens, while also ignoring padding tokens in the
input sequence. We may term it the <code class="docutils literal notranslate"><span class="pre">target_mask</span></code>.</p>
<section id="first-sample-first-token">
<h4><a class="toc-backref" href="#id23" role="doc-backlink">First Sample First Token</a><a class="headerlink" href="#first-sample-first-token" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">target_padding_mask</span></code> has size of <code class="docutils literal notranslate"><span class="pre">[4,</span> <span class="pre">5]</span></code>.</p>
<ul>
<li><p>We zoom in to the first row (sample) which is of length 5.</p></li>
<li><p>This length 5 is the sequence length, which is <code class="docutils literal notranslate"><span class="pre">T,</span> <span class="pre">T,</span> <span class="pre">T,</span> <span class="pre">F,</span> <span class="pre">F</span></code>
indicating the last 2 tokens being padded.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">future_mask</span></code> has size of <code class="docutils literal notranslate"><span class="pre">[5,</span> <span class="pre">5]</span></code>.</p>
<ul>
<li><p>We note that this is indepedent of batch size. Each sample should have
the same future mask shape of <code class="docutils literal notranslate"><span class="pre">[L,</span> <span class="pre">L]</span></code>.</p></li>
<li><p>This <code class="docutils literal notranslate"><span class="pre">L=5</span></code> should necessary be same for the sequence length in
<code class="docutils literal notranslate"><span class="pre">target_padding_mask</span></code>.</p></li>
</ul>
</li>
<li><p>First, let’s consider one batch of 4 samples. What we do first is to
broadcast <code class="docutils literal notranslate"><span class="pre">future_mask</span></code> to <code class="docutils literal notranslate"><span class="pre">[4,</span> <span class="pre">5,</span> <span class="pre">5]</span></code> because we want each sample/row in
the batch to have the same future mask. As shown below:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pprint</span><span class="p">(</span><span class="n">future_mask</span><span class="p">)</span>
<span class="n">future_mask</span> <span class="o">=</span> <span class="n">future_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">future_mask</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">future_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">]])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">]]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">]]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">]]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">]]])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">torch.Size</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span><span style="font-weight: bold">])</span>
</pre>
</div></div>
</div>
<ul class="simple">
<li><p>Now, we can zoom in to one particular sample since both
<code class="docutils literal notranslate"><span class="pre">target_padding_mask</span></code> and <code class="docutils literal notranslate"><span class="pre">future_mask</span></code> have the same first dimension of
batch size.</p></li>
<li><p>What is incomplete is that we need to broadcast <code class="docutils literal notranslate"><span class="pre">target_padding_mask</span></code>’s last
dimension to have the same dimensions as <code class="docutils literal notranslate"><span class="pre">future_mask</span></code>. This means we
broadcast <code class="docutils literal notranslate"><span class="pre">[4,</span> <span class="pre">5]</span></code> to <code class="docutils literal notranslate"><span class="pre">[4,</span> <span class="pre">5,</span> <span class="pre">5]</span></code>. But why?</p></li>
<li><p>For simplicity, we slice the first same of both below.</p></li>
<li><p>The first row of the <code class="docutils literal notranslate"><span class="pre">future_mask</span></code> of the first sample is <code class="docutils literal notranslate"><span class="pre">T,</span> <span class="pre">F,</span> <span class="pre">F,</span> <span class="pre">F,</span> <span class="pre">F</span></code>.
This corresponds to what? This is the future mask of the first token in the
sequence. Well, that is confusing, because it apparently have 5 elements,
and has “information” of the other 4 tokens in the sequence. Let’s explain
in details below:</p>
<ul>
<li><p>Regarding the first row of the <code class="docutils literal notranslate"><span class="pre">future_mask</span></code> in the first sample, which
is <code class="docutils literal notranslate"><span class="pre">[T,</span> <span class="pre">F,</span> <span class="pre">F,</span> <span class="pre">F,</span> <span class="pre">F]</span></code>, it might initially seem confusing why there are 5
elements. Each of these elements, in fact, corresponds to whether the
first token can attend to other tokens at each respective position in
the sequence. Here’s how to interpret it:</p>
<ul>
<li><p>The first element (<code class="docutils literal notranslate"><span class="pre">True</span></code>) indicates that the first token can attend
to itself.</p></li>
<li><p>The next four elements (<code class="docutils literal notranslate"><span class="pre">False</span></code>) specify that the first token should
not attend to any of the future tokens in the sequence.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Consequently, what is the first token in the sequence of the
<code class="docutils literal notranslate"><span class="pre">target_padding_mask</span></code>? Recall earlier we mentioned that the first sample’s
<code class="docutils literal notranslate"><span class="pre">target_padding_mask</span></code> is <code class="docutils literal notranslate"><span class="pre">T,</span> <span class="pre">T,</span> <span class="pre">T,</span> <span class="pre">F,</span> <span class="pre">F</span></code> and therefore the first token in
the sequence is <code class="docutils literal notranslate"><span class="pre">T</span></code>.</p></li>
<li><p>What do we want to achieve here? We want to make sure that the model does
not <strong>attend</strong> to tokens in the sequence that are masked with <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>In other words, the first token in the sequence of the first sample has
<code class="docutils literal notranslate"><span class="pre">target_padding_mask</span></code> of <code class="docutils literal notranslate"><span class="pre">T</span></code> and <code class="docutils literal notranslate"><span class="pre">future_masks</span></code> of <code class="docutils literal notranslate"><span class="pre">T,</span> <span class="pre">F,</span> <span class="pre">F,</span> <span class="pre">F,</span> <span class="pre">F</span></code>.</p></li>
<li><p>We need to broadcast this <code class="docutils literal notranslate"><span class="pre">T</span></code> to <code class="docutils literal notranslate"><span class="pre">T,</span> <span class="pre">T,</span> <span class="pre">T,</span> <span class="pre">T,</span> <span class="pre">T</span></code> to align with
<code class="docutils literal notranslate"><span class="pre">T,</span> <span class="pre">F,</span> <span class="pre">F,</span> <span class="pre">F,</span> <span class="pre">F</span></code> because? Because we need ensure that this first token in the
sequence is also able to considered in relation to every other token in the
sequence.</p></li>
<li><p>So the first token is not a padded token, which is <code class="docutils literal notranslate"><span class="pre">T</span></code>, similarly, the first
token needs to attend to itself at the first position, hence <code class="docutils literal notranslate"><span class="pre">T</span></code> and <code class="docutils literal notranslate"><span class="pre">T</span></code>
give <code class="docutils literal notranslate"><span class="pre">T</span></code>. But for the second <code class="docutils literal notranslate"><span class="pre">T</span></code> in the now broadcasted
<code class="docutils literal notranslate"><span class="pre">target_padding_mask</span></code>, it is still representing the first token or?</p></li>
<li><p>Broadcasting the first token’s <code class="docutils literal notranslate"><span class="pre">target_padding_mask</span></code> value of <code class="docutils literal notranslate"><span class="pre">T</span></code> to
<code class="docutils literal notranslate"><span class="pre">[T,</span> <span class="pre">T,</span> <span class="pre">T,</span> <span class="pre">T,</span> <span class="pre">T]</span></code> ensures that when this first token is being considered for
attention computations, it is free to attend to any position, barring any
restrictions set by <code class="docutils literal notranslate"><span class="pre">future_mask</span></code>.</p></li>
<li><p>Tricky: after broadcasting, each <code class="docutils literal notranslate"><span class="pre">T</span></code> in <code class="docutils literal notranslate"><span class="pre">[T,</span> <span class="pre">T,</span> <span class="pre">T,</span> <span class="pre">T,</span> <span class="pre">T]</span></code> is still
representing the first token. They indicate that when the first token is
compared with <em>any</em> token in the sequence (including itself), it is not a
padding token. The element-wise <code class="docutils literal notranslate"><span class="pre">AND</span></code> with the <code class="docutils literal notranslate"><span class="pre">future_mask</span></code> then further
refines this by restricting it from attending to future tokens.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pprint</span><span class="p">(</span><span class="n">target_padding_mask</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">target_padding_mask</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">target_padding_mask</span> <span class="o">=</span> <span class="n">target_padding_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">))</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">target_padding_mask</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">target_padding_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">]]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]]])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">torch.Size</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span><span style="font-weight: bold">])</span>
</pre>
</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pprint</span><span class="p">(</span><span class="n">target_padding_mask</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">future_mask</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">target_padding_mask</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&amp;</span> <span class="n">future_mask</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">]])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]])</span>
</pre>
</div></div>
</div>
</section>
<section id="first-sample-fourth-token">
<h4><a class="toc-backref" href="#id24" role="doc-backlink">First Sample Fourth Token</a><a class="headerlink" href="#first-sample-fourth-token" title="Link to this heading">#</a></h4>
<p>Now let’s look at another example—the 4th token in the sequence, where
<code class="docutils literal notranslate"><span class="pre">target_padding_mask</span> <span class="pre">=</span> <span class="pre">[T,</span> <span class="pre">T,</span> <span class="pre">T,</span> <span class="pre">F,</span> <span class="pre">F]</span></code> and <code class="docutils literal notranslate"><span class="pre">future_mask</span></code> is a lower triangular
matrix with <code class="docutils literal notranslate"><span class="pre">True</span></code>s.</p>
<ol class="arabic simple">
<li><p><strong>4th Token’s target_padding_mask</strong>: The 4th token has a value of <code class="docutils literal notranslate"><span class="pre">F</span></code> in
<code class="docutils literal notranslate"><span class="pre">target_padding_mask</span></code>, indicating it’s a padding token.</p></li>
<li><p><strong>4th Row of future_mask</strong>: The 4th row in <code class="docutils literal notranslate"><span class="pre">future_mask</span></code> is
<code class="docutils literal notranslate"><span class="pre">[True,</span> <span class="pre">True,</span> <span class="pre">True,</span> <span class="pre">True,</span> <span class="pre">False]</span></code>. This means that if this token were not a
padding token, it would be allowed to attend to all the previous tokens in
the sequence and itself, but not to any future token.</p></li>
<li><p><strong>Broadcast target_padding_mask</strong>: To align <code class="docutils literal notranslate"><span class="pre">target_padding_mask</span></code> with
<code class="docutils literal notranslate"><span class="pre">future_mask</span></code>, we’d broadcast <code class="docutils literal notranslate"><span class="pre">F</span></code> from the <code class="docutils literal notranslate"><span class="pre">target_padding_mask</span></code> to
<code class="docutils literal notranslate"><span class="pre">[F,</span> <span class="pre">F,</span> <span class="pre">F,</span> <span class="pre">F,</span> <span class="pre">F]</span></code>. This way, when we consider the 4th token in relation to
any other token in the sequence, it’s still marked as a padding token.</p></li>
<li><p><strong>Element-wise AND with future_mask</strong>: After broadcasting, you’d perform an
element-wise AND between <code class="docutils literal notranslate"><span class="pre">[F,</span> <span class="pre">F,</span> <span class="pre">F,</span> <span class="pre">F,</span> <span class="pre">F]</span></code> and
<code class="docutils literal notranslate"><span class="pre">[True,</span> <span class="pre">True,</span> <span class="pre">True,</span> <span class="pre">True,</span> <span class="pre">False]</span></code>, resulting in <code class="docutils literal notranslate"><span class="pre">[F,</span> <span class="pre">F,</span> <span class="pre">F,</span> <span class="pre">F,</span> <span class="pre">F]</span></code>.</p></li>
<li><p><strong>Interpretation</strong>: This effectively means that the 4th token won’t attend to
any other token in the sequence, and no token will attend to it either, as it
is a padding token.</p></li>
</ol>
<p>So, the masks are doing their jobs correctly: the <code class="docutils literal notranslate"><span class="pre">target_padding_mask</span></code>
indicates whether each token is a padding token or not, and <code class="docutils literal notranslate"><span class="pre">future_mask</span></code>
dictates the “rules” of attention regarding what each token can attend to.
Combining them ensures that both conditions are met.</p>
</section>
</section>
<section id="further-add-a-singleton-dimension-in-target-masks">
<h3><a class="toc-backref" href="#id25" role="doc-backlink">Further Add a Singleton Dimension in Target Masks</a><a class="headerlink" href="#further-add-a-singleton-dimension-in-target-masks" title="Link to this heading">#</a></h3>
<p>Now both masks are of shape: <code class="docutils literal notranslate"><span class="pre">(B,</span> <span class="pre">L,</span> <span class="pre">L)</span></code> but we need to add a singleton
dimension to the last dimension to make it <code class="docutils literal notranslate"><span class="pre">(B,</span> <span class="pre">1,</span> <span class="pre">L,</span> <span class="pre">L)</span></code>.</p>
<p>In deep learning frameworks like PyTorch, the dimensions of the tensors involved
in operations like matrix multiplication or attention mechanisms often have
specific semantic meanings. In the context of attention mechanisms, especially
in the transformer architecture, the attention mask usually has a shape that is
compatible with the attention logits for element-wise multiplication.</p>
<p>In the transformer model, the attention logits are often computed as a dot
product between query and key vectors, resulting in a tensor of shape
<code class="docutils literal notranslate"><span class="pre">(Batch</span> <span class="pre">size,</span> <span class="pre">Num</span> <span class="pre">heads,</span> <span class="pre">Sequence</span> <span class="pre">length,</span> <span class="pre">Sequence</span> <span class="pre">length)</span></code> or <code class="docutils literal notranslate"><span class="pre">(B,</span> <span class="pre">H,</span> <span class="pre">L,</span> <span class="pre">L)</span></code>.
Here, <code class="docutils literal notranslate"><span class="pre">B</span></code> is the batch size, <code class="docutils literal notranslate"><span class="pre">H</span></code> is the number of attention heads, and <code class="docutils literal notranslate"><span class="pre">L</span></code> is
the sequence length.</p>
<p>To make the mask tensor compatible for element-wise operations with this 4D
tensor, it needs to have a shape that can be broadcasted to <code class="docutils literal notranslate"><span class="pre">(B,</span> <span class="pre">H,</span> <span class="pre">L,</span> <span class="pre">L)</span></code>. A
mask of shape <code class="docutils literal notranslate"><span class="pre">(B,</span> <span class="pre">1,</span> <span class="pre">L,</span> <span class="pre">L)</span></code> fulfills this requirement.</p>
<p>The singleton dimension is added so that the mask can be easily broadcast to the
shape of the attention logits tensor during the computation. When a tensor with
shape <code class="docutils literal notranslate"><span class="pre">(B,</span> <span class="pre">1,</span> <span class="pre">L,</span> <span class="pre">L)</span></code> is element-wise multiplied with a tensor of shape
<code class="docutils literal notranslate"><span class="pre">(B,</span> <span class="pre">H,</span> <span class="pre">L,</span> <span class="pre">L)</span></code>, the singleton dimension (the <code class="docutils literal notranslate"><span class="pre">1</span></code>) allows the mask to be used for
each attention head without explicitly replicating the mask <code class="docutils literal notranslate"><span class="pre">H</span></code> times. This is
more memory-efficient and often faster.</p>
<p>Thus, adding a singleton dimension in masks is a preparatory step that allows
for efficient element-wise operations later in the model’s forward pass.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">target_padding_mask</span> <span class="o">=</span> <span class="n">target_padding_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">target_padding_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">future_mask</span> <span class="o">=</span> <span class="n">future_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">future_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">target_mask</span> <span class="o">=</span> <span class="n">target_padding_mask</span> <span class="o">&amp;</span> <span class="n">future_mask</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">target_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">torch.Size</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span><span style="font-weight: bold">])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">torch.Size</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span><span style="font-weight: bold">])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">torch.Size</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span><span style="font-weight: bold">])</span>
</pre>
</div></div>
</div>
</section>
<section id="why-mask-our-target-in-adder">
<h3><a class="toc-backref" href="#id26" role="doc-backlink">Why mask our target in Adder?</a><a class="headerlink" href="#why-mask-our-target-in-adder" title="Link to this heading">#</a></h3>
<p>If you see the source code of how the <code class="docutils literal notranslate"><span class="pre">AdderDataset</span></code> is constructed, you will
see that we masked out all the tokens before (and including) the equal sign.</p>
<p>For example, if our sequence is <code class="docutils literal notranslate"><span class="pre">12+97=109</span></code>, the input sequence will be
tokenized to the following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">input</span> <span class="o">=</span> <span class="p">[</span><span class="n">BOS</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">+</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="o">=</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">9</span><span class="p">]</span>
<span class="n">target</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">+</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="o">=</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="n">EOS</span><span class="p">]</span>
</pre></div>
</div>
<p>What our code below does is to mask out the tokens before the equal sign for the
target sequence.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">target</span> <span class="o">=</span> <span class="p">[</span><span class="n">MASK</span><span class="p">,</span> <span class="n">MASK</span><span class="p">,</span> <span class="n">MASK</span><span class="p">,</span> <span class="n">MASK</span><span class="p">,</span> <span class="n">MASK</span><span class="p">,</span> <span class="n">MASK</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="n">EOS</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">construct_target_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_sequence</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">:</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">input_sequence</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="n">where_equal_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">input_sequence</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">equal_token_id</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">where_equal_index</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">where_equal_index</span><span class="p">)</span>  <span class="c1"># to appease mypy lol</span>
    <span class="n">target</span><span class="p">[:</span> <span class="n">where_equal_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_token_id</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">target</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
</pre></div>
</div>
<p>Simply put, we do not care what the model predict for anything before the equal
sign. By masking out (or ignoring) the tokens before the =, we are asking the
model to “focus” on generating the correct answer after the equal sign.</p>
</section>
<section id="split-to-train-valid-test">
<h3><a class="toc-backref" href="#id27" role="doc-backlink">Split to Train-Valid-Test</a><a class="headerlink" href="#split-to-train-valid-test" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">batch_size</span>   <span class="o">=</span> <span class="mi">256</span>

<span class="n">composer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">train_loader</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_size</span>
<span class="n">composer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">valid_loader</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_size</span>
<span class="n">composer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">test_loader</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_size</span>

<span class="n">train_dataset</span><span class="p">,</span> <span class="n">valid_dataset</span><span class="p">,</span> <span class="n">test_dataset</span> <span class="o">=</span> <span class="n">split_dataset</span><span class="p">(</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">split</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">global_</span><span class="o">.</span><span class="n">seed</span>
<span class="p">)</span>

<span class="n">train_size</span><span class="p">,</span> <span class="n">valid_size</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">)</span>
<span class="n">train_size</span><span class="p">,</span> <span class="n">valid_size</span><span class="p">,</span> <span class="n">test_size</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(7000, 2000, 1000)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># max_seq_len is determined by 1+ num_digits + 1 + num_digits + 1 + num_digits + 1 + 1</span>
<span class="c1"># where the 1s represent BOS, Plus sign, Equal sign, the extra digit in the sum, EOS, respectively.</span>
<span class="n">max_seq_len</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">composer</span><span class="o">.</span><span class="n">constants</span><span class="o">.</span><span class="n">NUM_DIGITS</span> <span class="o">+</span> <span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">constants</span><span class="o">.</span><span class="n">NUM_DIGITS</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">max_seq_len</span> <span class="o">==</span> <span class="n">composer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">context_length</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="create-dataloader">
<h3><a class="toc-backref" href="#id28" role="doc-backlink">Create DataLoader</a><a class="headerlink" href="#create-dataloader" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_loader</span> <span class="o">=</span> <span class="n">create_loader</span><span class="p">(</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">loader_config</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">train_loader</span><span class="p">,</span>
    <span class="n">collate_fn_config</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">valid_loader</span> <span class="o">=</span> <span class="n">create_loader</span><span class="p">(</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">valid_dataset</span><span class="p">,</span>
    <span class="n">loader_config</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">valid_loader</span><span class="p">,</span>
    <span class="n">collate_fn_config</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">test_loader</span> <span class="o">=</span> <span class="n">create_loader</span><span class="p">(</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">test_dataset</span><span class="p">,</span>
    <span class="n">loader_config</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">test_loader</span><span class="p">,</span>
    <span class="n">collate_fn_config</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">collate_fn</span></code> defines how to combine these variable-length samples into a
batch. This usually involves padding the sequences in the batch to a common
length, which is typically the length of the longest sequence in the batch. Note
here the padding in collate is “redundant” since in our earlier code we ensured
that all sample has same number of characters by way of padding zeros in front.
For example, <code class="docutils literal notranslate"><span class="pre">23</span> <span class="pre">+</span> <span class="pre">3</span> <span class="pre">=26</span></code> will become <code class="docutils literal notranslate"><span class="pre">23</span> <span class="pre">+</span> <span class="pre">03</span> <span class="pre">=</span> <span class="pre">026</span></code>. Consequently, all samples
in the mini-batch will have same length by definition.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">global_</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>

<span class="n">batch_index</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
    <span class="c1"># Each batch is a tuple containing all elements for the batch</span>
    <span class="n">inputs_padded</span><span class="p">,</span> <span class="n">targets_padded</span><span class="p">,</span> <span class="n">padding_masks_padded_and_expanded</span><span class="p">,</span> <span class="n">future_masks_expanded</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># Print the length of each component in the batch</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Batch Size:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs_padded</span><span class="p">))</span>

    <span class="c1"># Now you can print shapes or other properties of each batch element</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Inputs Shape:&quot;</span><span class="p">,</span> <span class="n">inputs_padded</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Targets Shape:&quot;</span><span class="p">,</span> <span class="n">targets_padded</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="c1"># Decoding and other processing can be done here</span>
    <span class="c1"># For example, decoding the first sequence in the batch</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Decoded First Equation/Sample of the Batch:&quot;</span><span class="p">,</span> <span class="n">decode_equation</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">,</span> <span class="n">inputs_padded</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()))</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>

    <span class="n">batch_index</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">batch_index</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span> <span class="k">break</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Batch Size: 256
Inputs Shape: torch.Size([256, 10])
Targets Shape: torch.Size([256, 10])
Decoded First Equation/Sample of the Batch: 31+04=035
--------------------------------------------------------------------------------
Batch Size: 256
Inputs Shape: torch.Size([256, 10])
Targets Shape: torch.Size([256, 10])
Decoded First Equation/Sample of the Batch: 37+49=086
--------------------------------------------------------------------------------
Batch Size: 256
Inputs Shape: torch.Size([256, 10])
Targets Shape: torch.Size([256, 10])
Decoded First Equation/Sample of the Batch: 47+26=073
--------------------------------------------------------------------------------
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Batch Size: 256
Inputs Shape: torch.Size([256, 10])
Targets Shape: torch.Size([256, 10])
Decoded First Equation/Sample of the Batch: 53+05=058
--------------------------------------------------------------------------------
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="model">
<h2><a class="toc-backref" href="#id29" role="doc-backlink">Model</a><a class="headerlink" href="#model" title="Link to this heading">#</a></h2>
<p>We have went into extensive details on the implementation of the decoder in the
<a class="reference internal" href="implementation.html"><span class="std std-doc">implementation section</span></a>. We will not repeat the concepts
here, instead we will just compile the model with the configurations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create individual component configurations</span>
<span class="n">masked_self_attention_mha_config</span> <span class="o">=</span> <span class="n">MultiHeadedAttentionConfig</span><span class="p">(</span>
     <span class="n">attention</span><span class="o">=</span><span class="n">ScaledDotProductAttention</span><span class="p">(),</span>
    <span class="n">d_model</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">H</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span>
<span class="p">)</span>

<span class="n">feed_forward_config</span> <span class="o">=</span> <span class="n">PositionwiseFeedForwardConfig</span><span class="p">(</span>
    <span class="n">d_model</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">d_ff</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(</span><span class="n">approximate</span><span class="o">=</span><span class="s2">&quot;tanh&quot;</span><span class="p">),</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="n">add_norm_config_1</span> <span class="o">=</span> <span class="n">AddNormConfig</span><span class="p">(</span><span class="n">feature_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">add_norm_config_2</span> <span class="o">=</span> <span class="n">AddNormConfig</span><span class="p">(</span><span class="n">feature_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Create DecoderBlockConfig</span>
<span class="n">decoder_block_config</span> <span class="o">=</span> <span class="n">DecoderBlockConfig</span><span class="p">(</span>
    <span class="n">masked_self_attention_mha</span><span class="o">=</span><span class="n">masked_self_attention_mha_config</span><span class="p">,</span>
    <span class="n">feed_forward</span><span class="o">=</span><span class="n">feed_forward_config</span><span class="p">,</span>
    <span class="n">add_norm_1</span><span class="o">=</span><span class="n">add_norm_config_1</span><span class="p">,</span>
    <span class="n">add_norm_2</span><span class="o">=</span><span class="n">add_norm_config_2</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Create the overall DecoderConfig</span>
<span class="n">model_config</span> <span class="o">=</span> <span class="n">DecoderConfig</span><span class="p">(</span>
    <span class="n">d_model</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">context_length</span><span class="o">=</span><span class="n">max_seq_len</span><span class="p">,</span>
    <span class="n">num_decoder_blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">decoder_block</span><span class="o">=</span><span class="n">decoder_block_config</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">GPTDecoder</span><span class="p">(</span><span class="n">model_config</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="n">model_size</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">total_trainable_parameters</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;model_size: </span><span class="si">{</span><span class="n">model_size</span><span class="si">}</span><span class="s1">, train_set_size: </span><span class="si">{</span><span class="n">train_size</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>model_size: 270226, train_set_size: 7000
</pre></div>
</div>
</div>
</div>
</section>
<section id="training-paradigm">
<h2><a class="toc-backref" href="#id30" role="doc-backlink">Training Paradigm</a><a class="headerlink" href="#training-paradigm" title="Link to this heading">#</a></h2>
<p>Here, we would list some of the training paradigms that we would be using in
this project.</p>
<section id="optimizer">
<h3><a class="toc-backref" href="#id31" role="doc-backlink">Optimizer</a><a class="headerlink" href="#optimizer" title="Link to this heading">#</a></h3>
<p>We start off by defining the optimizer for GPT-2. A common choice used is the
<a class="reference external" href="https://arxiv.org/abs/1412.6980">Adam</a> <span id="id1">[<a class="reference internal" href="../../bibliography.html#id24" title="Diederik P. Kingma and Jimmy Ba. Adam: a method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [Submitted on 22 Dec 2014 (v1)]. URL: https://arxiv.org/abs/1412.6980.">Kingma and Ba, 2014</a>]</span> or
<a class="reference external" href="https://arxiv.org/abs/1711.05101">AdamW</a> <span id="id2">[<a class="reference internal" href="../../bibliography.html#id23" title="Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [Submitted on 14 Nov 2017 (v1)]. URL: https://arxiv.org/abs/1711.05101.">Loshchilov and Hutter, 2017</a>]</span>. We
conveniently take the configuration provided in Karpathy’s
<a class="reference external" href="https://github.com/karpathy/nanoGPT">nanoGPT</a>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\eta_{\max} &amp;= 6 \times 10^{-4} \\
\beta_1 &amp;= 0.9 \\
\beta_2 &amp;= 0.95 \\
\epsilon &amp;= 10^{-8} \\
\lambda &amp;= 10^{-1}
\end{aligned}
\end{split}\]</div>
<p>Furthermore, we briefly mention that Karpathy applies weight decay to different
parameter groups - which is quite a common practice. As we can see from the code
below, we define whitelisted and blacklisted modules that we want to apply
weight decay to. The whitelist modules are <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> and the blacklist modules
are <code class="docutils literal notranslate"><span class="pre">nn.LayerNorm</span></code>, <code class="docutils literal notranslate"><span class="pre">nn.Embedding</span></code>.</p>
<p>Weight decay, which is basically L2 regularization penalizes the square of the
weights, encouraging smaller weight values. This can lead to a “spreading out”
effect, as it discourages the model from relying too heavily on a small number
of input features, promoting a more even distribution of weight values and, by
extension, a more balanced consideration of input dimensions. This
regularization technique is particularly beneficial for layers that perform
matrix multiplication, as it helps in ensuring that the model utilizes a broader
range of input features rather than becoming overly dependent on a few dominant
ones. We can find more intuition in the discussion
<a class="reference external" href="https://stats.stackexchange.com/questions/576463/why-not-perform-weight-decay-on-layernorm-embedding">Why not perform weight decay on layernorm/embedding?</a>,
<a class="reference external" href="https://discuss.pytorch.org/t/weight-decay-in-the-optimizers-is-a-bad-idea-especially-with-batchnorm/16994">Weight decay in the optimizers is a bad idea (especially with BatchNorm)</a>
and
<a class="reference external" href="https://github.com/karpathy/minGPT/pull/24#issuecomment-679316025">Weight decay exclusions (Karpathy)</a>.</p>
<div class="highlight-md notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span>def apply_weight_decay_to_different_param_groups(
<span class="linenos"> 2</span>    model: nn.Module, weight_decay: float
<span class="linenos"> 3</span>) -&gt; List[Dict[Literal[&quot;params&quot;, &quot;weight_decay&quot;], List[torch.nn.Parameter] | float]]:
<span class="linenos"> 4</span>    decay: Set[str] = set()
<span class="linenos"> 5</span>    no_decay: Set[str] = set()
<span class="hll"><span class="linenos"> 6</span>    whitelist_weight_modules: Tuple[Type[nn.Module], ...] = (nn.Linear,)
</span><span class="hll"><span class="linenos"> 7</span>    blacklist_weight_modules: Tuple[Type[nn.Module], ...] = (nn.LayerNorm, nn.Embedding, LayerNorm)
</span><span class="linenos"> 8</span>
<span class="linenos"> 9</span>    for module_name, module in model.named_modules():
<span class="linenos">10</span>        for parameter_name, <span class="ge">_parameter in module.named_</span>parameters():
<span class="linenos">11</span>            full_parameter_name = f&quot;{module_name}.{parameter_name}&quot; if module_name else parameter_name
<span class="linenos">12</span>            if parameter_name.endswith(&quot;bias&quot;):
<span class="linenos">13</span>                # biases of all modules are not decayed
<span class="linenos">14</span>                no_decay.add(full_parameter_name)
<span class="linenos">15</span>            elif parameter_name.endswith(&quot;weight&quot;) and isinstance(module, whitelist_weight_modules):
<span class="linenos">16</span>                # weights of whitelisted modules are decayed
<span class="linenos">17</span>                decay.add(full_parameter_name)
<span class="linenos">18</span>            elif parameter_name.endswith(&quot;in_proj_weight&quot;):
<span class="linenos">19</span>                # MHA projection layer, does not exist in my implementation
<span class="linenos">20</span>                decay.add(full_parameter_name)
<span class="linenos">21</span>            elif parameter_name.endswith(&quot;weight&quot;) and isinstance(module, blacklist_weight_modules):
<span class="linenos">22</span>                # weights of blacklisted modules are not decayed
<span class="linenos">23</span>                no_decay.add(full_parameter_name)
<span class="linenos">24</span>            elif (parameter_name.endswith(&quot;gamma&quot;) or parameter_name.endswith(&quot;beta&quot;)) and isinstance(
<span class="linenos">25</span>                module, LayerNorm
<span class="linenos">26</span>            ):
<span class="linenos">27</span>                # weights of LayerNorm modules are not decayed
<span class="linenos">28</span>                # TODO: why do I need to do this is because my custom LayerNorm has gamma and beta
<span class="linenos">29</span>                # as their &quot;weight&quot; and &quot;bias&quot; attributes, respectively.
<span class="linenos">30</span>                no_decay.add(full_parameter_name)
<span class="linenos">31</span>            elif parameter_name.endswith(&quot;pos_embed&quot;):
<span class="linenos">32</span>                no_decay.add(full_parameter_name)
<span class="linenos">33</span>
<span class="linenos">34</span>    param_dict = {parameter_name: parameter for parameter_name, parameter in model.named_parameters()}  # noqa: C416
<span class="linenos">35</span>    inter_params = decay &amp; no_decay
<span class="linenos">36</span>    union_params = decay | no_decay
<span class="linenos">37</span>    assert not inter_params, f&quot;Parameters {inter_params} are in both decay and no_decay sets.&quot;
<span class="linenos">38</span>    assert not (
<span class="linenos">39</span>        param_dict.keys() - union_params
<span class="linenos">40</span>    ), f&quot;Parameters {param_dict.keys() - union_params} were not categorized.&quot;
<span class="linenos">41</span>
<span class="linenos">42</span>    optim_groups: List[Dict[Literal[&quot;params&quot;, &quot;weight_decay&quot;], List[torch.nn.Parameter] | float]] = [
<span class="linenos">43</span>        {&quot;params&quot;: [param_dict[parameter_name] for parameter_name in sorted(decay)], &quot;weight_decay&quot;: weight_decay},
<span class="linenos">44</span>        {&quot;params&quot;: [param_dict[parameter_name] for parameter_name in sorted(no_decay)], &quot;weight_decay&quot;: 0.0},
<span class="linenos">45</span>    ]
<span class="linenos">46</span>
<span class="linenos">47</span>    return optim_groups
</pre></div>
</div>
<p>We won’t go into too much technical rigour on the optimizer, but note that more
modern variations exist, for instance
<a class="reference external" href="https://docs.mosaicml.com/projects/composer/en/stable/api_reference/generated/composer.optim.DecoupledAdamW.html">DecoupledAdamW</a>,
which furthers decouple the weight decay term <span class="math notranslate nohighlight">\(\lambda\)</span> from the learning rate,
as well <a class="reference external" href="https://arxiv.org/abs/1908.03265">RAdam</a> <span id="id3">[<a class="reference internal" href="../../bibliography.html#id25" title="Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265, 2019. [Submitted on 8 Aug 2019 (v1), last revised 26 Oct 2021 (this version, v4)]. URL: https://arxiv.org/abs/1908.03265.">Liu <em>et al.</em>, 2019</a>]</span>, which
is intended to address bias correction factors leading to higher variance in the
adaptive learning rate for the initial training iterations.</p>
<p>To this end, we create the optimizer in code as follows, noting that we would
not use the exact same configuration as Karpathy, but rather use what is
deemed fit for the case at hand.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pprint</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">optimizer</span><span class="p">)</span>
<span class="n">optimizer_config_cls</span> <span class="o">=</span> <span class="n">OPTIMIZER_REGISTRY</span><span class="p">[</span><span class="n">composer</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">name</span><span class="p">]</span>
<span class="n">optimizer_pydantic_config</span> <span class="o">=</span> <span class="n">optimizer_config_cls</span><span class="p">(</span><span class="o">**</span><span class="n">composer</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">model_dump</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s2">&quot;python&quot;</span><span class="p">))</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">optimizer_pydantic_config</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">AdamConfig</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">name</span>=<span style="color: #008000; text-decoration-color: #008000">'torch.optim.Adam'</span>, <span style="color: #808000; text-decoration-color: #808000">lr</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2</span>, <span style="color: #808000; text-decoration-color: #808000">betas</span>=<span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.98</span><span style="font-weight: bold">)</span>, <span style="color: #808000; text-decoration-color: #808000">eps</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1e-09</span>, <span style="color: #808000; text-decoration-color: #808000">weight_decay</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0</span><span style="font-weight: bold">)</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">AdamConfig</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">name</span>=<span style="color: #008000; text-decoration-color: #008000">'torch.optim.Adam'</span>, <span style="color: #808000; text-decoration-color: #808000">lr</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2</span>, <span style="color: #808000; text-decoration-color: #808000">betas</span>=<span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.98</span><span style="font-weight: bold">)</span>, <span style="color: #808000; text-decoration-color: #808000">eps</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1e-09</span>, <span style="color: #808000; text-decoration-color: #808000">weight_decay</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0</span><span style="font-weight: bold">)</span>
</pre>
</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="s2">&quot;weight_decay&quot;</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer_pydantic_config</span><span class="o">.</span><span class="n">build</span><span class="p">(</span>
    <span class="n">params</span><span class="o">=</span><span class="n">apply_weight_decay_to_different_param_groups</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">weight_decay</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Adam <span style="font-weight: bold">(</span>
Parameter Group <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>amsgrad: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>betas: <span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.98</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>capturable: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>differentiable: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>eps: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1e-09</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>foreach: <span style="color: #800080; text-decoration-color: #800080; font-style: italic">None</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>fused: <span style="color: #800080; text-decoration-color: #800080; font-style: italic">None</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>lr: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>maximize: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>weight_decay: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0</span>

Parameter Group <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>amsgrad: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>betas: <span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.98</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>capturable: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>differentiable: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>eps: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1e-09</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>foreach: <span style="color: #800080; text-decoration-color: #800080; font-style: italic">None</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>fused: <span style="color: #800080; text-decoration-color: #800080; font-style: italic">None</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>lr: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>maximize: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>weight_decay: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0</span>
<span style="font-weight: bold">)</span>
</pre>
</div></div>
</div>
</section>
<section id="learning-rate-scheduler">
<h3><a class="toc-backref" href="#id32" role="doc-backlink">Learning Rate Scheduler</a><a class="headerlink" href="#learning-rate-scheduler" title="Link to this heading">#</a></h3>
<section id="id4">
<h4><a class="toc-backref" href="#id33" role="doc-backlink">Motivation</a><a class="headerlink" href="#id4" title="Link to this heading">#</a></h4>
<p>In training deep neural networks, learning rate is definitely one of the most
important parameter to tune. Optimization algorithms like
<a class="reference external" href="https://arxiv.org/abs/1412.6980">Adam</a> and
<a class="reference external" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">SGD</a> tell us how the
weights <span class="math notranslate nohighlight">\(\boldsymbol{\theta} \in \boldsymbol{\Theta}\)</span> should be updated, but the
learning rate <span class="math notranslate nohighlight">\(\eta\)</span> tells us the <strong><em>rate</em></strong> at which the weights are being
updated.</p>
<p>Theoretically and empircally, the <strong><em>magnitude</em></strong> of the learning rate <span class="math notranslate nohighlight">\(\eta\)</span>
can have a significant impact on the training process. If the learning rate is
too <em>large</em>, we might experience
<a class="reference external" href="https://en.wikipedia.org/wiki/Divergence_(mathematics)">divergence</a>, on the
other hand, if the learning rate is too <em>small</em>, the model might take longer to
converge or might get stuck in a local
<a class="reference external" href="https://en.wikipedia.org/wiki/Maxima_and_minima">minima</a>. The condition number
of the problem also impacts optimization efficiency, as discussed in
<a class="reference external" href="https://d2l.ai/chapter_optimization/momentum.html#sec-momentum">the momentum section</a>,
where the concept can be understood as the ratio between the smallest and
largest changes possible in response to adjustments in different directions of
the parameter space, reflecting the variance in sensitivity across these
directions[^1] <span id="id5">[<a class="reference internal" href="../../bibliography.html#id4" title="Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola. Dive into Deep Learning. Cambridge University Press, 2023. URL: https://D2L.ai.">Zhang <em>et al.</em>, 2023</a>]</span>. As we progress through the training steps,
it is also equally important to apply a learning rate scheduler to adjust (may
<em>not</em> be monotonous decay) the learning rate <em>discriminatively</em>.</p>
<p>In the paper
<a class="reference external" href="https://arxiv.org/abs/1608.03983"><em>SGDR: Stochastic Gradient Descent with Restarts</em></a>
by Loshchilov and Hutter, they introduced an heuristic that relies on the
empirical observation that we can improve the convergence of the model (usually
in ill-conditioned situations) if we want follow an <strong><em>annealing</em></strong> process over
the learning rate. This means that at the beginning of training, we do not want
to decrease the learning too drastically. My (potentially wrong) intuition is
that this may allow the model to consider exploring a larger parameter space
without too much constraints than if we were to rapidly decrease the learning
rate. The authors further claim that as we progress towards the end of the
training, we would want to “fine-tune” the model parameters with a very small
learning rate, as it could potentially help “refine” the solution space to find
a “more optimal” set of parameters <span id="id6">[<a class="reference internal" href="../../bibliography.html#id26" title="Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with restarts. CoRR, 2016. URL: http://arxiv.org/abs/1608.03983, arXiv:1608.03983.">Loshchilov and Hutter, 2016</a>]</span>.
This idea <em>naturally lands</em> us to using <em>cosine function</em> because the cosine
curve starts with a <em>gentle slope</em>, which coincides with the idea of <em>gradual
decrease</em> in learning rate in the beginning, and the cosine curve naturally
flattens and approaches zero towards the end as it reaches the end of its cycle,
which again coincides with the idea of <em>fine-tuning</em> the model parameters with a
very small learning rate.</p>
<p>Consequently, a cosine decaying scheduler has the below function form for
learning rates in the range <span class="math notranslate nohighlight">\(t \in [0, T]\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\eta_t=\eta_T+\frac{\eta_0-\eta_T}{2}\left(1+\cos \left(\frac{\pi t}{T}\right)\right)
\]</div>
<p>Here <span class="math notranslate nohighlight">\(\eta_0\)</span> is the initial learning rate, <span class="math notranslate nohighlight">\(\eta_T\)</span> is the target rate at time
<span class="math notranslate nohighlight">\(T\)</span>. Furthermore, for <span class="math notranslate nohighlight">\(t&gt;T\)</span> we simply pin the value to <span class="math notranslate nohighlight">\(\eta_T\)</span> without
increasing it again. <span class="math notranslate nohighlight">\(T\)</span> represents the end of the learning rate annealing phase
rather than the absolute end of training. It’s the point in time when the
learning rate reaches <span class="math notranslate nohighlight">\(\eta_T\)</span>, the target rate, and beyond which the learning
rate is maintained constant at <span class="math notranslate nohighlight">\(\eta_T\)</span>.</p>
<ul class="simple">
<li><p>During <span class="math notranslate nohighlight">\(0 \leq t &lt; T\)</span>: The learning rate <span class="math notranslate nohighlight">\(\eta_t\)</span> is actively adjusted
according to the cosine annealing formula. It transitions from the initial
learning rate <span class="math notranslate nohighlight">\(\eta_0\)</span> towards the target rate <span class="math notranslate nohighlight">\(\eta_T\)</span>, following a
half-cosine wave.</p></li>
<li><p>For <span class="math notranslate nohighlight">\(t \geq T\)</span>: The learning rate is set to <span class="math notranslate nohighlight">\(\eta_T\)</span> and no longer changes.
This doesn’t necessarily mean that training must stop at <span class="math notranslate nohighlight">\(t = T\)</span>. Training
can continue beyond <span class="math notranslate nohighlight">\(T\)</span> with the learning rate fixed at <span class="math notranslate nohighlight">\(\eta_T\)</span>.</p></li>
</ul>
<p>In code, we can observe the behavior of the cosine annealing scheduler as
follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">List</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Optimizer</span>
<span class="kn">from</span> <span class="nn">torch.optim.lr_scheduler</span> <span class="kn">import</span> <span class="n">CosineAnnealingLR</span><span class="p">,</span> <span class="n">_LRScheduler</span>

<span class="k">def</span> <span class="nf">get_learning_rates</span><span class="p">(</span><span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">:</span> <span class="n">_LRScheduler</span><span class="p">,</span> <span class="n">steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
    <span class="n">lrs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
        <span class="n">lrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">])</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">lrs</span>

<span class="k">def</span> <span class="nf">plot_learning_rates</span><span class="p">(</span>
    <span class="n">lrs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="n">title</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">marker</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="p">:</span> <span class="n">plt</span><span class="o">.</span><span class="n">Axes</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span> <span class="ow">or</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lrs</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">title</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="n">marker</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Step&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Learning Rate&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">initial_lr</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">eta_min</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">steps</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">initial_lr</span><span class="p">)</span>
    <span class="n">scheduler_non_cyclic</span> <span class="o">=</span> <span class="n">CosineAnnealingLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="n">steps</span><span class="p">,</span> <span class="n">eta_min</span><span class="o">=</span><span class="n">eta_min</span><span class="p">)</span>
    <span class="n">lrs_non_cyclic</span> <span class="o">=</span> <span class="n">get_learning_rates</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler_non_cyclic</span><span class="p">,</span> <span class="n">steps</span><span class="p">)</span>

    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">initial_lr</span><span class="p">)</span>
    <span class="n">scheduler_cyclic</span> <span class="o">=</span> <span class="n">CosineAnnealingLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="n">steps</span> <span class="o">//</span> <span class="mi">8</span><span class="p">,</span> <span class="n">eta_min</span><span class="o">=</span><span class="n">eta_min</span><span class="p">)</span>
    <span class="n">lrs_cyclic</span> <span class="o">=</span> <span class="n">get_learning_rates</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler_cyclic</span><span class="p">,</span> <span class="n">steps</span><span class="p">)</span>

    <span class="c1"># Plotting</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">plot_learning_rates</span><span class="p">(</span><span class="n">lrs_non_cyclic</span><span class="p">,</span> <span class="s1">&#39;Non-Cyclic Cosine Annealing&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">plot_learning_rates</span><span class="p">(</span><span class="n">lrs_cyclic</span><span class="p">,</span> <span class="s1">&#39;Cyclic Cosine Annealing&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/46d0e2ac76509421dd652bfd3e5230ec9afa76c07d7397aa40e3975fbad62fcb.png" src="../../_images/46d0e2ac76509421dd652bfd3e5230ec9afa76c07d7397aa40e3975fbad62fcb.png" />
</div>
</div>
</section>
<section id="warmup">
<h4><a class="toc-backref" href="#id34" role="doc-backlink">Warmup</a><a class="headerlink" href="#warmup" title="Link to this heading">#</a></h4>
<p>Our motivation would have ended here, but in practice, we often see that the
cosine annealing scheduler is often combined with a warmup phase. In
<a class="reference internal" href="../../playbook/why_cosine_annealing_warmup_stabilize_training.html#why-cosine-warmup-loss-plot"><span class="std std-numref">Fig. 5</span></a>, we can see that the loss curve is
relatively smooth and converges way better than the ones without warmup.</p>
<figure class="align-default" id="why-cosine-warmup-loss-plot-duplicate">
<img alt="../../_images/warmup_loss_plot_uvadlc.svg" src="../../_images/warmup_loss_plot_uvadlc.svg" /><figcaption>
<p><span class="caption-number">Fig. 4 </span><span class="caption-text">Training loss v.s. # of iterations of Transformers on the De-En IWSLT’14 dataset.</span><a class="headerlink" href="#why-cosine-warmup-loss-plot-duplicate" title="Link to this image">#</a></p>
<div class="legend">
<p><strong>Image Credit:</strong>
<a class="reference external" href="https://arxiv.org/pdf/1908.03265.pdf">ON THE VARIANCE OF THE ADAPTIVE LEARNING RATE AND BEYOND</a></p>
</div>
</figcaption>
</figure>
<p>It might be worth having some intuition on why warmup works so well in practice,
and in particular, in language models like
<a class="reference external" href="https://arxiv.org/abs/1706.03762">Transformers</a>.</p>
<p>Firstly, the <a class="reference external" href="https://arxiv.org/pdf/1908.03265.pdf">RAdam</a> paper suggests warmup
works as a variance reduction technique, which overcomes the problem of
<a class="reference external" href="https://stats.stackexchange.com/questions/232741/why-is-it-important-to-include-a-bias-correction-term-for-the-adam-optimizer-for">bias correction factors</a>
in optimizers like Adam, where having these bias correction factors would lead
to larger variance in the adaptive learning rate during the <strong>initial</strong> training
iterations <span id="id7">[<a class="reference internal" href="../../bibliography.html#id17" title="Phillip Lippe. UvA Deep Learning Tutorials. https://uvadlc-notebooks.readthedocs.io/en/latest/, 2023.">Lippe, 2023</a>]</span>. More concretely, Adam estimates the first
and second moments of the gradient to change the learning rate of each
individual parameter (hence adaptive) and having high variance between adaptive
learning rates may de-stablize the training. If we don’t want to swap out Adam,
then this calls for a warmup phase to stabilize the learning rate and reduce the
variance in the early stages of training.</p>
<p>Secondly, language models like Transformers use iteratively applied Layer
Normalization across layers can lead to very high gradients during the first
iterations, which can be solved by using
<a class="reference external" href="https://proceedings.icml.cc/static/paper_files/icml/2020/328-Paper.pdf">Pre-Layer Normalization</a>
(similar to Pre-Activation ResNet), which applies normalization before the
layer’s main operations, contributing to gradient stabilization and reducing the
necessity for a warm-up phase, or replacing Layer Normalization by other
techniques
(<a class="reference external" href="https://proceedings.icml.cc/static/paper_files/icml/2020/328-Paper.pdf">Adaptive Normalization</a>,
<a class="reference external" href="https://arxiv.org/abs/2003.07845">Power Normalization</a>)
<span id="id8">[<a class="reference internal" href="../../bibliography.html#id17" title="Phillip Lippe. UvA Deep Learning Tutorials. https://uvadlc-notebooks.readthedocs.io/en/latest/, 2023.">Lippe, 2023</a>]</span>.</p>
<p>However, even though there are solutions to the problem, certain setups still
use the Adam optimizer, and therefore warmup is still a simple and effective
technique to stabilize the learning rate in the early stages of training -
solving the afforementioned problems (i.e. stabilize the bias correction
factors, moving averages of gradients and squared gradients).</p>
<p>To this end, we end our discussion on the motivation behind 1) using cosine
annealing schedulers and 2) using warmup phases, often coupled with cosine
annealing schedulers. In what follows, we will provide a more formal definition
of the cosine annealing scheduler with warmup, and provide a running example to
illustrate the behavior of the scheduler.</p>
</section>
<section id="definition">
<h4><a class="toc-backref" href="#id35" role="doc-backlink">Definition</a><a class="headerlink" href="#definition" title="Link to this heading">#</a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">CosineAnnealingWithWarmupScheduler</span></code> decays the learning rate <span class="math notranslate nohighlight">\(\eta\)</span>
according to the decreasing part of a cosine curve, with an initial warmup
<span class="math notranslate nohighlight">\(t_{\text{warmup}}\)</span>.</p>
<p>This scheduler modulates <span class="math notranslate nohighlight">\(\eta\)</span> within defined upper and lower bounds over a
predetermined interval, employing a cosine function. The formula for cosine
annealing reflects the shape of a half-cosine wave, which decreases from a
maximum value to a minimum and then increases back to the maximum. This cycle
can repeat multiple times over the training process, depending on how the
scheduler is configured. Although this approach suggests cyclic adjustments
(oscillations) within the training duration, for simplicity’s sake, our specific
implementation, inspired by
<a class="reference external" href="https://docs.mosaicml.com/projects/composer/en/latest/api_reference/generated/composer.optim.CosineAnnealingWithWarmupScheduler.html"><strong>MosaicML’s Composer’s CosineAnnealingWithWarmupScheduler</strong></a>,
explicitly excludes considerations for such cycles/oscillations.</p>
<div class="proof definition admonition" id="why-do-we-use-warmup-cosine-scheduler-definition-duplicate">
<p class="admonition-title"><span class="caption-number">Definition 15 </span> (Cosine Annealing With Warmup)</p>
<section class="definition-content" id="proof-content">
<p>The <code class="docutils literal notranslate"><span class="pre">CosineAnnealingWithWarmupScheduler</span></code> modulates the <strong>learning rate</strong> <span class="math notranslate nohighlight">\(\eta\)</span>
according to a <strong>two-phase</strong> process: a <strong><em>warmup</em></strong> phase followed by a
<strong>cosine annealing</strong> phase. The learning rate <em>multiplier</em>[^lr-multiplier]
<span class="math notranslate nohighlight">\(\alpha_{t}\)</span> at any given time (step) <span class="math notranslate nohighlight">\(t\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation}
\alpha_{t} = \begin{cases}
    \frac{t}{t_{\text{warmup}}}, &amp; \text{if } t &lt; t_{\text{warmup}} \\
    \alpha_f + (1 - \alpha_f) \times \frac{1}{2} \left[1 + \cos(\pi \times \tau_w) \right], &amp; \text{otherwise}
\end{cases}
\end{equation}
\end{split}\]</div>
<p>where we denote:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(t\)</span> represents the <strong>current</strong> training step or epoch.</p></li>
<li><p><span class="math notranslate nohighlight">\(\eta_{\max}\)</span> as the <strong>maximum</strong> learning rate reached during training, and
often is the <strong>initial</strong> learning rate given into an optimizer.</p></li>
<li><p><span class="math notranslate nohighlight">\(t_{\text{warmup}}\)</span> denotes the duration of the warmup period, in terms of
the number of steps or epochs, during which the learning rate <strong>linearly</strong>
increases to the maximum learning rate <span class="math notranslate nohighlight">\(\eta_{\max}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(t_{\max}\)</span> as the <strong>maximum</strong> number of training steps, or maximum number of
iterations in an epoch (see
<a class="reference external" href="https://github.com/skorch-dev/skorch/issues/610">here</a>).</p></li>
<li><p><span class="math notranslate nohighlight">\(\tau_w = \frac{t - t_{\text{warmup}}}{t_{\max}}\)</span>, the fraction of
post-warmup time elapsed,</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha_f\)</span> is a <em>scaling</em> factor that determines the <strong>final</strong> learning rate
multiplier to decay to (a value between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>), and this is a <em>fixed</em>
value. For example, if <span class="math notranslate nohighlight">\(\alpha_f = 0.1\)</span> and the initial learning rate is
<span class="math notranslate nohighlight">\(\eta_{\max} = 3e-4\)</span>, then the final learning rate will be
<span class="math notranslate nohighlight">\(\eta_{\min} = 3e-4 \times 0.1 = 3e-5\)</span>.</p></li>
</ul>
<p>The actual learning rate <span class="math notranslate nohighlight">\(\eta_{t}\)</span> at time (step) <span class="math notranslate nohighlight">\(t\)</span> is then computed as:</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
    \eta_{t} = \alpha_{t} \times \eta_{\max}
\end{equation}
\]</div>
<p>where we emphasize again that <span class="math notranslate nohighlight">\(\eta_{\max}\)</span> is the <strong>maximum</strong> learning rate
reached during training.</p>
</section>
</div><div class="note admonition">
<p class="admonition-title">A Word on Oscillations</p>
<p>Note that if you set <span class="math notranslate nohighlight">\(t_{\max}\)</span> to the total number of training steps that is
needed for the entire dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>, the scheduler <em>will only decay</em> the
learning rate after the warmup phase and not oscillate further. This
configuration means that after completing the linear increase during the warmup,
the learning rate will decrease following a cosine curve until it reaches the
final learning rate determined by <span class="math notranslate nohighlight">\(\alpha_f\)</span>.</p>
<ul class="simple">
<li><p><strong>Single Cycle (No Oscillation)</strong>: If <span class="math notranslate nohighlight">\(t_{\max}\)</span> is set to cover exactly one
half-cycle of the cosine function from the end of the warmup phase to the
conclusion of training, the learning rate will monotonically decrease from
its maximum value (at the end of warmup) to its minimum value (as determined
by <span class="math notranslate nohighlight">\(\alpha_f\)</span>) without oscillating. This is because the scheduler’s active
period only spans a single descent phase of the cosine wave.</p></li>
<li><p><strong>Multiple Cycles (Oscillation)</strong>: If <span class="math notranslate nohighlight">\(t_{\max}\)</span> is set to allow for a
longer duration than what is needed for a single half-cycle descent, the
cosine annealing function can complete its initial descent and then begin to
ascend as part of a new cycle. This leads to oscillations in the learning
rate—after decreasing, it will start to increase again, potentially multiple
times, depending on the total number of cycles fitted within <span class="math notranslate nohighlight">\(t_{\max}\)</span>.
This is where the term “oscillation” comes into play; it describes the
periodic increase and decrease in the learning rate according to the cosine
function over multiple cycles.</p></li>
</ul>
<p>True oscillation, where the learning rate decreases and then increases within a
training regime, typically requires either a restart mechanism (as seen in
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html">Cosine Annealing with Warm Restarts</a>)
or an explicit multi-cycle configuration. A standard cosine annealing scheduler,
especially with a warmup phase, generally only supports a monotonic decrease
within a single cycle, unless it is specifically designed to handle restarts or
multiple cycles.</p>
</div>
</section>
<section id="implementation">
<h4><a class="toc-backref" href="#id36" role="doc-backlink">Implementation</a><a class="headerlink" href="#implementation" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="kn">from</span> <span class="nn">torch.optim.lr_scheduler</span> <span class="kn">import</span> <span class="n">LambdaLR</span>
<span class="kn">from</span> <span class="nn">torch.optim.optimizer</span> <span class="kn">import</span> <span class="n">Optimizer</span>

<span class="k">def</span> <span class="nf">_get_cosine_schedule_with_warmup_lr_lambda</span><span class="p">(</span>
    <span class="n">current_step</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">num_warmup_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_training_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">alpha_f</span><span class="p">:</span> <span class="nb">float</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper function for calculating the learning rate using cosine annealing</span>
<span class="sd">    with warmup.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    current_step: int</span>
<span class="sd">        The current step in the training process.</span>
<span class="sd">    num_warmup_steps: int</span>
<span class="sd">        The number of steps for the warmup phase.</span>
<span class="sd">    num_training_steps: int</span>
<span class="sd">        The total number of training steps.</span>
<span class="sd">    alpha_f: float</span>
<span class="sd">        The minimum learning rate at the end of the schedule.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    float</span>
<span class="sd">        The calculated learning rate.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">current_step</span> <span class="o">&lt;</span> <span class="n">num_warmup_steps</span><span class="p">:</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">current_step</span> <span class="o">/</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_warmup_steps</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">tau_w</span> <span class="o">=</span> <span class="p">(</span><span class="n">current_step</span> <span class="o">-</span> <span class="n">num_warmup_steps</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_training_steps</span>
        <span class="n">tau_w</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">tau_w</span><span class="p">)</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha_f</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha_f</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">tau_w</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="k">return</span> <span class="n">alpha</span>


<span class="k">def</span> <span class="nf">get_cosine_annealing_with_warmup</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>
    <span class="n">num_warmup_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_training_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">alpha_f</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">last_epoch</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LambdaLR</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create a schedule with a learning rate that decreases following the values</span>
<span class="sd">    of the cosine function between the initial lr set in the optimizer to 0,</span>
<span class="sd">    after a warmup period during which it increases linearly between 0 and the</span>
<span class="sd">    initial lr set in the optimizer.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    optimizer: `~torch.optim.Optimizer`</span>
<span class="sd">        The optimizer for which to schedule the learning rate.</span>
<span class="sd">    num_warmup_steps: int</span>
<span class="sd">        The number of steps for the warmup phase.</span>
<span class="sd">    num_training_steps: int</span>
<span class="sd">        The total number of training steps.</span>
<span class="sd">    alpha_f: float</span>
<span class="sd">        The minimum learning rate at the end of the schedule, by default 0.1.</span>
<span class="sd">    last_epoch: int</span>
<span class="sd">        The index of the last epoch when resuming training, by default -1.</span>
<span class="sd">    verbose: bool</span>
<span class="sd">        Whether to print the learning rate at every update, by default False.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    `torch.optim.lr_scheduler.LambdaLR`</span>
<span class="sd">        The scheduler with the appropriate schedule.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from torch import nn</span>
<span class="sd">    &gt;&gt;&gt; from torch.optim import Adam</span>
<span class="sd">    &gt;&gt;&gt; dummy_model = nn.Linear(1, 1)</span>
<span class="sd">    &gt;&gt;&gt; optimizer = Adam(dummy_model.parameters(), lr=3e-4)</span>
<span class="sd">    &gt;&gt;&gt; scheduler = get_cosine_annealing_with_warmup(optimizer, num_warmup_steps=5, num_training_steps=10, alpha_f=0.5)</span>
<span class="sd">    &gt;&gt;&gt; assert isinstance(scheduler, LambdaLR)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">lr_lambda</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
        <span class="n">_get_cosine_schedule_with_warmup_lr_lambda</span><span class="p">,</span>
        <span class="n">num_warmup_steps</span><span class="o">=</span><span class="n">num_warmup_steps</span><span class="p">,</span>
        <span class="n">num_training_steps</span><span class="o">=</span><span class="n">num_training_steps</span><span class="p">,</span>
        <span class="n">alpha_f</span><span class="o">=</span><span class="n">alpha_f</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">LambdaLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">lr_lambda</span><span class="p">,</span> <span class="n">last_epoch</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_warmup_steps</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
<span class="n">num_training_steps</span> <span class="o">=</span> <span class="n">composer</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">max_epochs</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span> <span class="o">//</span> <span class="n">composer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">train_loader</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">])</span>
<span class="n">alpha_f</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># as if no decay</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">get_cosine_annealing_with_warmup</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">num_warmup_steps</span><span class="o">=</span><span class="n">num_warmup_steps</span><span class="p">,</span> <span class="n">num_training_steps</span><span class="o">=</span><span class="n">num_training_steps</span><span class="p">,</span> <span class="n">alpha_f</span><span class="o">=</span><span class="n">alpha_f</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">omnivault.transformer.core.scheduler</span> <span class="kn">import</span> <span class="n">noam_lr_decay</span>
<span class="n">warmup_steps</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>

<span class="c1"># lr first increases in the warmup steps, and then decays</span>
<span class="n">noam</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">step</span><span class="p">:</span> <span class="n">noam_lr_decay</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="o">=</span><span class="n">warmup_steps</span><span class="p">)</span>  <span class="c1"># noqa: E731</span>

<span class="n">scheduler_config_cls</span> <span class="o">=</span> <span class="n">SCHEDULER_REGISTRY</span><span class="p">[</span><span class="n">cfg</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">name</span><span class="p">]</span>

<span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">scheduler_config_cls</span><span class="p">,</span> <span class="n">LambdaLRConfig</span><span class="p">):</span>
    <span class="n">scheduler_pydantic_config</span> <span class="o">=</span> <span class="n">scheduler_config_cls</span><span class="p">(</span><span class="n">lr_lambda</span><span class="o">=</span><span class="n">noam</span><span class="p">,</span> <span class="o">**</span><span class="n">cfg</span><span class="o">.</span><span class="n">scheduler</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">scheduler_pydantic_config</span> <span class="o">=</span> <span class="n">scheduler_config_cls</span><span class="p">(</span><span class="o">**</span><span class="n">cfg</span><span class="o">.</span><span class="n">scheduler</span><span class="p">)</span>  <span class="c1"># type: ignore[assignment]</span>

<span class="n">composer</span><span class="o">.</span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">scheduler_pydantic_config</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">scheduler_pydantic_config</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="criterion">
<h3><a class="toc-backref" href="#id37" role="doc-backlink">Criterion</a><a class="headerlink" href="#criterion" title="Link to this heading">#</a></h3>
<p>The Cross Entropy Loss function calculates the difference between two
probability distributions - the predicted probability distribution output by the
model (logits) and the actual distribution (target labels). It’s primarily used
in classification tasks involving <span class="math notranslate nohighlight">\(C\)</span> classes.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{B}\)</span> : Denotes batch size,</p></li>
<li><p><span class="math notranslate nohighlight">\(K\)</span> : The number of additional dimensions beyond batch and class,
representing spatial or other feature dimensions in the input tensor,</p></li>
<li><p><span class="math notranslate nohighlight">\(N=\mathcal{B} \times d_1 \times \ldots \times d_K\)</span> : Total count of
individual elements across all dimensions, including batch and spatial
dimensions. This value adjusts as per the dimensional complexity:</p></li>
<li><p>For <span class="math notranslate nohighlight">\(K=0, N=\mathcal{B}\)</span>,</p></li>
<li><p>For <span class="math notranslate nohighlight">\(K=1, N=\mathcal{B} \times d_1\)</span>,</p></li>
<li><p>For <span class="math notranslate nohighlight">\(K&gt;1, N\)</span> scales accordingly.</p></li>
<li><p><span class="math notranslate nohighlight">\(C\)</span> : The total number of classification categories,</p></li>
<li><p><span class="math notranslate nohighlight">\(x\)</span> : Represents the input logits tensor,</p></li>
<li><p><span class="math notranslate nohighlight">\(y\)</span> : Denotes the target tensor,</p></li>
<li><p><span class="math notranslate nohighlight">\(w\)</span> : An optional tensor assigning weights to each class,</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{L}\)</span> : Symbolizes the aggregate loss prior to any reduction,</p></li>
<li><p><span class="math notranslate nohighlight">\(l_b\)</span> : The loss corresponding to the <span class="math notranslate nohighlight">\(b\)</span> th element within the batch,
ranging over <span class="math notranslate nohighlight">\(b=1\)</span> to <span class="math notranslate nohighlight">\(\mathcal{B}\)</span>.</p></li>
</ul>
<section id="inputs-and-targets">
<h4><a class="toc-backref" href="#id38" role="doc-backlink">Inputs and Targets</a><a class="headerlink" href="#inputs-and-targets" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Inputs (Logits)</strong>: The function expects unnormalized logits for each class
per input. These logits do not necessarily need to be positive values nor
sum to 1. The shape of the input tensor can be:</p>
<ul>
<li><p>For unbatched input: <span class="math notranslate nohighlight">\((C)\)</span>,</p></li>
<li><p>For batched input: <span class="math notranslate nohighlight">\((\mathcal{B}, C)\)</span>,</p></li>
<li><p>For <span class="math notranslate nohighlight">\(K\)</span>-dimensional input: <span class="math notranslate nohighlight">\((\mathcal{B}, C, d_1, d_2, \ldots, d_K)\)</span>,
suitable for tasks like pixel-wise classification in images where
<span class="math notranslate nohighlight">\(K \geq 1\)</span>.</p></li>
</ul>
</li>
<li><p><strong>Targets</strong>: When configuring the targets for the Cross Entropy Loss
function, their expected shapes vary based on the nature of the targets
(class indices vs. probabilities) and the dimensionality of the input:</p>
<ul>
<li><p><strong>For Class Indices as Targets</strong>:</p>
<ul>
<li><p>Unbatched input: The shape should be a scalar representing a single
class index in <span class="math notranslate nohighlight">\([0, C)\)</span>.</p></li>
<li><p>Batched input: The shape should be <span class="math notranslate nohighlight">\((\mathcal{B},)\)</span>, where each
element is a class index for the corresponding input in the batch.</p></li>
<li><p><span class="math notranslate nohighlight">\(K\)</span>-dimensional input: The shape should be
<span class="math notranslate nohighlight">\((\mathcal{B}, d_1, d_2, \ldots, d_K)\)</span> for the <span class="math notranslate nohighlight">\(K\)</span>-dimensional case,
with each element representing a class index for the corresponding
spatial location.</p></li>
</ul>
</li>
<li><p><strong>For Probabilities as Targets</strong> (applicable in advanced scenarios like
label smoothing or multi-label classification):</p>
<ul>
<li><p>The shape of the targets must match the shape of the input logits
tensor: <span class="math notranslate nohighlight">\((\mathcal{B}, C)\)</span> for batched input or
<span class="math notranslate nohighlight">\((\mathcal{B}, C,
d_1, d_2, \ldots, d_K)\)</span> for
<span class="math notranslate nohighlight">\(K\)</span>-dimensional input. Each element in this tensor should be a
probability corresponding to the likelihood of the class, with
values in <span class="math notranslate nohighlight">\([0, 1]\)</span>.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="loss-computation">
<h4><a class="toc-backref" href="#id39" role="doc-backlink">Loss Computation</a><a class="headerlink" href="#loss-computation" title="Link to this heading">#</a></h4>
<ol class="arabic">
<li><p><strong>For Class Indices as Targets</strong>:</p>
<p>The loss for each element <span class="math notranslate nohighlight">\(n\)</span>, accurately spanning across all considered
dimensions, is calculated as:</p>
<div class="math notranslate nohighlight">
\[
    \ell(x, y) = \mathcal{L} = \{l_1, \ldots, l_{N}\}^{\top}, \quad l_n = -w_{y_n} \cdot \log \left( \frac{\exp(x_{n, y_n})}{\sum_{c=1}^{C} \exp(x_{n, c})} \right) \cdot \mathbb{1}\{y_n \neq \text{ignore_index}\}
    \]</div>
<p>Here, <span class="math notranslate nohighlight">\(N\)</span> correctly reflects the aggregate count of elements when considering
<span class="math notranslate nohighlight">\(\mathcal{B}\)</span> and the <span class="math notranslate nohighlight">\(K\)</span>-dimensional context. Consequently, if <span class="math notranslate nohighlight">\(K=0\)</span>, <span class="math notranslate nohighlight">\(N\)</span>
reduces to <span class="math notranslate nohighlight">\(\mathcal{B}\)</span>.</p>
</li>
<li><p><strong>For Probabilities as Targets</strong>:</p>
<p>In cases where the targets are probabilities, the calculation over each element
<span class="math notranslate nohighlight">\(n\)</span>, aligning with <span class="math notranslate nohighlight">\(N\)</span>’s definition, should be:</p>
<div class="math notranslate nohighlight">
\[
    \ell(x, y) = \mathcal{L} = \{l_1, \ldots, l_{N}\}^{\top}, \quad l_n = -\sum_{c=1}^{C} w_c \cdot y_{n, c} \cdot \log \left( \frac{\exp(x_{n, c})}{\sum_{i=1}^{C} \exp(x_{n, i})} \right)
    \]</div>
</li>
</ol>
</section>
<section id="reduction">
<h4><a class="toc-backref" href="#id40" role="doc-backlink">Reduction</a><a class="headerlink" href="#reduction" title="Link to this heading">#</a></h4>
<ul>
<li><p><strong>No Reduction</strong> (<code class="docutils literal notranslate"><span class="pre">reduction='none'</span></code>):</p>
<p>When the reduction is set to ‘none’, the loss computation preserves the
original dimensionality of the input, effectively returning a tensor that
maps directly to each input element’s loss. This tensor has the shape
<span class="math notranslate nohighlight">\((\mathcal{B}, d_1, \ldots, d_K)\)</span>, where each element <span class="math notranslate nohighlight">\(l_{n}\)</span> within this
tensor represents the computed loss for the corresponding input element
across all dimensions, including the batch and any <span class="math notranslate nohighlight">\(K\)</span>-dimensional space:</p>
<div class="math notranslate nohighlight">
\[
    \mathcal{L} = \{l_1, \ldots, l_N\}
    \]</div>
<p>This preserves the granularity of loss across the dataset, allowing for
detailed analysis or custom reduction post hoc.</p>
</li>
<li><p><strong>Mean Reduction</strong> (<code class="docutils literal notranslate"><span class="pre">reduction='mean'</span></code>):</p>
<p>For the ‘mean’ reduction, the losses across all elements are averaged to
yield a single scalar value. This operation accounts for the total count of
elements (<span class="math notranslate nohighlight">\(N\)</span>), including those spanning batch and additional dimensions,
and is not merely an average over the batch size <span class="math notranslate nohighlight">\(\mathcal{B}\)</span>, but over all
<span class="math notranslate nohighlight">\(N\)</span> elements:</p>
<div class="math notranslate nohighlight">
\[
    \mathcal{L}_{mean} = \frac{1}{N} \sum_{n=1}^{N} l_n
    \]</div>
<p>Here, traditionally we think of <span class="math notranslate nohighlight">\(N\)</span> as just the number of elements in the
batch, but in the implementation, it spans all elements across the batch and
<span class="math notranslate nohighlight">\(K\)</span>-dimensional spaces.</p>
</li>
<li><p><strong>Sum Reduction</strong> (<code class="docutils literal notranslate"><span class="pre">reduction='sum'</span></code>):</p>
<p>With ‘sum’ reduction, the losses for all elements are aggregated into a
single scalar through summation, without averaging. This sums the losses
across all elements, including those across the batch and <span class="math notranslate nohighlight">\(K\)</span>-dimensional
spaces:</p>
<div class="math notranslate nohighlight">
\[
    \mathcal{L}_{sum} = \sum_{n=1}^{N} l_n
    \]</div>
<p>This scalar represents the total loss accumulated across the entire input
set, providing a measure of overall loss magnitude without
normalization by the number of elements.</p>
</li>
</ul>
</section>
<section id="simple-binary-classification-example">
<h4><a class="toc-backref" href="#id41" role="doc-backlink">Simple Binary Classification Example</a><a class="headerlink" href="#simple-binary-classification-example" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">nn.CrossEntropyLoss</span></code> in PyTorch expects the input logits to be of shape
<code class="docutils literal notranslate"><span class="pre">[N,</span> <span class="pre">C,</span> <span class="pre">d1,</span> <span class="pre">d2,</span> <span class="pre">...,</span> <span class="pre">dK]</span></code> (where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the batch size, <code class="docutils literal notranslate"><span class="pre">C</span></code> is the number of
classes, and <code class="docutils literal notranslate"><span class="pre">d1</span></code> to <code class="docutils literal notranslate"><span class="pre">dK</span></code> are optional additional dimensions) and the target
to be of shape <code class="docutils literal notranslate"><span class="pre">[N,</span> <span class="pre">d1,</span> <span class="pre">d2,</span> <span class="pre">...,</span> <span class="pre">dK]</span></code>.</p></li>
<li><p>Let’s look a simplified example in image classification. The target is a
single integer representing the class label, and the input logits are a
vector of length <code class="docutils literal notranslate"><span class="pre">C</span></code> (the number of classes).</p></li>
<li><p>More concretely, in the below example, we have <span class="math notranslate nohighlight">\(\mathcal{B} = 4\)</span> (batch size),
<span class="math notranslate nohighlight">\(C = 2\)</span> (number of classes), <span class="math notranslate nohighlight">\(K = 0\)</span> (no additional dimensions), and <span class="math notranslate nohighlight">\(N = 4\)</span>
(total number of elements across all dimensions).</p>
<ul>
<li><p>Our inputs (logits) are of shape <span class="math notranslate nohighlight">\((\mathcal{B}, C) = (4, 2)\)</span>.</p></li>
<li><p>Our targets are of shape <span class="math notranslate nohighlight">\((\mathcal{B}) = (4)\)</span>.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rng</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">global_</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)</span>
<span class="n">B</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span> <span class="c1"># indicating sample 1 is class 1 and sample 2 is class 0</span>
<span class="n">logits</span>  <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">]])</span>
<span class="k">assert</span> <span class="n">logits</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
<span class="n">loss</span>   <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.5232</span><span style="font-weight: bold">)</span>
</pre>
</div></div>
</div>
</section>
<section id="gpt-example">
<h4><a class="toc-backref" href="#id42" role="doc-backlink">GPT Example</a><a class="headerlink" href="#gpt-example" title="Link to this heading">#</a></h4>
<p>First we set up the criterion for the model. We use the <code class="docutils literal notranslate"><span class="pre">nn.CrossEntropyLoss</span></code>
criterion, which is commonly used for classification tasks.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="n">vocabulary</span><span class="o">.</span><span class="n">token_to_index</span><span class="p">[</span><span class="n">vocabulary</span><span class="o">.</span><span class="n">PAD</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>In scenarios involving classification tasks, targets and inputs (logits) usually
align in a straightforward manner where each target is a single integer that
signifies the class label, and the corresponding input logits form a vector of
length <span class="math notranslate nohighlight">\(C\)</span> (the number of classes).</p>
<p>However, complexity arises when dealing with sequences, such as in decoder
outputs, this is because we are predicting a sequence of class labels for each
token in the sequence. So if a sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> has <span class="math notranslate nohighlight">\(10\)</span> tokens
<span class="math notranslate nohighlight">\(x_1, x_2, \ldots, x_{10}\)</span>, the target is a sequence of class labels
<span class="math notranslate nohighlight">\(y_1, y_2, \ldots, y_{10}\)</span>. While the target shape is still <span class="math notranslate nohighlight">\((\mathcal{B}, T)\)</span>,
and for each sample, you can think of it as <span class="math notranslate nohighlight">\(10\)</span> samples embedded within, each
with a single class label corresponding to the token at that step. Consequently,
the logits shape becomes <span class="math notranslate nohighlight">\((\mathcal{B}, T, \mathcal{V})\)</span>, where <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>
aligns with <span class="math notranslate nohighlight">\(\mathcal{C}\)</span>, the number of classes. In other words, this
counter-intuitive structure can be easily reduced to our normal classification
problem if we remove the batch dimension to have <span class="math notranslate nohighlight">\((T, )\)</span> and <span class="math notranslate nohighlight">\((T, \mathcal{V})\)</span>
respectively for the target and logits - in which case we can treat <span class="math notranslate nohighlight">\(T\)</span> as the
batch size <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> and proceed as usual. To this end, if we want to
introduce the batch dimension back, then a simple idea is to flatten the target
and logits to <span class="math notranslate nohighlight">\((\mathcal{B} \times T, )\)</span> and
<span class="math notranslate nohighlight">\((\mathcal{B} \times T, \mathcal{V})\)</span> respectively. This way, say we have a
batch size of <span class="math notranslate nohighlight">\(2\)</span> and a sequence length of <span class="math notranslate nohighlight">\(3\)</span>, we can easily think of it as <span class="math notranslate nohighlight">\(6\)</span>
samples in total, each with a single class label. Why this idea isn’t obvious to
me at first is because not having the fundamentals from earlier models such as
RNNs and LSTMs.</p>
<p>Let’s consider the example below, with batch size <span class="math notranslate nohighlight">\(\mathcal{B} = 2\)</span>, sequence length <span class="math notranslate nohighlight">\(T = 10\)</span>, and number of classes <span class="math notranslate nohighlight">\(V = 18\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rng</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">global_</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>
<span class="n">input_tokens</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>

<span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">18</span>

<span class="c1"># get only first two samples</span>
<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">input_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">B</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">B</span><span class="p">,</span> <span class="p">:]</span>
<span class="k">assert</span> <span class="n">input_tokens</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">targets</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>

<span class="c1"># get logits</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">logits</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>  <span class="c1"># 2, 10, 18</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">targets</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(torch.Size([20, 18]), torch.Size([20]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logits_flattened</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">targets_flattened</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="k">assert</span> <span class="n">logits_flattened</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span><span class="n">B</span> <span class="o">*</span> <span class="n">T</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">logits_flattened</span><span class="p">,</span> <span class="n">targets_flattened</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.8830</span>, <span style="color: #808000; text-decoration-color: #808000">grad_fn</span>=<span style="font-weight: bold">&lt;</span><span style="color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold">NllLossBackward0</span><span style="font-weight: bold">&gt;)</span>
</pre>
</div></div>
</div>
<p>The second way we will use is essentially the same
as the first way, but more implicit.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logits</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([2, 18, 10])
</pre></div>
</div>
</div>
</div>
<p>We first permute the logits tensor to have the shape
<span class="math notranslate nohighlight">\((\mathcal{B} \times \mathcal{V}, T)\)</span>, why so? Because recall our earlier
definition in PyTorch’s <code class="docutils literal notranslate"><span class="pre">nn.CrossEntropyLoss</span></code> where the input logits should be
of shape <code class="docutils literal notranslate"><span class="pre">[B,</span> <span class="pre">C,</span> <span class="pre">d1,</span> <span class="pre">d2,</span> <span class="pre">...,</span> <span class="pre">dK]</span></code> (where <code class="docutils literal notranslate"><span class="pre">B</span></code> is the batch size, <code class="docutils literal notranslate"><span class="pre">C</span></code> is the
number of classes, and <code class="docutils literal notranslate"><span class="pre">d1</span></code> to <code class="docutils literal notranslate"><span class="pre">dK</span></code> are optional additional dimensions) and the
target to be of shape <code class="docutils literal notranslate"><span class="pre">[B,</span> <span class="pre">d1,</span> <span class="pre">d2,</span> <span class="pre">...,</span> <span class="pre">dK]</span></code>. In our case, we have
<span class="math notranslate nohighlight">\(\mathcal{B} = 2\)</span> (batch size), <span class="math notranslate nohighlight">\(C = 18\)</span> (number of classes), <span class="math notranslate nohighlight">\(K = 1\)</span>
(additional dimension), but however we are in the wrong order of dimension
because PyTorch expects the first dimension to be of batch size, which is fine,
but the second dimension must be the class dimension, which in our case is not
because our second dimension is the sequence length <span class="math notranslate nohighlight">\(T\)</span>. So we swap the second
and third dimension to have the shape <span class="math notranslate nohighlight">\((\mathcal{B}, \mathcal{V}, T)\)</span>, and then
PyTorch’s <code class="docutils literal notranslate"><span class="pre">nn.CrossEntropyLoss</span></code> would then reshape the logits to
<span class="math notranslate nohighlight">\((\mathcal{B} \times T, \mathcal{V})\)</span> and the targets to
<span class="math notranslate nohighlight">\((\mathcal{B} \times T, )\)</span> - coinciding with our earlier discussion.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(),</span> <span class="n">targets</span><span class="o">.</span><span class="n">contiguous</span><span class="p">())</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span> <span class="c1"># same</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.8830</span>, <span style="color: #808000; text-decoration-color: #808000">grad_fn</span>=<span style="font-weight: bold">&lt;</span><span style="color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold">NllLoss2DBackward0</span><span style="font-weight: bold">&gt;)</span>
</pre>
</div></div>
</div>
<p>In a GPT-like model, each token in the sequence is used to predict the next
token. The model takes a sequence of tokens and produces a new sequence of the
same length where each new token is predicted based on all the preceding tokens
in the input sequence. The loss is then computed between the predicted sequence
and the target sequence.</p>
<p>Let’s take a closer look at an example:</p>
<ul class="simple">
<li><p>The original tensor: <code class="docutils literal notranslate"><span class="pre">[15,</span> <span class="pre">9,</span> <span class="pre">0,</span> <span class="pre">10,</span> <span class="pre">3,</span> <span class="pre">8,</span> <span class="pre">13,</span> <span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">8,</span> <span class="pre">14]</span></code> which
corresponds to <code class="docutils literal notranslate"><span class="pre">&lt;BOS&gt;90+38=128&lt;EOS&gt;</span></code></p></li>
<li><p>Input tensor: <code class="docutils literal notranslate"><span class="pre">[15,</span> <span class="pre">9,</span> <span class="pre">0,</span> <span class="pre">10,</span> <span class="pre">3,</span> <span class="pre">8,</span> <span class="pre">13,</span> <span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">8]</span></code>, which corresponds to
<code class="docutils literal notranslate"><span class="pre">&lt;BOS&gt;90+38=128</span></code> without <code class="docutils literal notranslate"><span class="pre">EOS</span></code></p></li>
<li><p>Target tensor: <code class="docutils literal notranslate"><span class="pre">[9,</span> <span class="pre">0,</span> <span class="pre">10,</span> <span class="pre">3,</span> <span class="pre">8,</span> <span class="pre">13,</span> <span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">8,</span> <span class="pre">14]</span></code>
<code class="docutils literal notranslate"><span class="pre">[16,</span> <span class="pre">16,</span> <span class="pre">16,</span> <span class="pre">16,</span> <span class="pre">16,</span> <span class="pre">16,</span> <span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">8,</span> <span class="pre">14]</span></code></p></li>
</ul>
<p>During training:</p>
<ol class="arabic simple">
<li><p><strong>First Timestep</strong>: The model takes <code class="docutils literal notranslate"><span class="pre">[15]</span></code> (or <code class="docutils literal notranslate"><span class="pre">[&lt;BOS&gt;]</span></code> if 15 is your BOS
token) and tries to predict the next token. Ideally, it should predict <code class="docutils literal notranslate"><span class="pre">9</span></code>.
But here, your target sequence starts with masked tokens (<code class="docutils literal notranslate"><span class="pre">16</span></code>, if 16 is your
masking token). So the loss is computed between the predicted token and the
masked token <code class="docutils literal notranslate"><span class="pre">16</span></code>. But since <code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code> has an <code class="docutils literal notranslate"><span class="pre">ignore_index</span></code> (now
you know what they are right!), you can set it to say <code class="docutils literal notranslate"><span class="pre">16</span></code> or (default <code class="docutils literal notranslate"><span class="pre">-1</span></code>
but you would need to change padding number) and tell the model that whenever
the ground truth is <code class="docutils literal notranslate"><span class="pre">16</span></code>, the loss is zeroed out so it is not counted? This
allows the model to focus on learning from the relevant parts of the sequence
while ignoring the masked portions.</p></li>
<li><p><strong>Second Timestep</strong>: The model takes <code class="docutils literal notranslate"><span class="pre">[15,</span> <span class="pre">9]</span></code> and predicts the next token,
which should be <code class="docutils literal notranslate"><span class="pre">0</span></code>. Again, the target is a masked token <code class="docutils literal notranslate"><span class="pre">16</span></code>.</p></li>
<li><p><strong>…</strong></p></li>
<li><p><strong>Eighth Timestep</strong>: The model takes <code class="docutils literal notranslate"><span class="pre">[15,</span> <span class="pre">9,</span> <span class="pre">0,</span> <span class="pre">10,</span> <span class="pre">3,</span> <span class="pre">8,</span> <span class="pre">13]</span></code> (which is
<code class="docutils literal notranslate"><span class="pre">90+38=</span></code>) and predicts the next token. Now the target is <code class="docutils literal notranslate"><span class="pre">1</span></code>, so the loss is
computed between the predicted token and <code class="docutils literal notranslate"><span class="pre">1</span></code>. There is no mask anymore here,
so the loss will be computed.</p></li>
<li><p><strong>Ninth Timestep</strong>: The model takes <code class="docutils literal notranslate"><span class="pre">[15,</span> <span class="pre">9,</span> <span class="pre">0,</span> <span class="pre">10,</span> <span class="pre">3,</span> <span class="pre">8,</span> <span class="pre">13,</span> <span class="pre">1]</span></code> (which is
<code class="docutils literal notranslate"><span class="pre">90+38=1</span></code>) and predicts the next token. Now the target is <code class="docutils literal notranslate"><span class="pre">2</span></code>, so the loss is
computed between the predicted token and <code class="docutils literal notranslate"><span class="pre">2</span></code>.</p>
<ol class="arabic simple">
<li><p>Here’s an important thing for beginners (me), In a typical GPT-like
architecture used for sequence-to-sequence tasks like this one, the model
doesn’t use its own predictions as input during training. Instead, it
uses the original, ground-truth input sequence. This is known as “teacher
forcing.” In teacher forcing, even if the model predicts a wrong token at
some timestep, it doesn’t affect the input sequence for subsequent
timesteps. The model continues to get the original input sequence for the
entire training epoch.</p></li>
<li><p>So if model predicts a <code class="docutils literal notranslate"><span class="pre">3</span></code> during the eighth timestep, where the ground
trut is <code class="docutils literal notranslate"><span class="pre">1</span></code>, the model would simply incur a higher loss for that
prediction. However, the input for the ninth timestep would still be the
ground truth sequence up to that point, regardless of what the model
predicted at the eighth timestep.</p></li>
<li><p>But it is noted that this behaviour is still autoregressive.</p></li>
</ol>
</li>
<li><p><strong>Tenth</strong>: The model takes <code class="docutils literal notranslate"><span class="pre">[15,</span> <span class="pre">9,</span> <span class="pre">0,</span> <span class="pre">10,</span> <span class="pre">3,</span> <span class="pre">8,</span> <span class="pre">13,</span> <span class="pre">1,</span> <span class="pre">2]</span></code> and predicts the
next token which is <code class="docutils literal notranslate"><span class="pre">8</span></code>.</p></li>
<li><p><strong>Last</strong>: The model takes <code class="docutils literal notranslate"><span class="pre">[15,</span> <span class="pre">9,</span> <span class="pre">0,</span> <span class="pre">10,</span> <span class="pre">3,</span> <span class="pre">8,</span> <span class="pre">13,</span> <span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">8]</span></code> and predicts
the next token which is <code class="docutils literal notranslate"><span class="pre">14</span></code> the <code class="docutils literal notranslate"><span class="pre">EOS</span></code>.</p>
<ol class="arabic simple">
<li><p>The reason you need to predict <code class="docutils literal notranslate"><span class="pre">EOS</span></code> is simple intuitively, consider the
case where there’s no need for <code class="docutils literal notranslate"><span class="pre">EOS</span></code>, then the model will not know when
to stop.</p></li>
</ol>
</li>
</ol>
<p>This goes on until the entire sequence is processed. Note that the model never
actually “sees” the target tokens during the prediction. It is solely relying on
the tokens that came before the current token in the input sequence. After the
model makes its prediction, then the predicted tokens are compared to the target
tokens to compute the loss, which is then backpropagated to update the model
weights.</p>
</section>
<section id="a-smaller-example-for-illustration">
<h4><a class="toc-backref" href="#id43" role="doc-backlink">A Smaller Example for Illustration</a><a class="headerlink" href="#a-smaller-example-for-illustration" title="Link to this heading">#</a></h4>
<p>The above example has too big of a dimension, let’s
scale down <span class="math notranslate nohighlight">\(T=10\)</span> to <span class="math notranslate nohighlight">\(T=3\)</span> and <span class="math notranslate nohighlight">\(V=18\)</span> to <span class="math notranslate nohighlight">\(V=4\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># fmt: off</span>
<span class="n">rng</span>        <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">global_</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>

<span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">V</span>    <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span>                                                   <span class="c1"># Assuming we have B = batch size, T = sequence length, V = vocab size</span>

<span class="n">logits</span>     <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>                       <span class="c1"># logits from the head</span>
<span class="n">targets</span>    <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">V</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>  <span class="c1"># targets are the labels</span>
<span class="c1"># fmt: on</span>

<span class="n">pprint</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># logits for the first sequence [T=3, V=4]</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">targets</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># target for the first sequence [T=3]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.9269</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.4873</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9007</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-2.1055</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.6784</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.2345</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0431</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.6047</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3559</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.6866</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.4934</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2415</span><span style="font-weight: bold">]]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.1109</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0915</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-2.3169</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.2168</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.3097</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.3957</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.8034</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.6216</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.5920</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0631</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.8286</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3309</span><span style="font-weight: bold">]]])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span><span style="font-weight: bold">]])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.9269</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.4873</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9007</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-2.1055</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.6784</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.2345</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0431</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.6047</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3559</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.6866</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.4934</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2415</span><span style="font-weight: bold">]])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span><span style="font-weight: bold">])</span>
</pre>
</div></div>
</div>
<p>We establish some conceptual understanding first:</p>
<ul class="simple">
<li><p>Each sample in the batch has the following characteristics:</p>
<ul>
<li><p>Denote <code class="docutils literal notranslate"><span class="pre">target</span></code> and <code class="docutils literal notranslate"><span class="pre">logit</span></code> as the target and logits for a particular
sample in the batch.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">target</span></code> is of shape <code class="docutils literal notranslate"><span class="pre">[T]</span> <span class="pre">=</span> <span class="pre">[3]</span></code> and each element is the class/vocab
label for each token in the sequence.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">logit</span></code> is of shape <code class="docutils literal notranslate"><span class="pre">[T,</span> <span class="pre">V]</span> <span class="pre">=</span> <span class="pre">[3,</span> <span class="pre">4]</span></code> and each row is the logits for
each token in the sequence.</p></li>
<li><p>Therefore, we want to compare each row in <code class="docutils literal notranslate"><span class="pre">logit</span></code> with each element in
<code class="docutils literal notranslate"><span class="pre">target</span></code> to compute the loss.</p></li>
<li><p>We can think of each row in <code class="docutils literal notranslate"><span class="pre">logit</span></code> as the prediction for each token in
the sequence, and each element in <code class="docutils literal notranslate"><span class="pre">target</span></code> as the ground truth for each
token in the sequence.</p></li>
<li><p>Intuitively this means that within each sample, there are many
“sub-samples” where each sub-sample is a token in the sequence. If you
can visualize this, then there should be no confusion.</p></li>
</ul>
</li>
<li><p>In code, we can do so with the following manner:</p>
<ul>
<li><p>Calculate loss for each token in each sample individually and then sum
them up.</p></li>
<li><p>Reduction by mean will mean we need to divide our <code class="docutils literal notranslate"><span class="pre">total_loss</span></code> by the
total number of samples in the batch. But remember that even though
technically we have 2 samples in the batch, we are actually treating
each token in each sample as a sub-sample, so the total samples is
<code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">*</span> <span class="pre">T</span></code> where <code class="docutils literal notranslate"><span class="pre">B</span></code> is the batch size and <code class="docutils literal notranslate"><span class="pre">T</span></code> is the sequence length.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">criterion</span>  <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)</span>

<span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
        <span class="n">logit</span>      <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">target</span>     <span class="o">=</span> <span class="n">targets</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">logit</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="n">pprint</span><span class="p">(</span><span class="n">total_loss</span><span class="p">)</span>
<span class="n">total_loss</span>  <span class="o">=</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="p">(</span><span class="n">B</span> <span class="o">*</span> <span class="n">T</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">total_loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">9.0105</span><span style="font-weight: bold">)</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.5017</span><span style="font-weight: bold">)</span>
</pre>
</div></div>
</div>
<p>In PyTorch however, if you have a logits tensor of shape <code class="docutils literal notranslate"><span class="pre">[B,</span> <span class="pre">S,</span> <span class="pre">V]</span></code>, you need
to permute it to <code class="docutils literal notranslate"><span class="pre">[B,</span> <span class="pre">V,</span> <span class="pre">S]</span></code> to align with the format that <code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code>
wants, so that <code class="docutils literal notranslate"><span class="pre">V</span></code> (vocab size) is treated as <code class="docutils literal notranslate"><span class="pre">C</span></code> (number of classes), and <code class="docutils literal notranslate"><span class="pre">S</span></code>
(sequence length) is treated as one of the additional dimensions
<code class="docutils literal notranslate"><span class="pre">d1,</span> <span class="pre">d2,</span> <span class="pre">...,</span> <span class="pre">dK</span></code>.</p>
<p>But all in all, if you understood the previous loop to calculate the loss for
each token in each sample individually and then sum them up, then dividing to
fulfill reduction of mean, then you should be fine.</p>
</section>
<section id="masking-and-ignore-index">
<h4><a class="toc-backref" href="#id44" role="doc-backlink">Masking and Ignore Index</a><a class="headerlink" href="#masking-and-ignore-index" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># fmt: off</span>
<span class="n">rng</span>        <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">global_</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>

<span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">V</span>    <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span>                                                   <span class="c1"># Assuming we have B = batch size, L = sequence length, V = vocab size</span>

<span class="n">logits</span>     <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>                       <span class="c1"># logits from the head</span>
<span class="n">targets</span>    <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">V</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>  <span class="c1"># targets are the labels</span>
<span class="c1"># fmt: on</span>

<span class="n">pprint</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># logits for the first sequence [L=10, V=18]</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">targets</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># target for the first sequence [L=10]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.9269</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.4873</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9007</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-2.1055</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.6784</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.2345</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0431</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.6047</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3559</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.6866</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.4934</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2415</span><span style="font-weight: bold">]]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.1109</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0915</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-2.3169</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.2168</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.3097</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.3957</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.8034</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.6216</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.5920</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0631</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.8286</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3309</span><span style="font-weight: bold">]]])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span><span style="font-weight: bold">]])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.9269</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.4873</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9007</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-2.1055</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.6784</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.2345</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0431</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.6047</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3559</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.6866</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.4934</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2415</span><span style="font-weight: bold">]])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span><span style="font-weight: bold">])</span>
</pre>
</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">targets</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">123</span>
<span class="n">PAD_</span> <span class="o">=</span> <span class="o">-</span><span class="mi">123</span>

<span class="n">criterion</span>  <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="n">PAD_</span><span class="p">)</span>

<span class="n">NON_IGNORE_COUNT</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
        <span class="n">logit</span>      <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">target</span>     <span class="o">=</span> <span class="n">targets</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">target</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">PAD_</span><span class="p">]):</span>
            <span class="k">continue</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">logit</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">NON_IGNORE_COUNT</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="n">pprint</span><span class="p">(</span><span class="n">total_loss</span><span class="p">)</span>
<span class="n">total_loss</span>  <span class="o">=</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="n">NON_IGNORE_COUNT</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">total_loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6.2188</span><span style="font-weight: bold">)</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.5547</span><span style="font-weight: bold">)</span>
</pre>
</div></div>
</div>
<p>NOTE: <code class="docutils literal notranslate"><span class="pre">NON_IGNORE_COUNT</span></code> is used instead of <code class="docutils literal notranslate"><span class="pre">BxT</span></code>, why? Cause we are averaging
over all non-ignored guys!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Permute logits to shape [B, V, S]</span>
<span class="n">logits_permuted</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Instantiate the CrossEntropyLoss</span>
<span class="c1"># By default, it reduces by averaging the losses over each observation in the input</span>
<span class="n">criterion</span>  <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="n">PAD_</span><span class="p">)</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">logits_permuted</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.5547</span><span style="font-weight: bold">)</span>
</pre>
</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Permute logits to shape [B, V, S]</span>
<span class="n">logits_permuted</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Instantiate the CrossEntropyLoss</span>
<span class="c1"># By default, it reduces by averaging the losses over each observation in the input</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="n">PAD_</span><span class="p">)</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">logits_permuted</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.5547</span><span style="font-weight: bold">)</span>
</pre>
</div></div>
</div>
</section>
<section id="initializing-criterion-with-composer">
<h4><a class="toc-backref" href="#id45" role="doc-backlink">Initializing Criterion With Composer</a><a class="headerlink" href="#initializing-criterion-with-composer" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">omnivault.transformer.config.criterion</span> <span class="kn">import</span> <span class="n">CRITERION_REGISTRY</span>

<span class="n">criterion_config_cls</span> <span class="o">=</span> <span class="n">CRITERION_REGISTRY</span><span class="p">[</span><span class="n">cfg</span><span class="o">.</span><span class="n">criterion</span><span class="o">.</span><span class="n">name</span><span class="p">]</span>
<span class="n">criterion_pydantic_config</span> <span class="o">=</span> <span class="n">criterion_config_cls</span><span class="p">(</span><span class="o">**</span><span class="n">cfg</span><span class="o">.</span><span class="n">criterion</span><span class="p">)</span>

<span class="n">criterion</span> <span class="o">=</span> <span class="n">criterion_pydantic_config</span><span class="o">.</span><span class="n">create_instance</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">criterion</span><span class="o">.</span><span class="n">ignore_index</span> <span class="o">==</span> <span class="n">vocabulary</span><span class="o">.</span><span class="n">token_to_index</span><span class="p">[</span><span class="n">vocabulary</span><span class="o">.</span><span class="n">PAD</span><span class="p">]</span>

<span class="n">pprint</span><span class="p">(</span><span class="n">criterion</span><span class="o">.</span><span class="n">ignore_index</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">criterion</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #008000; text-decoration-color: #008000">'mean'</span>
</pre>
</div></div>
</div>
</section>
</section>
</section>
<section id="state">
<h2><a class="toc-backref" href="#id46" role="doc-backlink">State</a><a class="headerlink" href="#state" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">omnivault.transformer.core.state</span> <span class="kn">import</span> <span class="n">State</span>

<span class="n">state</span> <span class="o">=</span> <span class="n">State</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">criterion</span><span class="o">=</span><span class="n">criterion</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
    <span class="n">scheduler</span><span class="o">=</span><span class="n">scheduler</span><span class="p">,</span>
    <span class="n">vocabulary</span><span class="o">=</span><span class="n">vocabulary</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">state</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">State</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">model</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">GPTDecoder</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">  </span><span style="font-weight: bold">(</span>tok_embed<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Embedding</span><span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">18</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">  </span><span style="font-weight: bold">(</span>decoder_blocks<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">ModuleList</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>-<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span><span style="font-weight: bold">)</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span> x <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">GPTDecoderBlock</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│     </span><span style="font-weight: bold">(</span>masked_self_attention_mha<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">MultiHeadedAttention</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">(</span>W_Q<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Linear</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">in_features</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #808000; text-decoration-color: #808000">out_features</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #808000; text-decoration-color: #808000">bias</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">(</span>W_K<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Linear</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">in_features</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #808000; text-decoration-color: #808000">out_features</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #808000; text-decoration-color: #808000">bias</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">(</span>W_V<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Linear</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">in_features</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #808000; text-decoration-color: #808000">out_features</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #808000; text-decoration-color: #808000">bias</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">(</span>W_O<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Linear</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">in_features</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #808000; text-decoration-color: #808000">out_features</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #808000; text-decoration-color: #808000">bias</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">(</span>attention<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">ScaledDotProductAttention</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │     </span><span style="font-weight: bold">(</span>dropout<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Dropout</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">p</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0</span>, <span style="color: #808000; text-decoration-color: #808000">inplace</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">(</span>dropout<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Dropout</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">p</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1</span>, <span style="color: #808000; text-decoration-color: #808000">inplace</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│     </span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│     </span><span style="font-weight: bold">(</span>feed_forward<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">PositionwiseFeedForward</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">(</span>ffn<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">ModuleDict</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │     </span><span style="font-weight: bold">(</span>context_fc<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Linear</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">in_features</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #808000; text-decoration-color: #808000">out_features</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">256</span>, <span style="color: #808000; text-decoration-color: #808000">bias</span>=<span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │     </span><span style="font-weight: bold">(</span>activation<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">GELU</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">approximate</span>=<span style="color: #008000; text-decoration-color: #008000">'tanh'</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │     </span><span style="font-weight: bold">(</span>context_projection<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Linear</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">in_features</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">256</span>, <span style="color: #808000; text-decoration-color: #808000">out_features</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #808000; text-decoration-color: #808000">bias</span>=<span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │     </span><span style="font-weight: bold">(</span>dropout<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Dropout</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">p</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1</span>, <span style="color: #808000; text-decoration-color: #808000">inplace</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│     </span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│     </span><span style="font-weight: bold">(</span>add_norm_1<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">AddNorm</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">(</span>dropout<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Dropout</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">p</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1</span>, <span style="color: #808000; text-decoration-color: #808000">inplace</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">(</span>layer_norm<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">LayerNorm</span><span style="font-weight: bold">((</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>,<span style="font-weight: bold">)</span>, <span style="color: #808000; text-decoration-color: #808000">eps</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1e-05</span>, <span style="color: #808000; text-decoration-color: #808000">elementwise_affine</span>=<span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│     </span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│     </span><span style="font-weight: bold">(</span>add_norm_2<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">AddNorm</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">(</span>dropout<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Dropout</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">p</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1</span>, <span style="color: #808000; text-decoration-color: #808000">inplace</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">(</span>layer_norm<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">LayerNorm</span><span style="font-weight: bold">((</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>,<span style="font-weight: bold">)</span>, <span style="color: #808000; text-decoration-color: #808000">eps</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1e-05</span>, <span style="color: #808000; text-decoration-color: #808000">elementwise_affine</span>=<span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│     </span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">  </span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">  </span><span style="font-weight: bold">(</span>dropout<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Dropout</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">p</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1</span>, <span style="color: #808000; text-decoration-color: #808000">inplace</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">  </span><span style="font-weight: bold">(</span>layer_norm<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">LayerNorm</span><span style="font-weight: bold">((</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>,<span style="font-weight: bold">)</span>, <span style="color: #808000; text-decoration-color: #808000">eps</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1e-05</span>, <span style="color: #808000; text-decoration-color: #808000">elementwise_affine</span>=<span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">  </span><span style="font-weight: bold">(</span>head<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Linear</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">in_features</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #808000; text-decoration-color: #808000">out_features</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">18</span>, <span style="color: #808000; text-decoration-color: #808000">bias</span>=<span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">)</span>
<span style="font-weight: bold">)</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">criterion</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">CrossEntropyLoss</span><span style="font-weight: bold">()</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">optimizer</span>=<span style="color: #800080; text-decoration-color: #800080">Adam</span> <span style="font-weight: bold">(</span>
Parameter Group <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>amsgrad: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>betas: <span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.98</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>capturable: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>differentiable: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>eps: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1e-09</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>foreach: <span style="color: #800080; text-decoration-color: #800080; font-style: italic">None</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>fused: <span style="color: #800080; text-decoration-color: #800080; font-style: italic">None</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>initial_lr: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>lr: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.2961808030073203e-05</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>maximize: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>weight_decay: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0</span>

Parameter Group <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>amsgrad: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>betas: <span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.98</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>capturable: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>differentiable: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>eps: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1e-09</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>foreach: <span style="color: #800080; text-decoration-color: #800080; font-style: italic">None</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>fused: <span style="color: #800080; text-decoration-color: #800080; font-style: italic">None</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>initial_lr: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>lr: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.2961808030073203e-05</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>maximize: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span>weight_decay: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0</span>
<span style="font-weight: bold">)</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">scheduler</span>=<span style="font-weight: bold">&lt;</span><span style="color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold">torch.optim.lr_scheduler.LambdaLR</span><span style="color: #000000; text-decoration-color: #000000"> object at </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0x7ff56c32d580</span><span style="color: #000000; text-decoration-color: #000000">&gt;,</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">epoch_index</span><span style="color: #000000; text-decoration-color: #000000">=</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span><span style="color: #000000; text-decoration-color: #000000">,</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">train_batch_index</span><span style="color: #000000; text-decoration-color: #000000">=</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span><span style="color: #000000; text-decoration-color: #000000">,</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">step_index</span><span style="color: #000000; text-decoration-color: #000000">=</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span><span style="color: #000000; text-decoration-color: #000000">,</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">history</span><span style="color: #000000; text-decoration-color: #000000">=</span><span style="color: #000000; text-decoration-color: #000000; font-weight: bold">{}</span><span style="color: #000000; text-decoration-color: #000000">,</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">vocabulary</span><span style="color: #000000; text-decoration-color: #000000">=&lt;omnivault.transformer.core.vocabulary.AdderVocabulary object at </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0x7ff4b4024fd0</span><span style="color: #000000; text-decoration-color: #000000">&gt;,</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">tokenizer</span><span style="color: #000000; text-decoration-color: #000000">=&lt;omnivault.transformer.core.tokenizer.AdderTokenizer object at </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0x7ff4b4024c40</span><span style="font-weight: bold">&gt;</span>
<span style="font-weight: bold">)</span>
</pre>
</div></div>
</div>
</section>
<section id="trainer">
<h2><a class="toc-backref" href="#id47" role="doc-backlink">Trainer</a><a class="headerlink" href="#trainer" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">state</span><span class="o">=</span><span class="n">state</span><span class="p">,</span>
    <span class="n">composer</span><span class="o">=</span><span class="n">composer</span><span class="p">,</span>
    <span class="c1"># logger=LOGGER,</span>
    <span class="n">device</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>  <span class="c1"># type: ignore[arg-type]</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">remove_callback</span><span class="p">(</span><span class="n">event</span><span class="o">=</span><span class="n">TrainerEvent</span><span class="o">.</span><span class="n">ON_VALID_EPOCH_END</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">save_state</span><span class="p">)</span>

<span class="n">trainer</span><span class="o">.</span><span class="n">add_callback</span><span class="p">(</span>
    <span class="n">TrainerEvent</span><span class="o">.</span><span class="n">ON_VALID_EPOCH_END</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
    <span class="k">lambda</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">evaluate_and_generate_on_valid_epoch_end</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">num_batches_to_eval</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">_trained_state</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_loader</span><span class="o">=</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="o">=</span><span class="n">valid_loader</span><span class="p">,</span> <span class="n">test_loader</span><span class="o">=</span><span class="n">test_loader</span><span class="p">)</span>
<span class="c1"># _trained_state.pretty_print()</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">_trained_state</span><span class="o">.</span><span class="n">history</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">AssertionError</span><span class="g g-Whitespace">                            </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">59</span><span class="p">],</span> <span class="n">line</span> <span class="mi">13</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span> <span class="n">trainer</span><span class="o">.</span><span class="n">remove_callback</span><span class="p">(</span><span class="n">event</span><span class="o">=</span><span class="n">TrainerEvent</span><span class="o">.</span><span class="n">ON_VALID_EPOCH_END</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">save_state</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">9</span> <span class="n">trainer</span><span class="o">.</span><span class="n">add_callback</span><span class="p">(</span>
<span class="g g-Whitespace">     </span><span class="mi">10</span>     <span class="n">TrainerEvent</span><span class="o">.</span><span class="n">ON_VALID_EPOCH_END</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">11</span>     <span class="k">lambda</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">evaluate_and_generate_on_valid_epoch_end</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">num_batches_to_eval</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
<span class="g g-Whitespace">     </span><span class="mi">12</span> <span class="p">)</span>
<span class="ne">---&gt; </span><span class="mi">13</span> <span class="n">_trained_state</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_loader</span><span class="o">=</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="o">=</span><span class="n">valid_loader</span><span class="p">,</span> <span class="n">test_loader</span><span class="o">=</span><span class="n">test_loader</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">14</span> <span class="c1"># _trained_state.pretty_print()</span>
<span class="g g-Whitespace">     </span><span class="mi">15</span> <span class="n">history</span> <span class="o">=</span> <span class="n">_trained_state</span><span class="o">.</span><span class="n">history</span>

<span class="nn">File ~/work/omniverse/omniverse/omnivault/transformer/core/trainer.py:458,</span> in <span class="ni">Trainer.fit</span><span class="nt">(self, train_loader, valid_loader, test_loader)</span>
<span class="g g-Whitespace">    </span><span class="mi">455</span> <span class="bp">self</span><span class="o">.</span><span class="n">test_loader</span> <span class="o">=</span> <span class="n">test_loader</span>
<span class="g g-Whitespace">    </span><span class="mi">457</span> <span class="c1"># put callback here because depends on dataloader</span>
<span class="ne">--&gt; </span><span class="mi">458</span> <span class="bp">self</span><span class="o">.</span><span class="n">trigger_callbacks</span><span class="p">(</span><span class="n">TrainerEvent</span><span class="o">.</span><span class="n">ON_FIT_START</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">460</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">461</span>     <span class="c1"># fmt: off</span>
<span class="g g-Whitespace">    </span><span class="mi">462</span>     <span class="bp">self</span><span class="o">.</span><span class="n">epoch_index</span> <span class="o">+=</span> <span class="mi">1</span>               <span class="c1"># to match range(1, max_epochs + 1) because we start from 1</span>

<span class="nn">File ~/work/omniverse/omniverse/omnivault/transformer/core/trainer.py:206,</span> in <span class="ni">Trainer.trigger_callbacks</span><span class="nt">(self, event, *args, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">204</span> <span class="n">params</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">callback</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span>
<span class="g g-Whitespace">    </span><span class="mi">205</span> <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">param</span> <span class="ow">in</span> <span class="n">params</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">206</span>     <span class="n">callback</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">207</span> <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">208</span>     <span class="n">callback</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

<span class="nn">File ~/work/omniverse/omniverse/omnivault/transformer/core/callbacks.py:72,</span> in <span class="ni">log_on_fit_start</span><span class="nt">(trainer)</span>
<span class="g g-Whitespace">     </span><span class="mi">69</span> <span class="n">total_params</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">total_parameters</span>
<span class="g g-Whitespace">     </span><span class="mi">70</span> <span class="n">trainable_params</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">total_trainable_parameters</span>
<span class="ne">---&gt; </span><span class="mi">72</span> <span class="k">assert</span> <span class="n">trainer</span><span class="o">.</span><span class="n">composer</span><span class="o">.</span><span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">MISSING</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">composer</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">Missing</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">73</span> <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">composer</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">vocab_size</span>
<span class="g g-Whitespace">     </span><span class="mi">74</span> <span class="n">context_length</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">composer</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">context_length</span>

<span class="ne">AssertionError</span>: 
</pre></div>
</div>
</div>
</div>
</section>
<section id="trained-attention-heatmaps">
<h2><a class="toc-backref" href="#id48" role="doc-backlink">Trained Attention Heatmaps</a><a class="headerlink" href="#trained-attention-heatmaps" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">global_</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>

<span class="n">batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>

<span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">target_padding_masks</span><span class="p">,</span> <span class="n">future_masks</span> <span class="o">=</span> <span class="n">batch</span>

<span class="n">trained_model</span> <span class="o">=</span> <span class="n">_trained_state</span><span class="o">.</span><span class="n">model</span>
<span class="n">trained_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target_padding_masks</span><span class="o">=</span><span class="n">target_padding_masks</span><span class="p">,</span> <span class="n">future_masks</span><span class="o">=</span><span class="n">future_masks</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We zoom into just 1 example in the batch, note that the predicted values
are jibberish for the first few tokens (before equal sign) because recall
we told the model to practically “don’t care” about any tokens before the
answer by padding them with masks. As long as our answer is correct, then it is
expected.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">example_input</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">example_input</span><span class="p">)</span>

<span class="n">decoded_input</span> <span class="o">=</span> <span class="n">decode_equation</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">,</span> <span class="n">example_input</span><span class="p">,</span> <span class="n">show_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">decoded_input</span><span class="p">)</span>

<span class="n">example_target</span> <span class="o">=</span> <span class="n">targets</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">example_target</span><span class="p">)</span>

<span class="n">decoded_target</span> <span class="o">=</span> <span class="n">decode_equation</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">,</span> <span class="n">example_target</span><span class="p">,</span> <span class="n">show_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">decoded_target</span><span class="p">)</span>

<span class="n">example_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">example_prediction</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">example_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">example_prediction</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">decode_equation</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">,</span> <span class="n">example_prediction</span><span class="p">,</span> <span class="n">show_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">14</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">13</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span><span style="font-weight: bold">])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #008000; text-decoration-color: #008000">'&lt;BOS&gt;31+04=035'</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">15</span><span style="font-weight: bold">])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #008000; text-decoration-color: #008000">'&lt;PAD&gt;&lt;PAD&gt;&lt;PAD&gt;&lt;PAD&gt;&lt;PAD&gt;&lt;PAD&gt;035&lt;EOS&gt;'</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">15</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">9</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">15</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">15</span><span style="font-weight: bold">])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #008000; text-decoration-color: #008000">'10&lt;EOS&gt;93&lt;EOS&gt;035&lt;EOS&gt;'</span>
</pre>
</div></div>
</div>
<p>Indeed we get a correct answer 35.</p>
<p>We take last decoder block’s attention weights to visualize, we can take any
decoder block’s attention weights to visualize, but we take the last one for
simplicity and hinging on the fact that the last decoder block is the one that
contains the most information about the input sequence.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">last_decoder_block</span> <span class="o">=</span> <span class="n">trained_model</span><span class="o">.</span><span class="n">decoder_blocks</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">masked_self_attention_mha</span> <span class="o">=</span> <span class="n">last_decoder_block</span><span class="o">.</span><span class="n">masked_self_attention_mha</span>

<span class="n">context_vector</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">masked_self_attention_mha</span><span class="o">.</span><span class="n">context_vector</span><span class="p">,</span> <span class="n">masked_self_attention_mha</span><span class="o">.</span><span class="n">attention_weights</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">attention_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">torch.Size</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">256</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10</span><span style="font-weight: bold">])</span>
</pre>
</div></div>
</div>
<p>We want to select the example earlier, which is conveniently the first example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># take first sample</span>
<span class="n">example_attention_weights</span> <span class="o">=</span> <span class="n">attention_weights</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">example_attention_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">torch.Size</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10</span><span style="font-weight: bold">])</span>
</pre>
</div></div>
</div>
<p>So for the xtick and ytick, the attention weight matrix is <span class="math notranslate nohighlight">\(T \times T\)</span>, and
first row is first token, seonc row is second token etc. And note that the x
axis is keys and y axis is queries. This convention is easy to see from the
attention scores formula of <code class="docutils literal notranslate"><span class="pre">Q</span> <span class="pre">&#64;</span> <span class="pre">K.T</span></code> where <code class="docutils literal notranslate"><span class="pre">Q</span></code> is the query and <code class="docutils literal notranslate"><span class="pre">K</span></code> is the key.</p>
<p>The resulting attention scores matrix has dimensions (<code class="docutils literal notranslate"><span class="pre">num_queries</span></code>,
<code class="docutils literal notranslate"><span class="pre">num_keys</span></code>), where each row corresponds to a query and each column corresponds
to a key. In our case is just <code class="docutils literal notranslate"><span class="pre">TxT</span></code> since both query and keys have hame length.
When visualizing the attention scores as a heatmap, the x-axis corresponds to
the keys dimension (num_keys), and the y-axis corresponds to the queries
dimension (num_queries). This alignment matches the mathematical formulation of
the attention computation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xticks</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;&lt;BOS&gt;&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">decoded_input</span><span class="p">[</span><span class="mi">5</span><span class="p">:])</span>
<span class="n">yticks</span> <span class="o">=</span> <span class="n">xticks</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">show_attention_heatmaps</span><span class="p">(</span>
    <span class="n">attention_weights</span><span class="o">=</span><span class="n">example_attention_weights</span><span class="p">,</span>
    <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Keys&quot;</span><span class="p">,</span>
    <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Queries&quot;</span><span class="p">,</span>
    <span class="n">xticks</span><span class="o">=</span><span class="n">xticks</span><span class="p">,</span>
    <span class="n">yticks</span><span class="o">=</span><span class="n">yticks</span><span class="p">,</span>
    <span class="n">show_title</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">show_values</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">value_dp</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">figure_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;figsize&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="mi">15</span><span class="p">)},</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/6b64487430bdc4402f658e2741bf9dba4260ace82ab0f32b30d93123c10743be.png" src="../../_images/6b64487430bdc4402f658e2741bf9dba4260ace82ab0f32b30d93123c10743be.png" />
</div>
</div>
<p>If we want two samples.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span> <span class="o">=</span> <span class="n">show_attention_heatmaps</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">],</span> <span class="n">show_title</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">show_values</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/008104b12a05049183ee5f823279ee3df81cf0f279e60cfc183c40fd8dd754a6.png" src="../../_images/008104b12a05049183ee5f823279ee3df81cf0f279e60cfc183c40fd8dd754a6.png" />
</div>
</div>
<p>the xy axis is keys and queries, which is correct <code class="docutils literal notranslate"><span class="pre">Q</span> <span class="pre">&#64;</span> <span class="pre">K.T</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">last_decoder_block</span> <span class="o">=</span> <span class="n">trained_model</span><span class="o">.</span><span class="n">decoder_blocks</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">masked_self_attention_mha</span> <span class="o">=</span> <span class="n">last_decoder_block</span><span class="o">.</span><span class="n">masked_self_attention_mha</span>
<span class="n">context_vector</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">masked_self_attention_mha</span><span class="o">.</span><span class="n">context_vector</span><span class="p">,</span> <span class="n">masked_self_attention_mha</span><span class="o">.</span><span class="n">attention_weights</span>

<span class="n">num_heads</span> <span class="o">=</span> <span class="n">attention_weights</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Labels for each character in the sequence, including BOS</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;&lt;BOS&gt;&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="s1">&#39;59+14=073&#39;</span><span class="p">)</span>

<span class="c1"># Loop over each head and plot its heatmap</span>
<span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

    <span class="c1"># Extract attention weights for the last sample in the last batch for this head</span>
    <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">attention_weights</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">head</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">attention_matrix</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Attention Weights Heatmap for &#39;&lt;BOS&gt;59+14=073&#39; - Head </span><span class="si">{</span><span class="n">head</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Keys&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Queries&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="generation">
<h2><a class="toc-backref" href="#id49" role="doc-backlink">Generation</a><a class="headerlink" href="#generation" title="Link to this heading">#</a></h2>
<div class="caution admonition">
<p class="admonition-title">Deprecated To Redo</p>
<p><code class="docutils literal notranslate"><span class="pre">logits.argmax(dim=-1)</span></code> basically compress 1x7x18 to 1x7 where for each row of
the 7 rows, find the index that is maximum for example, first row 7.8 is max of
all 18 elements, so index 0 is returned. <code class="docutils literal notranslate"><span class="pre">tensor([[0,</span> <span class="pre">8,</span> <span class="pre">8,</span> <span class="pre">1,</span> <span class="pre">6,</span> <span class="pre">14,</span> <span class="pre">1]])</span></code></p>
<p>There is some meaning here too, remember our input <code class="docutils literal notranslate"><span class="pre">[15,</span> <span class="pre">9,</span> <span class="pre">8,</span> <span class="pre">10,</span> <span class="pre">3,</span> <span class="pre">5,</span> <span class="pre">13]</span></code>
this is basically the BOS (15) up till the equal sign, then
<code class="docutils literal notranslate"><span class="pre">[</span> <span class="pre">0,</span> <span class="pre">8,</span> <span class="pre">8,</span> <span class="pre">1,</span> <span class="pre">6,</span> <span class="pre">14,</span> <span class="pre">1]</span></code> is basically the prediction of each token what comes
next.</p>
<ol class="arabic simple">
<li><p><strong>Input Sequence</strong>: Your input sequence is <code class="docutils literal notranslate"><span class="pre">[15,</span> <span class="pre">9,</span> <span class="pre">8,</span> <span class="pre">10,</span> <span class="pre">3,</span> <span class="pre">5,</span> <span class="pre">13]</span></code>. In
this context, <code class="docutils literal notranslate"><span class="pre">15</span></code> could be a special token like BOS (Beginning of Sentence)
or something else depending on your encoding scheme.</p></li>
<li><p><strong>Output Tensor Interpretation</strong>: The output tensor
<code class="docutils literal notranslate"><span class="pre">tensor([[</span> <span class="pre">0,</span> <span class="pre">8,</span> <span class="pre">8,</span> <span class="pre">1,</span> <span class="pre">6,</span> <span class="pre">14,</span> <span class="pre">1]])</span></code> represents the model’s sequential
predictions for each step of the input:</p>
<ul class="simple">
<li><p>The first element <code class="docutils literal notranslate"><span class="pre">0</span></code> is the prediction following the first element <code class="docutils literal notranslate"><span class="pre">15</span></code>
of the input.</p></li>
<li><p>The second element <code class="docutils literal notranslate"><span class="pre">8</span></code> is the prediction after seeing the first two
elements <code class="docutils literal notranslate"><span class="pre">15,</span> <span class="pre">9</span></code> of the input.</p></li>
<li><p>The third element <code class="docutils literal notranslate"><span class="pre">8</span></code> is predicted after seeing <code class="docutils literal notranslate"><span class="pre">15,</span> <span class="pre">9,</span> <span class="pre">8</span></code>.</p></li>
<li><p>The fourth element <code class="docutils literal notranslate"><span class="pre">1</span></code> follows after <code class="docutils literal notranslate"><span class="pre">15,</span> <span class="pre">9,</span> <span class="pre">8,</span> <span class="pre">10</span></code>.</p></li>
<li><p>The sequence continues in this manner, with each new prediction based on
an increasingly longer prefix of the input sequence.</p></li>
</ul>
</li>
<li><p><strong>Sequential Predictions</strong>: This output suggests that the model is working in
an autoregressive manner. It generates predictions one token at a time, and
each prediction is based on the sequence of tokens it has seen up to that
point.</p></li>
<li><p><strong>Specific Meanings of Output Tokens</strong>: The actual meaning of each token in
your output tensor (<code class="docutils literal notranslate"><span class="pre">0</span></code>, <code class="docutils literal notranslate"><span class="pre">8</span></code>, <code class="docutils literal notranslate"><span class="pre">1</span></code>, <code class="docutils literal notranslate"><span class="pre">6</span></code>, <code class="docutils literal notranslate"><span class="pre">14</span></code>, etc.) depends on your specific
encoding and task. In a language model, these would correspond to specific
words or characters. In a numerical context, they could represent numbers or
operations.</p></li>
</ol>
<p>In summary, the output tensor reflects the model’s predictions for what comes
next in the sequence, based on the current and all previous input tokens. Each
element in the output is the model’s guess for the next token, considering the
sequence of tokens it has seen up to that point.</p>
<blockquote>
<div><p>Then we move on to the concat operation:</p>
</div></blockquote>
<ul class="simple">
<li><p>In our model, after processing the input <code class="docutils literal notranslate"><span class="pre">[15,</span> <span class="pre">9,</span> <span class="pre">8,</span> <span class="pre">10,</span> <span class="pre">3,</span> <span class="pre">5,</span> <span class="pre">13]</span></code>, it
predicts the next token to be <code class="docutils literal notranslate"><span class="pre">1</span></code>. This prediction is based on the entire
sequence seen so far.</p></li>
<li><p>The process of extending the input sequence with this new token (<code class="docutils literal notranslate"><span class="pre">1</span></code>) and
then feeding this extended sequence back into the model for further
predictions is indeed an example of greedy decoding. The model is
iteratively building a longer sequence, one token at a time, always choosing
the most likely next token at each step.</p></li>
<li><p>This process would continue until a stopping condition is met, which might
be the prediction of an EOS (End of Sentence) token or reaching a maximum
sequence length.</p></li>
</ul>
<blockquote>
<div><p>for i in range(num_digits + 2): now you know why loop over 4 times in total if
num digits is 2. This is because, after equal sign, we will have answer of 3
digits (xyz) and an EOS token, our stop condition!</p>
</div></blockquote>
<p>Lastly: <code class="docutils literal notranslate"><span class="pre">tensor([[15,</span> <span class="pre">9,</span> <span class="pre">8,</span> <span class="pre">10,</span> <span class="pre">3,</span> <span class="pre">5,</span> <span class="pre">13,</span> <span class="pre">1,</span> <span class="pre">3,</span> <span class="pre">3,</span> <span class="pre">14]])</span></code> is the full predicted
after EOS is met.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># def what_is(question: str) -&gt; str:</span>
<span class="c1">#     &quot;function for computing the sum of two numbers with input in literal string format&quot;</span>
<span class="c1">#     pred = compute_sum(model, encode_equation(question, num_digits).view(1, -1))</span>
<span class="c1">#     pred = decode_equation(pred)</span>
<span class="c1">#     pred = pred[pred.index(&quot;=&quot;) + 1 :]</span>
<span class="c1">#     return question + pred</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="references-and-further-readings">
<h2><a class="toc-backref" href="#id50" role="doc-backlink">References and Further Readings</a><a class="headerlink" href="#references-and-further-readings" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="github reference external" href="https://github.com/karpathy/minGPT">karpathy/minGPT</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./transformer/decoder"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="implementation.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">The Implementation of Generative Pre-trained Transformers (GPT)</p>
      </div>
    </a>
    <a class="right-next"
       href="../../playbook/how_to_calculate_flops_in_gpt2.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">How to Calculate the Number of FLOPs in GPT-2</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">Motivation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#config">Config</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reproducibility">Reproducibility</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vocabulary">Vocabulary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenization">Tokenization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">Dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-dataset">Create Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#encoding-strategy-overview">Encoding Strategy Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#constructing-pytorch-dataset">Constructing PyTorch Dataset</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#construct-batches-collate-function-and-dataloader">Construct Batches, Collate Function and DataLoader</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-and-target">Input and Target</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#target-padding-mask">Target Padding Mask</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#future-mask">Future Mask</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#merge-padding-and-future-masks">Merge Padding and Future Masks</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#first-sample-first-token">First Sample First Token</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#first-sample-fourth-token">First Sample Fourth Token</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#further-add-a-singleton-dimension-in-target-masks">Further Add a Singleton Dimension in Target Masks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-mask-our-target-in-adder">Why mask our target in Adder?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#split-to-train-valid-test">Split to Train-Valid-Test</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-dataloader">Create DataLoader</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model">Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-paradigm">Training Paradigm</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizer">Optimizer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-scheduler">Learning Rate Scheduler</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Motivation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#warmup">Warmup</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#definition">Definition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation">Implementation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#criterion">Criterion</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#inputs-and-targets">Inputs and Targets</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-computation">Loss Computation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#reduction">Reduction</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-binary-classification-example">Simple Binary Classification Example</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-example">GPT Example</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-smaller-example-for-illustration">A Smaller Example for Illustration</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#masking-and-ignore-index">Masking and Ignore Index</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#initializing-criterion-with-composer">Initializing Criterion With Composer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#state">State</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#trainer">Trainer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#trained-attention-heatmaps">Trained Attention Heatmaps</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generation">Generation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references-and-further-readings">References and Further Readings</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gao Hongnan
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>