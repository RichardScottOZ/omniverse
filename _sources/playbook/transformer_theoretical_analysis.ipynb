{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Theoretical Analysis on Estimation of FLOPs, Parameters, Peak Memory Footprint, and Checkpoint Size\n",
    "\n",
    "[![Twitter Handle](https://img.shields.io/badge/Twitter-@gaohongnan-blue?style=social&logo=twitter)](https://twitter.com/gaohongnan)\n",
    "[![LinkedIn Profile](https://img.shields.io/badge/@gaohongnan-blue?style=social&logo=linkedin)](https://linkedin.com/in/gao-hongnan)\n",
    "[![GitHub Profile](https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&logo=github)](https://github.com/gao-hongnan)\n",
    "\n",
    "```{contents}\n",
    ":local:\n",
    "```\n",
    "\n",
    "This notebook references from\n",
    "[Andrej Karpathy's NanoGPT](https://github.com/karpathy/nanoGPT/blob/master/transformer_sizing.ipynb),\n",
    "which originally stores a bunch of analysis about a Transformer, e.g. estimates\n",
    "the number of FLOPs, parameters, peak memory footprint, checkpoint size, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum, IntEnum\n",
    "from typing import Literal\n",
    "\n",
    "import pandas as pd\n",
    "from rich.pretty import pprint\n",
    "from tabulate import tabulate\n",
    "from torch import nn\n",
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations, Constants and Enums\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    num_decoder_blocks: int = 12\n",
    "    context_length: int = 1024\n",
    "    n_embd: int = 768\n",
    "    ffw_size: int = 3072  # note, this is 4 * n_embd\n",
    "    n_head: int = 12\n",
    "    vocab_size: int = 50257\n",
    "    bias: Literal[False] = False\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        assert self.ffw_size == 4 * self.n_embd, \"ffw_size must be 4 * n_embd\"\n",
    "        assert self.bias is False, \"bias must be False in this experiment.\"\n",
    "\n",
    "\n",
    "class GPT2ModelType(Enum):\n",
    "    GPT2 = \"gpt2\"\n",
    "    GPT2_MEDIUM = \"gpt2-medium\"\n",
    "    GPT2_LARGE = \"gpt2-large\"\n",
    "    GPT2_XL = \"gpt2-xl\"\n",
    "\n",
    "\n",
    "class ByteUnits(IntEnum):\n",
    "    B = 1  # Byte = 1 byte\n",
    "    KB = 1000  # Kilobyte = 10^3 bytes\n",
    "    MB = 1000**2  # Megabyte = 10^6 bytes\n",
    "    GB = 1000**3  # Gigabyte = 10^9 bytes\n",
    "\n",
    "\n",
    "class FloatingPointPrecision(IntEnum):\n",
    "    FP32 = 4  # 32-bit floating-point, 4 bytes\n",
    "    FP16 = 2  # 16-bit floating-point, 2 bytes\n",
    "    BFLOAT16 = 2  # bfloat16, 16-bit, 2 bytes\n",
    "\n",
    "\n",
    "class GPUMemory(Enum):\n",
    "    A100_40GB = 40e9  # 40 GB for NVIDIA A100\n",
    "    V100_16GB = 16e9  # 16 GB for NVIDIA V100\n",
    "    V100_32GB = 32e9  # 32 GB for NVIDIA V100\n",
    "    T4_16GB = 16e9  # 16 GB for NVIDIA T4\n",
    "    P100_16GB = 16e9  # 16 GB for NVIDIA P100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GPTConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">num_decoder_blocks</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">context_length</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1024</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">n_embd</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">ffw_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3072</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">n_head</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">vocab_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50257</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mGPTConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mnum_decoder_blocks\u001b[0m=\u001b[1;36m12\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mcontext_length\u001b[0m=\u001b[1;36m1024\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mn_embd\u001b[0m=\u001b[1;36m768\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mffw_size\u001b[0m=\u001b[1;36m3072\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mn_head\u001b[0m=\u001b[1;36m12\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mvocab_size\u001b[0m=\u001b[1;36m50257\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt2_config = GPTConfig()\n",
    "pprint(gpt2_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Trainable Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_trainable_parameters(model: nn.Module, include_bias: bool = True) -> int:\n",
    "    \"\"\"Returns the number of trainable parameters in the model.\"\"\"\n",
    "    if not include_bias:\n",
    "        return sum(p.numel() for name, p in model.named_parameters() if p.requires_grad and \"bias\" not in name)\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = GPT2LMHeadModel.from_pretrained(GPT2ModelType.GPT2.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters in GPT2 model: 124337664 (excluding bias) and 124439808 (including bias).\n"
     ]
    }
   ],
   "source": [
    "gpt2_params_no_bias = total_trainable_parameters(gpt2, include_bias=False)\n",
    "gpt2_params_with_bias = total_trainable_parameters(gpt2, include_bias=True)\n",
    "\n",
    "print(\n",
    "    f\"Number of trainable parameters in GPT2 model: {gpt2_params_no_bias} (excluding bias) and {gpt2_params_with_bias} (including bias).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Karpathy's blog post assumed that there is no bias for simplicity, we will\n",
    "also assume that there is no bias in the linear layers. We confirmed that the\n",
    "number of params (`124337664`) for the smallest GPT-2 model indeed matches the\n",
    "number of params given by Karpathy.\n",
    "\n",
    "In what follows, we would assume the smallest GPT-2 model and work out the\n",
    "theoretical model for the Transformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_args = {\n",
    "#     'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "#     'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "#     'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "#     'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "# }[model_type]\n",
    "\n",
    "\n",
    "def params(\n",
    "    num_decoder_blocks: int = 12,\n",
    "    context_length: int = 1024,\n",
    "    n_embd: int = 768,\n",
    "    ffw_size: int = 3072,\n",
    "    vocab_size: int = 50257,\n",
    ") -> OrderedDict[str, int]:\n",
    "    \"\"\"estimates the number of parameters in the model\"\"\"\n",
    "    out = OrderedDict()\n",
    "\n",
    "    # token and position embeddings\n",
    "    out[\"embedding/position\"] = n_embd * context_length\n",
    "    out[\"embedding/token\"] = n_embd * vocab_size\n",
    "    out[\"embedding\"] = out[\"embedding/position\"] + out[\"embedding/token\"]\n",
    "\n",
    "    # attention blocks\n",
    "    out[\"attention/ln\"] = n_embd  # note, bias=False in our LN\n",
    "    out[\"attention/kqv\"] = n_embd * 3 * n_embd\n",
    "    out[\"attention/proj\"] = n_embd**2\n",
    "    out[\"attention\"] = out[\"attention/ln\"] + out[\"attention/kqv\"] + out[\"attention/proj\"]\n",
    "\n",
    "    # MLP blocks\n",
    "    assert ffw_size == 4 * n_embd, \"ffw_size must be 4 * n_embd\"\n",
    "    out[\"mlp/ln\"] = n_embd\n",
    "    out[\"mlp/ffw\"] = n_embd * ffw_size\n",
    "    out[\"mlp/proj\"] = ffw_size * n_embd\n",
    "    out[\"mlp\"] = out[\"mlp/ln\"] + out[\"mlp/ffw\"] + out[\"mlp/proj\"]\n",
    "\n",
    "    # the transformer and the rest of it\n",
    "    out[\"block\"] = out[\"attention\"] + out[\"mlp\"]\n",
    "    out[\"transformer\"] = num_decoder_blocks * out[\"block\"]\n",
    "    out[\"ln_f\"] = n_embd  # final layernorm\n",
    "    out[\"dense\"] = 0  # 0 because of parameter sharing. This layer uses the weights from the embedding layer\n",
    "\n",
    "    # total\n",
    "    out[\"total\"] = out[\"embedding\"] + out[\"transformer\"] + out[\"ln_f\"] + out[\"dense\"]\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We see: 124337664, Expected: 124337664, Match: True\n",
      "\n",
      "+--------------------+------------+-----------------------+\n",
      "|        Name        | Parameters |       Ratio (%)       |\n",
      "+--------------------+------------+-----------------------+\n",
      "| embedding/position |   786432   |  0.6324970042866496   |\n",
      "|  embedding/token   |  38597376  |  31.042384711361475   |\n",
      "|     embedding      |  39383808  |  31.674881715648123   |\n",
      "|    attention/ln    |    768     | 0.0006176728557486812 |\n",
      "|   attention/kqv    |  1769472   |  1.4231182596449616   |\n",
      "|   attention/proj   |   589824   |  0.47437275321498723  |\n",
      "|     attention      |  2360064   |  1.8981086857156975   |\n",
      "|       mlp/ln       |    768     | 0.0006176728557486812 |\n",
      "|      mlp/ffw       |  2359296   |   1.897491012859949   |\n",
      "|      mlp/proj      |  2359296   |   1.897491012859949   |\n",
      "|        mlp         |  4719360   |   3.795599698575646   |\n",
      "|       block        |  7079424   |   5.693708384291344   |\n",
      "|    transformer     |  84953088  |   68.32450061149613   |\n",
      "|        ln_f        |    768     | 0.0006176728557486812 |\n",
      "|       dense        |     0      |          0.0          |\n",
      "|       total        | 124337664  |         100.0         |\n",
      "+--------------------+------------+-----------------------+\n"
     ]
    }
   ],
   "source": [
    "params_dict = params()\n",
    "gpt2_params_no_bias_manual = params_dict[\"total\"]\n",
    "\n",
    "# Compare to expected PyTorch model parameter count\n",
    "expected_params = gpt2_params_no_bias\n",
    "comparison_result = gpt2_params_no_bias_manual == expected_params\n",
    "comparison_msg = f\"We see: {gpt2_params_no_bias_manual}, Expected: {expected_params}, Match: {comparison_result}\"\n",
    "\n",
    "data = {\n",
    "    \"Name\": params_dict.keys(),\n",
    "    \"Parameters\": params_dict.values(),\n",
    "    \"Ratio (%)\": [value / gpt2_params_no_bias_manual * 100 for value in params_dict.values()],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Printing comparison result and parameter distribution table\n",
    "print(comparison_msg + \"\\n\")\n",
    "print(tabulate(df, headers=\"keys\", tablefmt=\"pretty\", showindex=False, numalign=\"right\", floatfmt=\".4f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Checkpoint Size and Fluff Ratio\n",
    "\n",
    "The functions below perform a series of calculations related to the size\n",
    "of a GPT-2 model checkpoint, both measured and estimated, and computes the\n",
    "\"fluff ratio\" to compare these sizes. The purpose of these calculations is to\n",
    "evaluate how closely the estimated size of a GPT-2 model checkpoint matches the\n",
    "actual measured size, and to quantify any overhead or additional data in the\n",
    "checkpoint file as a percentage of the estimated size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_checkpoint_size(params_count: int, precision: FloatingPointPrecision, units: ByteUnits) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the estimated checkpoint size in specified units.\n",
    "\n",
    "    This function estimates the checkpoint size for a model given the number\n",
    "    of parameters, the precision of these parameters, and\n",
    "    the desired units for the result. It accounts for the AdamW optimizer's\n",
    "    storage requirements by adding two times the parameter bytes to account\n",
    "    for the optimizer's moment and velocity vectors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params_count : int\n",
    "        The number of parameters excluding biases.\n",
    "    precision : FloatingPointPrecision\n",
    "        The floating point precision of the parameters.\n",
    "    units : ByteUnits\n",
    "        The units for the resulting checkpoint size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The estimated checkpoint size in the specified units.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The AdamW optimizer requires additional storage for each parameter\n",
    "    for maintaining momentum and variance vectors, hence the calculation\n",
    "    includes 2 * params_bytes to accommodate these.\n",
    "    \"\"\"\n",
    "    params_bytes = params_count * precision.value\n",
    "    params_and_buffers_bytes = params_bytes + 2 * params_bytes  # AdamW optimizer buffers\n",
    "    return params_and_buffers_bytes / units.value\n",
    "\n",
    "\n",
    "def calculate_fluff_ratio(measured_bytes: int, estimated_bytes: float, units: ByteUnits) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the fluff ratio between measured and estimated checkpoint sizes.\n",
    "\n",
    "    The fluff ratio is a measure of the overhead or additional data in the\n",
    "    checkpoint file, expressed as a percentage of the estimated size. This\n",
    "    function converts the estimated size from gigabytes (or specified units)\n",
    "    to bytes before calculating the ratio to ensure consistency in units.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    measured_bytes : int\n",
    "        The actual size of the checkpoint file, in bytes.\n",
    "    estimated_bytes : float\n",
    "        The estimated size of the checkpoint file, in the specified units.\n",
    "    units : ByteUnits\n",
    "        The units in which the estimated bytes are provided.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The fluff ratio, expressed as a percentage.\n",
    "    \"\"\"\n",
    "    estimated_bytes_in_bytes = estimated_bytes * units.value\n",
    "    return (measured_bytes / estimated_bytes_in_bytes) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Measured Checkpoint Size in Bytes**:\n",
    "\n",
    "   - `gpt2_checkpoint_size_measured_in_bytes` is assigned a numerical value\n",
    "     that represents the actual size of a GPT-2 model checkpoint file in bytes.\n",
    "     This value is obtained from the output of the Unix command\n",
    "     `wc -c ckpt.pt`, which counts the number of bytes in the file `ckpt.pt`.\n",
    "\n",
    "2. **Estimated Checkpoint Size in Bytes**:\n",
    "\n",
    "   - The `calculate_checkpoint_size` function is called with the number of\n",
    "     parameters excluding biases (`gpt2_params_no_bias`), the precision of the\n",
    "     model's parameters (`FloatingPointPrecision.FP32`), and the unit of\n",
    "     measurement (`ByteUnits.B` for bytes). This function calculates the\n",
    "     estimated total size of the checkpoint in bytes, taking into account the\n",
    "     parameters and the additional storage required for the AdamW optimizer's\n",
    "     buffers.\n",
    "   - It is worth noting we are assuming floating-point precision of 32 bits (4\n",
    "     bytes) for the model's parameters, and hence we are multiplying the number\n",
    "     of parameters by 4 to obtain the size in bytes.\n",
    "\n",
    "   - The AdamW optimizer, which is commonly used in training deep learning\n",
    "     models for tasks like those involving GPT-2, maintains two additional\n",
    "     values (buffers) for each parameter: the first for the moment vector (`m`)\n",
    "     and the second for the squared moment vector (`v`). These buffers are used\n",
    "     to adapt the learning rates for each parameter during training. This is\n",
    "     why the storage requirement triples (`params_bytes + 2 * params_bytes`),\n",
    "     accounting for the original parameters plus the two buffers.\n",
    "\n",
    "3. **Fluff Ratio Calculation**:\n",
    "\n",
    "   - The `calculate_fluff_ratio` function is called with the measured size in\n",
    "     bytes, the estimated size in bytes, and the unit of measurement for the\n",
    "     estimated size (bytes). This function calculates the fluff ratio, which\n",
    "     indicates the percentage of overhead or additional data in the measured\n",
    "     checkpoint file compared to the estimated size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-------------------+\n",
      "|              Metric               |       Value       |\n",
      "+-----------------------------------+-------------------+\n",
      "| Measured Checkpoint Size (bytes)  |    1542470366     |\n",
      "|   Measured Checkpoint Size (GB)   |    1.542470366    |\n",
      "| Estimated Checkpoint Size (bytes) |   1492051968.0    |\n",
      "|  Estimated Checkpoint Size (GB)   |    1.492051968    |\n",
      "|            Fluff Ratio            | 103.3791314968461 |\n",
      "+-----------------------------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "gpt2_checkpoint_size_measured_in_bytes = 1542470366  # from 'wc -c ckpt.pt'\n",
    "gpt2_checkpoint_size_measured_in_gb = gpt2_checkpoint_size_measured_in_bytes / ByteUnits.GB\n",
    "\n",
    "gpt2_checkpoint_size_estimated_in_bytes = calculate_checkpoint_size(\n",
    "    params_count=gpt2_params_no_bias,\n",
    "    precision=FloatingPointPrecision.FP32,\n",
    "    units=ByteUnits.B,\n",
    ")\n",
    "gpt2_checkpoint_size_estimated_in_gb = gpt2_checkpoint_size_estimated_in_bytes / ByteUnits.GB\n",
    "\n",
    "\n",
    "fluff_ratio = calculate_fluff_ratio(\n",
    "    measured_bytes=gpt2_checkpoint_size_measured_in_bytes,\n",
    "    estimated_bytes=gpt2_checkpoint_size_estimated_in_bytes,\n",
    "    units=ByteUnits.B,\n",
    ")\n",
    "\n",
    "data = [\n",
    "    [\"Measured Checkpoint Size (bytes)\", gpt2_checkpoint_size_measured_in_bytes],\n",
    "    [\"Measured Checkpoint Size (GB)\", gpt2_checkpoint_size_measured_in_gb],\n",
    "    [\"Estimated Checkpoint Size (bytes)\", gpt2_checkpoint_size_estimated_in_bytes],\n",
    "    [\"Estimated Checkpoint Size (GB)\", gpt2_checkpoint_size_estimated_in_gb],\n",
    "    [\"Fluff Ratio\", fluff_ratio],\n",
    "]\n",
    "\n",
    "print(tabulate(data, headers=[\"Metric\", \"Value\"], tablefmt=\"pretty\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Memory Footprint of Loading Model and Optimizer\n",
    "\n",
    "We can roughly understand that a checkpoint represents the amount of memory\n",
    "needed to store not just the model itself (its weights) but also additional\n",
    "information related to the optimizer state when you're using GPUs for deep\n",
    "learning tasks.\n",
    "\n",
    "When loading a model from a checkpoint for further training or inference, the\n",
    "GPU memory must accommodate the model weights and the optimizer state (if\n",
    "continuing training).\n",
    "\n",
    "Below, we estimate the ratio of our GPU memory that will be taken up by\n",
    "the model and optimizer state when loading a GPT-2 model from a checkpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory ratio taken up just for parameters: 3.73%\n"
     ]
    }
   ],
   "source": [
    "def calculate_memory_ratio(checkpoint_size: float, gpu_memory: GPUMemory) -> str:\n",
    "    memory_ratio = checkpoint_size / gpu_memory.value * 100\n",
    "    return f\"Memory ratio taken up just for parameters: {memory_ratio:.2f}%\"\n",
    "\n",
    "\n",
    "print(calculate_memory_ratio(checkpoint_size=gpt2_checkpoint_size_estimated_in_bytes, gpu_memory=GPUMemory.A100_40GB))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming an A100 GPU with roughly 40GB memory, then the code calculates the\n",
    "percentage of the GPU memory that the estimated checkpoint size (in bytes)\n",
    "occupies. This calculation gives an insight into how much of the GPU's memory is\n",
    "dedicated to storing the model's weights and the optimizer's buffers, without\n",
    "considering other memory usages such as activations during forward and backward\n",
    "passes.\n",
    "\n",
    "This percentage is relatively small, implying that most of the GPU memory is\n",
    "actually used for activations. Activations are the intermediate outputs of\n",
    "layers during the forward pass and their gradients during the backward pass,\n",
    "which can consume significant amounts of memory, especially in deep models and\n",
    "with large batch sizes.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating FLOPs for a Single Forward Pass\n",
    "\n",
    "In order to estimate FLOPs for a single forward pass, we would first need to\n",
    "define what is a FLOPS.\n",
    "\n",
    "### Basics of Floating Point Numbers\n",
    "\n",
    "- **Floating Point Representation**: In computers, numbers can be represented\n",
    "  in various formats, and one common format is floating point. This format is\n",
    "  used to represent real numbers (numbers with fractions) using a fixed amount\n",
    "  of memory, allowing for a wide range of values. A floating point number is\n",
    "  composed of a sign, an exponent, and a mantissa (or significand). This\n",
    "  representation can handle very large numbers, very small numbers, and\n",
    "  fractions.\n",
    "- **Operations on Floating Point Numbers**: Operations on floating point\n",
    "  numbers include addition, subtraction, multiplication, and division. Each of\n",
    "  these operations takes one or more floating point numbers as input and\n",
    "  produces a floating point number as output.\n",
    "\n",
    "### Floating Point Operations (FLOPs)\n",
    "\n",
    "Floating Point Operations, or FLOPs, refer to individual mathematical operations\n",
    "(additions, subtractions, multiplications, divisions) performed on\n",
    "[floating point numbers](https://en.wikipedia.org/wiki/Floating-point_arithmetic).\n",
    "Each operation counts as one FLOP.\n",
    "\n",
    "### Counting FLOPs of Matrix Multiplications\n",
    "\n",
    "In the context of deep learning, many operations are done via matrix\n",
    "multiplications, we will take a look at how to count FLOPs for matrix\n",
    "multiplications next.\n",
    "\n",
    "Deep learning, particularly in neural networks, relies heavily on matrix\n",
    "multiplications. A single matrix multiplication operation involves multiple\n",
    "floating point multiplications and additions.\n",
    "\n",
    "Consider two matrices $\\mathbf{A}$ and $\\mathbf{B}$ of size $m \\times n$ and\n",
    "$n \\times p$:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix}\n",
    "a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n",
    "a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1} & a_{m2} & \\cdots & a_{mn}\n",
    "\\end{bmatrix}_{m \\times n} \\quad \\mathbf{B} = \\begin{bmatrix}\n",
    "b_{11} & b_{12} & \\cdots & b_{1p} \\\\\n",
    "b_{21} & b_{22} & \\cdots & b_{2p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "b_{n1} & b_{n2} & \\cdots & b_{np}\n",
    "\\end{bmatrix}_{n \\times p}\n",
    "$$\n",
    "\n",
    "It is easy to see that if we want to compute the product\n",
    "$\\mathbf{C} = \\mathbf{A} \\mathbf{B}$, the element $c_{ij}$ of $\\mathbf{C}$ is\n",
    "given by:\n",
    "\n",
    "$$\n",
    "c_{ij} = \\sum_{k=1}^{n} a_{ik} b_{kj}\n",
    "$$\n",
    "\n",
    "and therefore there are a total of $m \\times n \\times p$ multiplications and\n",
    "$m \\times (n-1) \\times p$ additions. This amounts to roughly:\n",
    "\n",
    "$$\n",
    "m \\times n \\times p + m \\times (n-1) \\times p \\approx 2 \\times m \\times n \\times p\n",
    "$$ (playbook-transformer-flops-approximation)\n",
    "\n",
    "FLOPs. Note this is basically because matrix multiplication is a series of dot\n",
    "products, and each dot product involves $n$ multiplications and $n-1$ additions.\n",
    "\n",
    "### Estimating FLOPs for a Single Forward Pass of GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flops(\n",
    "    num_decoder_blocks: int = 12,\n",
    "    context_length: int = 1024,\n",
    "    n_embd: int = 768,\n",
    "    n_head: int = 12,\n",
    "    ffw_size: int = 3072,\n",
    "    vocab_size: int = 50257,\n",
    ") -> OrderedDict[str, int]:\n",
    "    # we only count Weight FLOPs, all other layers (LayerNorm, Softmax, etc) are effectively irrelevant\n",
    "    # we count actual FLOPs, not MACs. Hence 2* all over the place\n",
    "    # basically for any matrix multiply A (BxC) @ B (CxD) -> (BxD) flops are 2*B*C*D\n",
    "\n",
    "    out = OrderedDict()\n",
    "    head_size = n_embd // n_head\n",
    "\n",
    "    # attention blocks\n",
    "    # 1) the projection to key, query, values\n",
    "    out[\"attention/kqv\"] = 2 * context_length * (n_embd * 3 * n_embd)\n",
    "    # 2) calculating the attention scores\n",
    "    out[\"attention/scores\"] = 2 * context_length * context_length * n_embd\n",
    "    # 3) the reduction of the values (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "    out[\"attention/reduce\"] = 2 * n_head * (context_length * context_length * head_size)\n",
    "    # 4) the final linear projection\n",
    "    out[\"attention/proj\"] = 2 * context_length * (n_embd * n_embd)\n",
    "    out[\"attention\"] = sum(out[\"attention/\" + k] for k in [\"kqv\", \"scores\", \"reduce\", \"proj\"])\n",
    "\n",
    "    # MLP blocks\n",
    "    ffw_size = 4 * n_embd  # feed forward size\n",
    "    out[\"mlp/ffw1\"] = 2 * context_length * (n_embd * ffw_size)\n",
    "    out[\"mlp/ffw2\"] = 2 * context_length * (ffw_size * n_embd)\n",
    "    out[\"mlp\"] = out[\"mlp/ffw1\"] + out[\"mlp/ffw2\"]\n",
    "\n",
    "    # the transformer and the rest of it\n",
    "    out[\"block\"] = out[\"attention\"] + out[\"mlp\"]\n",
    "    out[\"transformer\"] = num_decoder_blocks * out[\"block\"]\n",
    "    out[\"dense\"] = 2 * context_length * (n_embd * vocab_size)\n",
    "\n",
    "    # forward,backward,total\n",
    "    out[\"forward_total\"] = out[\"transformer\"] + out[\"dense\"]\n",
    "    out[\"backward_total\"] = 2 * out[\"forward_total\"]  # use common estimate of bwd = 2*fwd\n",
    "    out[\"total\"] = out[\"forward_total\"] + out[\"backward_total\"]\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flops function calculates the **total** number of **floating point\n",
    "operations** required to process a **single** sample\n",
    "$\\mathbf{x} = \\left(x_1, x_2, \\ldots, x_{T}\\right)$ of length $T$ through the\n",
    "entire model for a single **forward** pass.\n",
    "\n",
    "We take one sample snippet of code to explain how the flops are calculated:\n",
    "\n",
    "```python\n",
    "# 2) calculating the attention scores\n",
    "out[\"attention/scores\"] = 2 * context_length * context_length * n_embd\n",
    "```\n",
    "\n",
    "This is not difficult to see if one recalls the attention mechanism:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q} \\mathbf{K}^T}{\\sqrt{d_k}}\\right) \\mathbf{V}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{Q}$, $\\mathbf{K}$, and $\\mathbf{V}$ are the query, key, and value\n",
    "matrices of size $T \\times d_q$, $T \\times d_k$, and $T \\times d_v$,\n",
    "respectively. For simplicity, we assume that $d_q = d_k = d_v = D$ (which is\n",
    "`n_embd` in the code).\n",
    "\n",
    "In particular `attention_scores` is calculated as:\n",
    "\n",
    "```python\n",
    "attention_scores  = torch.matmul(query, key.transpose(dim0=-2, dim1=-1)) / torch.sqrt(torch.tensor(d_q).float())\n",
    "```\n",
    "\n",
    "which is the dot product of the query and key matrices, divided by the square\n",
    "root of the dimension of the query matrix and is of shape $T \\times T$. However,\n",
    "recall that ultimately the matrix multiplication of the two matrices\n",
    "$\\mathbf{Q}$ and $\\mathbf{K}^{\\top}$ is of shape $T \\times D$ and $D \\times T$,\n",
    "and by our earlier equation {eq}`playbook-transformer-flops-approximation`, this\n",
    "would be a total of $2 \\times T \\times T \\times D$ FLOPs, coinciding with the\n",
    "code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+---------------------+\n",
      "|       name       |    flops     |      ratio (%)      |\n",
      "+------------------+--------------+---------------------+\n",
      "|  attention/kqv   |  3623878656  | 1.2425508965889174  |\n",
      "| attention/scores |  1610612736  | 0.5522448429284077  |\n",
      "| attention/reduce |  1610612736  | 0.5522448429284077  |\n",
      "|  attention/proj  |  1207959552  | 0.41418363219630583 |\n",
      "|    attention     |  8053063680  | 2.7612242146420387  |\n",
      "|     mlp/ffw1     |  4831838208  | 1.6567345287852233  |\n",
      "|     mlp/ffw2     |  4831838208  | 1.6567345287852233  |\n",
      "|       mlp        |  9663676416  | 3.3134690575704466  |\n",
      "|      block       | 17716740096  |  6.074693272212485  |\n",
      "|   transformer    | 212600881152 |  72.89631926654981  |\n",
      "|      dense       | 79047426048  |  27.10368073345018  |\n",
      "|  forward_total   | 291648307200 |        100.0        |\n",
      "|  backward_total  | 583296614400 |        200.0        |\n",
      "|      total       | 874944921600 |        300.0        |\n",
      "+------------------+--------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "f = flops()\n",
    "flops_total = f[\"forward_total\"]\n",
    "\n",
    "table = [(\"name\", \"flops\", \"ratio (%)\")]\n",
    "for k, v in f.items():\n",
    "    table.append((k, v, v / flops_total * 100))\n",
    "\n",
    "print(tabulate(table, headers=\"firstrow\", tablefmt=\"pretty\", numalign=\"right\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check with Palm Paper's FLOPs Calculation\n",
    "\n",
    "The `palm_flops` function, as inspired by the\n",
    "[technical report of Google's PaLM 2](https://ai.google/static/documents/palm2techreport.pdf),\n",
    "calculates an estimate of the model's floating point operations (FLOPs) but\n",
    "introduces a specific formula for computing model FLOPs utilization (MFU) per\n",
    "token and for the entire model.\n",
    "\n",
    "1. **Non-Embedding Model Parameters (`N`)**: This calculation starts by\n",
    "   estimating the number of non-embedding model parameters. In transformer\n",
    "   models like PaLM, a significant portion of the parameters resides in the\n",
    "   embedding layers. This formula adjusts for that by subtracting the\n",
    "   embedding/position parameters from the total, focusing on the parameters\n",
    "   actively involved in computations outside of embeddings.\n",
    "\n",
    "2. **Model Dimensions (`L`, `H`, `Q`, `T`)**:\n",
    "\n",
    "   - `L` = Number of layers (`n_layer`)\n",
    "   - `H` = Number of attention heads (`n_head`)\n",
    "   - `Q` = Size of each attention head (`n_embd // n_head`)\n",
    "   - `T` = Sequence length (`block_size`), also referred to as context length\n",
    "     in other discussions.\n",
    "\n",
    "3. **MF Per Token (`mf_per_token`)**: This represents the estimated FLOPs for\n",
    "   processing a single token, calculated as `6*N + 12*L*H*Q*T`.\n",
    "\n",
    "4. **Total Model FLOPs for a sequence (`mf_per_sequence`)**: This is calculated\n",
    "   by multiplying the per-token FLOPs estimate (`mf_per_token`) by the sequence\n",
    "   length (`block_size` or `T`). This gives the total estimated FLOPs for\n",
    "   processing a sequence of length `T`.\n",
    "\n",
    "This is more of a sanity check for Karpathy, and he confirms if using PaLM's\n",
    "`palm_flops` function to calculate FLOPs for our GPT-2 model yields similar\n",
    "results to the ones he wrote himself (`flops` function).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now here is an estimate copy pasted from the PaLM paper\n",
    "# this formula is often used to calculate MFU (model flops utilization)\n",
    "def palm_flops(\n",
    "    params: OrderedDict[str, int], num_decoder_blocks: int, n_head: int, n_embd: int, context_length: int\n",
    ") -> int:\n",
    "    \"\"\"Estimate of the model flops following PaLM paper formula.\"\"\"\n",
    "    # non-embedding model parameters. note that we do not subtract the\n",
    "    # embedding/token params because those are tied and get used in the last layer.\n",
    "    N = params()[\"total\"] - params()[\"embedding/position\"]\n",
    "    L, H, Q, T = num_decoder_blocks, n_head, n_embd // n_head, context_length\n",
    "    mf_per_token = 6 * N + 12 * L * H * Q * T\n",
    "    mf_per_sequence = mf_per_token * context_length\n",
    "    return mf_per_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PaLM paper estimate of GPT2 flops: 875062886400, Our estimate: 874944921600\n",
      "Ratio: 100.01348%\n"
     ]
    }
   ],
   "source": [
    "gpt2_flops_using_palm_flops_calculation = palm_flops(\n",
    "    params=params,\n",
    "    num_decoder_blocks=gpt2_config.num_decoder_blocks,\n",
    "    n_head=gpt2_config.n_head,\n",
    "    n_embd=gpt2_config.n_embd,\n",
    "    context_length=gpt2_config.context_length,\n",
    ")\n",
    "gpt2_flops_using_own_flops_calculation = f[\"total\"]\n",
    "print(\n",
    "    f\"PaLM paper estimate of GPT2 flops: {gpt2_flops_using_palm_flops_calculation}, Our estimate: {gpt2_flops_using_own_flops_calculation}\"\n",
    ")\n",
    "print(f\"Ratio: {gpt2_flops_using_palm_flops_calculation / gpt2_flops_using_own_flops_calculation * 100:.5f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Floating Point Operations Per Second (FLOPS)\n",
    "\n",
    "In computational tasks and processor performance assessment, we measure the\n",
    "capacity for floating-point computation in terms of\n",
    "[FLOPS](https://en.wikipedia.org/wiki/FLOPS), an acronym that stands for\n",
    "**_Floating Point Operations Per Second_**. This metric indicates the quantity\n",
    "of floating-point arithmetic operations—specifically, additions, subtractions,\n",
    "multiplications, and divisions—that a computing system is capable of performing\n",
    "_every second_. For example, a processor with the capability to execute one\n",
    "trillion such operations within a second is said to have a computational\n",
    "performance of 1 teraFLOP (TFLOP) or simply 1 TFLOPS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU precision point report is different flops for different precisions!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Practical Considerations for FLOPs in Deep Learning\n",
    "\n",
    "- In deep learning models, the number of FLOPs required for a forward pass\n",
    "  through the network gives an indication of the model's complexity and\n",
    "  efficiency. Models with higher FLOPs require more computational resources,\n",
    "  which can affect training and inference times, especially on large datasets.\n",
    "- Hardware wise we also can gauge what types of GPU is needed for the model,\n",
    "  especially when high FLOPS models are used, we would likely need a high-end\n",
    "  GPU that can operate at a higher FLOPS per second.\n",
    "\n",
    "### FLOPS Per Second in GPUs\n",
    "\n",
    "To this end, if we know the the **_total number of FLOPs required for a forward\n",
    "pass through a deep learning model_**, coupled with the **_the theoretical FLOPS\n",
    "per second of a GPU_**, we can estimate the time taken to perform the forward\n",
    "pass on the GPU. This is given by the formula:\n",
    "\n",
    "```text\n",
    "-   The total number of FLOPs required for a forward pass through a deep\n",
    "    learning model.\n",
    "-   The time taken to perform the forward pass.\n",
    "-   The number of GPUs used for the forward pass.\n",
    "-   The FLOPS per second of each GPU.\n",
    "-   The total number of FLOPS per second across all GPUs.\n",
    "-   The cost of running the GPUs for the forward pass.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_mfu(\n",
    "    num_decoder_blocks: int,\n",
    "    num_heads: int,\n",
    "    d_model: int,\n",
    "    context_length: int,\n",
    "    model_total_parameters: int,\n",
    "    forward_backward_per_step: int,\n",
    "    time_taken_per_step: float,\n",
    ") -> float:\n",
    "    \"\"\"Reference from nanogpt to estimate model flops utilization (MFU)\n",
    "    in units of A100 bfloat16 peak FLOPS. Likely used as a callback to log\n",
    "    the MFU during training.\"\"\"\n",
    "    # first estimate the number of flops we do per iteration.\n",
    "    # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
    "    L, H, Q, T = num_decoder_blocks, num_heads, d_model // num_heads, context_length\n",
    "    flops_per_token = 6 * model_total_parameters + 12 * L * H * Q * T\n",
    "    flops_per_fwdbwd = flops_per_token * T\n",
    "    flops_per_iter = flops_per_fwdbwd * forward_backward_per_step\n",
    "    # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
    "    flops_achieved = flops_per_iter * (1.0 / time_taken_per_step)  # per second\n",
    "    flops_promised = 312e12  # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
    "    mfu = flops_achieved / flops_promised\n",
    "    return mfu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model FLOPs Utilization (MFU)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given:\n",
    "\n",
    "-   Batch size with gradient accumulation factored in:\n",
    "    $\\text{batch_size} = 20\n",
    "    \\times 5$\n",
    "-   Measured time per iteration: $\\text{measured_time} = 0.755$ seconds (note\n",
    "    this is Karpathy's own measured time for the forward and backward pass - 1\n",
    "    iteration)\n",
    "-   Total FLOPs required by the model for one forward and backward pass:\n",
    "    $f[\\text{total}]$ for a single sample/sequence $\\mathbf{x} = \\left(x_1, x_2, \\ldots, x_{T}\\right)$\n",
    "\n",
    "1. **Measured Throughput**:\n",
    "\n",
    "    The throughput, in terms of samples processed per second, can be calculated\n",
    "    as follows:\n",
    "\n",
    "    $$\n",
    "    \\text{measured_throughput} = \\frac{\\text{batch_size}}{\\text{measured_time}}\n",
    "    $$\n",
    "\n",
    "    Substituting the given values:\n",
    "\n",
    "    $$\n",
    "    \\text{measured_throughput} = \\frac{20 \\times 5}{0.755}\n",
    "    $$\n",
    "\n",
    "2. **FLOPs Achieved**:\n",
    "\n",
    "    The total floating point operations per second (FLOPs) achieved, based on\n",
    "    the measured throughput, is:\n",
    "\n",
    "    $$\n",
    "    \\text{flops_achieved} = f[\\text{total}] \\times \\text{measured_throughput}\n",
    "    $$\n",
    "\n",
    "    Here, $f[\\text{total}]$ represents the total FLOPs required by the model for\n",
    "    one complete pass (forward and backward) of a single sample, and multiplying\n",
    "    it by the measured throughput gives the effective FLOPs achieved during\n",
    "    actual model execution.\n",
    "\n",
    "3. **Fraction of A100 Utilization**:\n",
    "\n",
    "    Given the A100's promised performance for bfloat16 operations:\n",
    "    \n",
    "    $$\n",
    "    \\text{a100_bfloat16_flops_promised} = 312 \\times 10^{12} \\text{ FLOPS}\n",
    "    $$\n",
    "\n",
    "    The fraction of the A100 GPU utilized can be expressed as a percentage of\n",
    "    the promised FLOPs:\n",
    "\n",
    "    $$\n",
    "    \\text{fraction of A100 used (\\%)} = \\left( \\frac{\\text{flops_achieved}}{\\text{a100_bfloat16_flops_promised}} \\right) \\times 100\n",
    "    $$\n",
    "\n",
    "    Substituting $\\text{flops_achieved}$ and\n",
    "    $\\text{a100_bfloat16_flops_promised}$ with their respective values provides\n",
    "    the percentage utilization of the A100 GPU's computational capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction of A100 used: 37.14%\n"
     ]
    }
   ],
   "source": [
    "# here is what we currently roughly measure\n",
    "batch_size = 20 * 5  # 5 is grad_accum, so total batch size is 100\n",
    "measured_time = 0.755  # in seconds per iteration\n",
    "measured_throughput = batch_size / measured_time # number of samples processed per second\n",
    "flops_achieved = f[\"total\"] * measured_throughput\n",
    "\n",
    "# A100 is cited to be 312 TFLOPS of bfloat16 running on tensor cores\n",
    "a100_bfloat16_flops_promised = 312e12\n",
    "\n",
    "# the fraction of the A100 that we are using:\n",
    "print(f\"fraction of A100 used: {flops_achieved / a100_bfloat16_flops_promised * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132.4503311258278"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, we'd prefer to be somewhere around 50%+, and not just for a single GPU but for an entire DDP run. So we still have some work to do, but at least we're within a factor of ~2X of what is achievable with this GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time needed to train the model: 3.46 days\n"
     ]
    }
   ],
   "source": [
    "# Finally let's check out the 6ND approximation as total cost of training in FLOPs\n",
    "model_size = params()[\"total\"]  # this is number of parameters, N\n",
    "tokens_num = 300e9  # 300B tokens, this is dataset size in tokens, D\n",
    "a100_flops = 312e12  # 312 TFLOPS\n",
    "assumed_mfu = 0.3  # assume this model flops utilization (take the current 37% from above and add some DDP overhead)\n",
    "flops_throughput = a100_flops * 8 * assumed_mfu  # assume an 8XA100 node at 30% utilization\n",
    "flops_needed = 6 * model_size * tokens_num  # 6ND\n",
    "time_needed_s = flops_needed / flops_throughput  # in seconds\n",
    "print(f\"time needed to train the model: {time_needed_s/3600/24:.2f} days\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not a bad estimate at all. I trained this model and it converged in roughly 4 days. Btw as a good reference for where 6ND comes from and some intuition around it I recommend [Dzmitry's post](https://medium.com/@dzmitrybahdanau/the-flops-calculus-of-language-model-training-3b19c1f025e4).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   The calculation of total FLOPs for a model's operations is independent of\n",
    "    the floating-point precision used (e.g., FP32, FP16, BF16). However when\n",
    "    training models, we may choose different floating-point precisions and by\n",
    "    extension, different hardware GPUs has different FLOPS for different\n",
    "    precisions.\n",
    "\n",
    "-   **Definition**: MFU measures how efficiently a deep learning model utilizes\n",
    "    the available FLOPS of the hardware on which it's running. It's expressed as\n",
    "    a percentage or ratio of the actual FLOPs used by the model during execution\n",
    "    to the maximum FLOPs capability of the hardware.\n",
    "\n",
    "-   **Calculation**: MFU is calculated by taking the actual FLOPs achieved by\n",
    "    the model during a task (such as processing a batch of data) and dividing it\n",
    "    by the theoretical FLOPs capability of the hardware, then multiplying by 100\n",
    "    to get a percentage.\n",
    "\n",
    "    $$\n",
    "    \\text{MFU} = \\left( \\frac{\\text{Actual FLOPs Achieved by\n",
    "    Model}}{\\text{Theoretical FLOPs Capability of Hardware}} \\right) \\times\n",
    "    100\\%\n",
    "    $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will leave the theoretical formula (i.e. $6ND$ etc) to the reader to\n",
    "understand from the technical report of Google's PaLM 2, as well\n",
    "[The FLOPs Calculus of Language Model Training](https://medium.com/@dzmitrybahdanau/the-flops-calculus-of-language-model-training-3b19c1f025e4)\n",
    "written by\n",
    "[Dzmitry Bahdanau](https://medium.com/@dzmitrybahdanau?source=post_page-----3b19c1f025e4--------------------------------).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://arxiv.org/pdf/2204.02311.pdf\n",
    "- https://ai.google/static/documents/palm2techreport.pdf\n",
    "- https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf\n",
    "- https://github.com/karpathy/nanoGPT/blob/master/transformer_sizing.ipynb\n",
    "- https://medium.com/@dzmitrybahdanau/the-flops-calculus-of-language-model-training-3b19c1f025e4\n",
    "- https://github.com/keras-team/tf-keras/issues/6\n",
    "- https://github.com/mosaicml/composer/blob/dev/composer/callbacks/speed_monitor.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f5833218766b48e6e35e4452ee875aac0e2188d05bbe5298f2c62b79f08b222"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
